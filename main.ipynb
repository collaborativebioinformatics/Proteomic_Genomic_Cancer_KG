{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a0ded3a-f8e1-4792-8c5e-6151bd845113",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T02:42:10.411298Z",
     "iopub.status.busy": "2025-10-03T02:42:10.410979Z",
     "iopub.status.idle": "2025-10-03T02:42:12.702620Z",
     "shell.execute_reply": "2025-10-03T02:42:12.701450Z",
     "shell.execute_reply.started": "2025-10-03T02:42:10.411276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-geometric in /opt/conda/lib/python3.12/site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.12/site-packages (from torch-geometric) (3.12.15)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch-geometric) (2024.12.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch-geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torch-geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.12/site-packages (from torch-geometric) (5.9.8)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.12/site-packages (from torch-geometric) (3.2.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from torch-geometric) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch-geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch-geometric) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch-geometric) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch-geometric) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch-geometric) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch-geometric) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch-geometric) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->torch-geometric) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /opt/conda/lib/python3.12/site-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.14.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch-geometric) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->torch-geometric) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->torch-geometric) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->torch-geometric) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d58dcd48-4d23-4071-a17c-822e9dca30d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T00:47:29.844001Z",
     "iopub.status.busy": "2025-10-03T00:47:29.843550Z",
     "iopub.status.idle": "2025-10-03T00:47:35.867221Z",
     "shell.execute_reply": "2025-10-03T00:47:35.865990Z",
     "shell.execute_reply.started": "2025-10-03T00:47:29.843922Z"
    }
   },
   "outputs": [],
   "source": [
    "import re, sys, math, json\n",
    "from io import StringIO\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv, global_mean_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb54680e-c859-4ddf-bc7c-a042e1adfe08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T00:47:35.869249Z",
     "iopub.status.busy": "2025-10-03T00:47:35.868396Z",
     "iopub.status.idle": "2025-10-03T00:47:35.875398Z",
     "shell.execute_reply": "2025-10-03T00:47:35.873451Z",
     "shell.execute_reply.started": "2025-10-03T00:47:35.869208Z"
    }
   },
   "outputs": [],
   "source": [
    "URLS = {\n",
    "    # Proteomics (gene-level)\n",
    "    \"prot_tumor\":  \"https://cptac-pancancer-data.s3.us-west-2.amazonaws.com/data_freeze_v1.2_reorganized/COAD/COAD_proteomics_gene_abundance_log2_reference_intensity_normalized_Tumor.txt\",\n",
    "    \"prot_norm\":   \"https://cptac-pancancer-data.s3.us-west-2.amazonaws.com/data_freeze_v1.2_reorganized/COAD/COAD_proteomics_gene_abundance_log2_reference_intensity_normalized_Normal.txt\",\n",
    "\n",
    "    # Phosphoproteomics (site-level)\n",
    "    \"phos_tumor\":  \"https://cptac-pancancer-data.s3.us-west-2.amazonaws.com/data_freeze_v1.2_reorganized/COAD/COAD_phospho_site_abundance_log2_reference_intensity_normalized_Tumor.txt\",\n",
    "    \"phos_norm\":   \"https://cptac-pancancer-data.s3.us-west-2.amazonaws.com/data_freeze_v1.2_reorganized/COAD/COAD_phospho_site_abundance_log2_reference_intensity_normalized_Normal.txt\",\n",
    "\n",
    "    # Mutations (gene-level binary), CNV, RNA\n",
    "    \"mut_gene_bin\":\"https://cptac-pancancer-data.s3.us-west-2.amazonaws.com/data_freeze_v1.2_reorganized/COAD/COAD_somatic_mutation_gene_level_binary.txt\",\n",
    "    \"cnv_log2\":    \"https://cptac-pancancer-data.s3.us-west-2.amazonaws.com/data_freeze_v1.2_reorganized/COAD/COAD_WES_CNV_gene_ratio_log2.txt\",\n",
    "    # alt CNV: GISTIC discrete\n",
    "    # \"cnv_gistic\":  \"https://cptac-pancancer-data.s3.us-west-2.amazonaws.com/data_freeze_v1.2_reorganized/COAD/COAD_WES_CNV_gene_gistic_level.txt\",\n",
    "\n",
    "    \"rna_gene_tumor\": \"https://cptac-pancancer-data.s3.us-west-2.amazonaws.com/data_freeze_v1.2_reorganized/COAD/COAD_RNAseq_gene_RSEM_coding_UQ_1500_log2_Tumor.txt\",\n",
    "\n",
    "    # CMS labels from Linkedomics clinical .tsi\n",
    "    \"tsi\": \"https://linkedomics.org/cptac-colon/Human__CPTAC_COAD__MS__Clinical__Clinical__03_01_2017__CPTAC__Clinical__BCM.tsi\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "765f7e3e-de0e-481d-86a3-860265f6532c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T02:46:03.723654Z",
     "iopub.status.busy": "2025-10-03T02:46:03.723270Z",
     "iopub.status.idle": "2025-10-03T02:46:03.745724Z",
     "shell.execute_reply": "2025-10-03T02:46:03.743981Z",
     "shell.execute_reply.started": "2025-10-03T02:46:03.723626Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import gzip\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from typing import Optional, Literal, Dict, List\n",
    "\n",
    "def fetch_tsv(url: str, index_col: int | None = 0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust TSV fetcher:\n",
    "      - Handles plain or gzipped content.\n",
    "      - Uses pandas dtype inference but preserves the index as string.\n",
    "      - Does not coerce to numeric here (we sanitize later, modality-specific).\n",
    "    \"\"\"\n",
    "    r = requests.get(url, timeout=180)\n",
    "    r.raise_for_status()\n",
    "    content = r.content\n",
    "    # Detect gzip by header\n",
    "    if content[:2] == b\"\\x1f\\x8b\":\n",
    "        buf = io.BytesIO(content)\n",
    "        with gzip.GzipFile(fileobj=buf, mode=\"rb\") as gz:\n",
    "            text = gz.read().decode(\"utf-8\", errors=\"replace\")\n",
    "        df = pd.read_csv(StringIO(text), sep=\"\\t\", header=0, low_memory=False)\n",
    "    else:\n",
    "        df = pd.read_csv(io.BytesIO(content), sep=\"\\t\", header=0, low_memory=False)\n",
    "\n",
    "    if index_col is not None:\n",
    "        # ensure index is string-like, then set as index\n",
    "        idx_name = df.columns[index_col]\n",
    "        df[idx_name] = df[idx_name].astype(str)\n",
    "        df = df.set_index(idx_name)\n",
    "    return df\n",
    "\n",
    "def strip_ensembl_version(x: str):\n",
    "    \"\"\"Drop trailing .version from Ensembl ids; no-op for non-Ensembl.\"\"\"\n",
    "    if isinstance(x, str) and x.startswith((\"ENS\", \"ens\")) and \".\" in x:\n",
    "        return x.split(\".\", 1)[0]\n",
    "    return x\n",
    "\n",
    "def clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Trim whitespace from column names, preserve order.\"\"\"\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def pick_valid_patients(id_list: List[str]) -> List[str]:\n",
    "    \"\"\"Keep non-empty, non-null patient IDs; trims whitespace.\"\"\"\n",
    "    keep = []\n",
    "    for p in id_list:\n",
    "        if isinstance(p, str):\n",
    "            s = p.strip()\n",
    "            if s and s.upper() not in {\"NA\", \"NAN\", \"NULL\"}:\n",
    "                keep.append(s)\n",
    "    return keep\n",
    "\n",
    "# ---------- Utilities you reuse later ----------\n",
    "\n",
    "def collapse_duplicate_rows(\n",
    "    df: pd.DataFrame,\n",
    "    how: Literal[\"median\", \"mean\", \"max_binary\"] = \"median\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collapse duplicate index rows:\n",
    "      - 'median' or 'mean' for continuous (proteo/phospho/RNA/CNV),\n",
    "      - 'max_binary' for mutation (0/1).\n",
    "    \"\"\"\n",
    "    if not df.index.has_duplicates:\n",
    "        return df\n",
    "    if how == \"max_binary\":\n",
    "        return df.groupby(level=0).max(numeric_only=True)\n",
    "    elif how == \"mean\":\n",
    "        return df.groupby(level=0).mean(numeric_only=True)\n",
    "    else:  # median default\n",
    "        return df.groupby(level=0).median(numeric_only=True)\n",
    "\n",
    "def sanitize_numeric(df: pd.DataFrame, clip_abs: float | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Make all entries numeric; non-numeric -> NaN; replace ±inf with NaN; optional clipping.\n",
    "    Use before KNN impute to avoid distance explosions.\n",
    "    \"\"\"\n",
    "    df2 = df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    df2 = df2.replace([np.inf, -np.inf], np.nan)\n",
    "    if clip_abs is not None:\n",
    "        df2 = df2.clip(lower=-clip_abs, upper=clip_abs)\n",
    "    return df2\n",
    "\n",
    "def knn_impute(df: pd.DataFrame, max_k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    KNN impute treating samples as rows (transpose inside).\n",
    "    Falls back to per-row median if samples < 2.\n",
    "    \"\"\"\n",
    "    from sklearn.impute import KNNImputer\n",
    "    n_samples = df.shape[1]\n",
    "    if n_samples < 2:\n",
    "        row_med = df.median(axis=1, skipna=True)\n",
    "        return df.apply(lambda col: col.fillna(row_med), axis=0)\n",
    "\n",
    "    k = min(max_k, max(1, n_samples - 1))\n",
    "    imp = KNNImputer(n_neighbors=k, weights=\"distance\")\n",
    "    vals = imp.fit_transform(df.T.values)  # [samples, features]\n",
    "    return pd.DataFrame(vals.T, index=df.index, columns=df.columns)\n",
    "\n",
    "def baseline_z_from_normals(\n",
    "    tumor_df: pd.DataFrame,\n",
    "    normal_df: pd.DataFrame,\n",
    "    clip: float = 5.0\n",
    ") -> tuple[pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Baseline-normalize tumor using normals only: (tumor - mean_norm) / std_norm.\n",
    "    Returns (Z, mu_norm, sd_norm). NaNs/±inf -> 0 after z, with clipping.\n",
    "    \"\"\"\n",
    "    mu = normal_df.mean(axis=1)\n",
    "    sd = normal_df.std(axis=1, ddof=0).replace(0, np.nan)\n",
    "    Z = (tumor_df.sub(mu, axis=0)).div(sd, axis=0)\n",
    "    Z = Z.replace([np.inf, -np.inf], np.nan).fillna(0.0).clip(-clip, clip)\n",
    "    return Z, mu, sd\n",
    "\n",
    "def z_by_train_only(\n",
    "    df_all: pd.DataFrame,\n",
    "    train_cols: List[str],\n",
    "    clip: float = 5.0\n",
    ") -> tuple[pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Train-only standardization: z-score each row using mean/std computed on TRAIN columns ONLY.\n",
    "    Returns (Z, mu_train, sd_train). NaNs/±inf -> 0 with clipping.\n",
    "    \"\"\"\n",
    "    mu = df_all[train_cols].mean(axis=1)\n",
    "    sd = df_all[train_cols].std(axis=1, ddof=0).replace(0, np.nan)\n",
    "    Z = (df_all.sub(mu, axis=0)).div(sd, axis=0)\n",
    "    Z = Z.replace([np.inf, -np.inf], np.nan).fillna(0.0).clip(-clip, clip)\n",
    "    return Z, mu, sd\n",
    "\n",
    "def parse_cms_from_tsi(url: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse the Linkedomics .tsi and return {patient_id: CMS_label} for 'Transcriptomic_subtype'.\n",
    "    The .tsi uses a wide format: first column is 'attrib_name', subsequent columns are patients.\n",
    "    \"\"\"\n",
    "    df = fetch_tsv(url, index_col=0)\n",
    "    df.index = df.index.astype(str)\n",
    "    # Choose the transcriptomic subtype row robustly\n",
    "    row_key = None\n",
    "    candidates = [idx for idx in df.index if \"Transcriptomic_subtype\" in str(idx)]\n",
    "    if candidates:\n",
    "        row_key = candidates[0]\n",
    "    else:\n",
    "        # fallback: any row containing CMS-like labels\n",
    "        for idx in df.index:\n",
    "            vals = set(str(v) for v in df.loc[idx].values)\n",
    "            if any(v.startswith(\"CMS\") for v in vals):\n",
    "                row_key = idx\n",
    "                break\n",
    "    if row_key is None:\n",
    "        return {}\n",
    "\n",
    "    row = df.loc[row_key]\n",
    "    # Build mapping: keep only non-empty CMS labels\n",
    "    cms_map = {}\n",
    "    for pid, lab in row.items():\n",
    "        if isinstance(lab, str):\n",
    "            lab_s = lab.strip()\n",
    "            if lab_s and lab_s.upper() != \"NA\":\n",
    "                cms_map[pid.strip()] = lab_s\n",
    "    return cms_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "feeea0e1-df5e-483d-b148-22c27a0b1c66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T02:47:30.231928Z",
     "iopub.status.busy": "2025-10-03T02:47:30.231597Z",
     "iopub.status.idle": "2025-10-03T02:47:30.237633Z",
     "shell.execute_reply": "2025-10-03T02:47:30.236559Z",
     "shell.execute_reply.started": "2025-10-03T02:47:30.231906Z"
    }
   },
   "outputs": [],
   "source": [
    "def phospho_index_to_gene(phos_idx: pd.Index) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Map phosphosite row IDs to gene identifiers.\n",
    "    Tries, in order:\n",
    "      - Regex search for Ensembl ID (ENSG...).\n",
    "      - Split on delimiters and look for ENSG or plausible gene symbol.\n",
    "    Returns a Series of gene IDs aligned to phos_idx.\n",
    "    \"\"\"\n",
    "    genes = []\n",
    "    for s in phos_idx.astype(str):\n",
    "        gene = None\n",
    "        # Prefer explicit Ensembl IDs\n",
    "        m = re.search(r\"(ENSG[0-9]+(?:\\.[0-9]+)?)\", s)\n",
    "        if m:\n",
    "            gene = strip_ensembl_version(m.group(1))\n",
    "        else:\n",
    "            parts = re.split(r\"[|,;:_\\s]+\", s)\n",
    "            for p in parts:\n",
    "                if p.startswith(\"ENSG\"):\n",
    "                    gene = strip_ensembl_version(p)\n",
    "                    break\n",
    "                if re.fullmatch(r\"[A-Za-z][A-Za-z0-9\\-]{0,20}\", p):\n",
    "                    gene = p.upper()\n",
    "                    break\n",
    "        genes.append(gene)\n",
    "    return pd.Series(genes, index=phos_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca465f83-3e29-47be-a1c6-bbf834d879de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T02:52:04.622116Z",
     "iopub.status.busy": "2025-10-03T02:52:04.621753Z",
     "iopub.status.idle": "2025-10-03T02:52:19.093352Z",
     "shell.execute_reply": "2025-10-03T02:52:19.092474Z",
     "shell.execute_reply.started": "2025-10-03T02:52:04.622087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prot_tumor (9151, 97) | prot_norm (9152, 100) (dup rows removed: T=0, N=0)\n",
      "phos_tumor (35487, 97) | phos_norm (35485, 100) (site-level; will aggregate to genes later)\n",
      "mut_bin    (14783, 96) (dup rows removed: 0)\n",
      "rna_tumor  (60624, 106) (dup rows removed: 45)\n",
      "cnv_log2   (60558, 105) (dup rows removed: 45)\n"
     ]
    }
   ],
   "source": [
    "# Proteomics (gene-level)\n",
    "prot_tumor = fetch_tsv(URLS[\"prot_tumor\"], index_col=0)\n",
    "prot_norm  = fetch_tsv(URLS[\"prot_norm\"],  index_col=0)\n",
    "prot_tumor.index = prot_tumor.index.map(strip_ensembl_version)\n",
    "prot_norm.index  = prot_norm.index.map(strip_ensembl_version)\n",
    "prot_tumor = clean_cols(prot_tumor)\n",
    "prot_norm  = clean_cols(prot_norm)\n",
    "\n",
    "# Phosphoproteomics (site-level) — stays site-level for now; aggregation happens later\n",
    "phos_tumor = fetch_tsv(URLS[\"phos_tumor\"], index_col=0)\n",
    "phos_norm  = fetch_tsv(URLS[\"phos_norm\"],  index_col=0)\n",
    "phos_tumor = clean_cols(phos_tumor)\n",
    "phos_norm  = clean_cols(phos_norm)\n",
    "\n",
    "# Mutations (gene-level binary)\n",
    "mut_bin = fetch_tsv(URLS[\"mut_gene_bin\"], index_col=0)\n",
    "mut_bin.index = mut_bin.index.map(strip_ensembl_version)\n",
    "mut_bin = clean_cols(mut_bin)\n",
    "\n",
    "# RNA (gene-level, tumor)\n",
    "rna_tumor = fetch_tsv(URLS[\"rna_gene_tumor\"], index_col=0)\n",
    "rna_tumor.index = rna_tumor.index.map(strip_ensembl_version)\n",
    "rna_tumor = clean_cols(rna_tumor)\n",
    "\n",
    "# CNV (gene-level log2 ratio, tumor)\n",
    "cnv_log2 = fetch_tsv(URLS[\"cnv_log2\"], index_col=0)\n",
    "cnv_log2.index = cnv_log2.index.map(strip_ensembl_version)\n",
    "cnv_log2 = clean_cols(cnv_log2)\n",
    "\n",
    "# ---- Light, safe cleanup: collapse duplicate gene rows ----\n",
    "dup_prot_t = int(prot_tumor.index.duplicated().sum())\n",
    "dup_prot_n = int(prot_norm.index.duplicated().sum())\n",
    "dup_mut    = int(mut_bin.index.duplicated().sum())\n",
    "dup_rna    = int(rna_tumor.index.duplicated().sum())\n",
    "dup_cnv    = int(cnv_log2.index.duplicated().sum())\n",
    "\n",
    "if dup_prot_t or dup_prot_n:\n",
    "    prot_tumor = collapse_duplicate_rows(prot_tumor, how=\"median\")\n",
    "    prot_norm  = collapse_duplicate_rows(prot_norm,  how=\"median\")\n",
    "\n",
    "if dup_mut:\n",
    "    # Ensure numeric first, then strict binary max across dups\n",
    "    mut_bin = mut_bin.apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "    mut_bin = collapse_duplicate_rows(mut_bin, how=\"max_binary\")\n",
    "\n",
    "if dup_rna:\n",
    "    rna_tumor = collapse_duplicate_rows(rna_tumor, how=\"median\")\n",
    "\n",
    "if dup_cnv:\n",
    "    cnv_log2 = collapse_duplicate_rows(cnv_log2, how=\"median\")\n",
    "\n",
    "# Strict binary for mutation table (robust if source encodes ints/floats/strings)\n",
    "mut_bin = mut_bin.apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "mut_bin = (mut_bin > 0).astype(np.int8)\n",
    "\n",
    "# Optional: drop obviously invalid/blank patient columns across all tables\n",
    "# (keeps order; other alignment happens later)\n",
    "for df_name in [\"prot_tumor\", \"prot_norm\", \"phos_tumor\", \"phos_norm\", \"mut_bin\", \"rna_tumor\", \"cnv_log2\"]:\n",
    "    df = locals()[df_name]\n",
    "    df.columns = pick_valid_patients(df.columns)\n",
    "\n",
    "print(\"prot_tumor\", prot_tumor.shape, \"| prot_norm\", prot_norm.shape, f\"(dup rows removed: T={dup_prot_t}, N={dup_prot_n})\")\n",
    "print(\"phos_tumor\", phos_tumor.shape, \"| phos_norm\", phos_norm.shape, \"(site-level; will aggregate to genes later)\")\n",
    "print(\"mut_bin   \", mut_bin.shape,    f\"(dup rows removed: {dup_mut})\")\n",
    "print(\"rna_tumor \", rna_tumor.shape,  f\"(dup rows removed: {dup_rna})\")\n",
    "print(\"cnv_log2  \", cnv_log2.shape,   f\"(dup rows removed: {dup_cnv})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f4eebfd-6466-4d51-9860-147f24c17128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T02:55:44.566455Z",
     "iopub.status.busy": "2025-10-03T02:55:44.566142Z",
     "iopub.status.idle": "2025-10-03T02:55:44.917838Z",
     "shell.execute_reply": "2025-10-03T02:55:44.916378Z",
     "shell.execute_reply.started": "2025-10-03T02:55:44.566431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMS classes (label->index): {'CMS1': 0, 'CMS2': 1, 'CMS3': 2, 'CMS4': 3}\n",
      "Total CMS-labeled patients: 85\n",
      "Class counts: Counter({1: 33, 3: 22, 2: 16, 0: 14})\n"
     ]
    }
   ],
   "source": [
    "# ----- Cell 7 (CMS labels from .tsi; robust parsing) -----\n",
    "tsi = fetch_tsv(URLS[\"tsi\"], index_col=None)\n",
    "assert \"attrib_name\" in tsi.columns, \"Unexpected .tsi format: missing 'attrib_name'\"\n",
    "tsi = tsi.set_index(\"attrib_name\")\n",
    "\n",
    "# Find the row containing CMS transcriptomic subtype\n",
    "row_key = None\n",
    "if \"Transcriptomic_subtype\" in tsi.index:\n",
    "    row_key = \"Transcriptomic_subtype\"\n",
    "else:\n",
    "    # try case-insensitive/contains\n",
    "    matches = [ix for ix in tsi.index if \"transcriptomic\" in str(ix).lower() and \"subtype\" in str(ix).lower()]\n",
    "    if matches:\n",
    "        row_key = matches[0]\n",
    "    else:\n",
    "        # fallback: any row whose values look like CMS labels\n",
    "        for ix in tsi.index:\n",
    "            vals = set(str(v) for v in tsi.loc[ix].values)\n",
    "            if any(v.upper().startswith(\"CMS\") for v in vals):\n",
    "                row_key = ix\n",
    "                break\n",
    "\n",
    "assert row_key is not None, \"Could not locate CMS subtype row in the .tsi file.\"\n",
    "\n",
    "cms_row = tsi.loc[row_key].to_dict()\n",
    "\n",
    "# Keep only valid patient → CMS pairs (trim id; accept CMS1..CMS4; drop NA/blank)\n",
    "valid_pairs = []\n",
    "for pid, lab in cms_row.items():\n",
    "    if not isinstance(pid, str) or not isinstance(lab, str):\n",
    "        continue\n",
    "    pid_s = pid.strip()\n",
    "    lab_s = lab.strip().upper()\n",
    "    if not pid_s or lab_s in {\"NA\", \"NAN\", \"\"}:\n",
    "        continue\n",
    "    if not lab_s.startswith(\"CMS\"):  # be strict; ignore non-CMS entries\n",
    "        continue\n",
    "    valid_pairs.append((pid_s, lab_s))\n",
    "\n",
    "assert len(valid_pairs) > 0, \"No valid CMS-labeled patients found.\"\n",
    "\n",
    "kept_patients, labels_str = zip(*valid_pairs)\n",
    "kept_patients = list(kept_patients)\n",
    "labels_str    = list(labels_str)\n",
    "\n",
    "# Stable class order (CMS1..CMS4 if present)\n",
    "classes = sorted(set(labels_str), key=lambda x: (len(x), x))  # CMS1,CMS2,CMS3,CMS4 in order\n",
    "vocab   = {c: i for i, c in enumerate(classes)}               # e.g. {'CMS1':0,'CMS2':1,...}\n",
    "labels_all = torch.tensor([vocab[s] for s in labels_str], dtype=torch.long)\n",
    "\n",
    "# Handy map used later to align labels to proteomics patients\n",
    "cms_pid2lab = {p: vocab[s] for p, s in zip(kept_patients, labels_str)}\n",
    "\n",
    "# Prints\n",
    "from collections import Counter\n",
    "print(\"CMS classes (label->index):\", vocab)\n",
    "print(\"Total CMS-labeled patients:\", len(kept_patients))\n",
    "print(\"Class counts:\", Counter(labels_all.numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bed579c1-c910-463a-ad43-f8bda08a9912",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T02:58:21.486267Z",
     "iopub.status.busy": "2025-10-03T02:58:21.485958Z",
     "iopub.status.idle": "2025-10-03T02:58:21.496412Z",
     "shell.execute_reply": "2025-10-03T02:58:21.495396Z",
     "shell.execute_reply.started": "2025-10-03T02:58:21.486245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMS in proteomics: 76 of 85\n",
      "Patients per modality: {'prot': 97, 'phos': 97, 'rna': 106, 'cnv': 105, 'mut': 96}\n",
      "Overlap w/ proteomics: {'phos': 97, 'rna': 96, 'cnv': 95, 'mut': 96}\n",
      "CMS in (prot ∩ rna ∩ cnv): 75\n"
     ]
    }
   ],
   "source": [
    "# ----- Cell 8 (normalize patient IDs + anchor CMS to proteomics) -----\n",
    "\n",
    "# 8.1) Normalize patient IDs across all matrices (strip whitespace only)\n",
    "for name in [\"prot_tumor\", \"prot_norm\", \"phos_tumor\", \"phos_norm\", \"mut_bin\", \"rna_tumor\", \"cnv_log2\"]:\n",
    "    df = locals()[name]\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    locals()[name] = df  # rebind explicitly\n",
    "\n",
    "# 8.2) Anchor = proteomics tumor columns\n",
    "prot_patients = set(prot_tumor.columns)\n",
    "\n",
    "# 8.3) Keep CMS patients present in proteomics tumor\n",
    "# (cms_pid2lab and kept_patients come from Cell 7)\n",
    "cms_in_prot = [p for p in kept_patients if p in prot_patients]\n",
    "labels_in_prot = torch.tensor([cms_pid2lab[p] for p in cms_in_prot], dtype=torch.long)\n",
    "\n",
    "print(\"CMS in proteomics:\", len(cms_in_prot), \"of\", len(kept_patients))\n",
    "\n",
    "# Optional quick diagnostics (no filtering performed here)\n",
    "sets = {\n",
    "    \"prot\": set(prot_tumor.columns),\n",
    "    \"phos\": set(phos_tumor.columns),\n",
    "    \"rna\":  set(rna_tumor.columns),\n",
    "    \"cnv\":  set(cnv_log2.columns),\n",
    "    \"mut\":  set(mut_bin.columns),\n",
    "}\n",
    "print(\"Patients per modality:\",\n",
    "      {k: len(v) for k, v in sets.items()})\n",
    "print(\"Overlap w/ proteomics:\",\n",
    "      {k: len(sets[k] & sets[\"prot\"]) for k in [\"phos\", \"rna\", \"cnv\", \"mut\"]})\n",
    "print(\"CMS in (prot ∩ rna ∩ cnv):\",\n",
    "      len(set(cms_in_prot) & sets[\"rna\"] & sets[\"cnv\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3238a7dc-ec60-4a8c-aa66-912fdf28c64b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T03:07:17.818736Z",
     "iopub.status.busy": "2025-10-03T03:07:17.818311Z",
     "iopub.status.idle": "2025-10-03T03:07:21.321876Z",
     "shell.execute_reply": "2025-10-03T03:07:21.320751Z",
     "shell.execute_reply.started": "2025-10-03T03:07:17.818702Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proteins kept (UNION): 7102 | Patients kept: 76\n"
     ]
    }
   ],
   "source": [
    "# Tunable missingness threshold (across tumor+normal); 0.20–0.40 common in proteomics\n",
    "PROT_MISS_MAX = 0.20\n",
    "\n",
    "# Aggregate phospho sites -> gene level (median)\n",
    "phos_gene_tumor_map = phospho_index_to_gene(phos_tumor.index)\n",
    "phos_gene_norm_map  = phospho_index_to_gene(phos_norm.index)\n",
    "\n",
    "phos_tumor_gene = phos_tumor.copy()\n",
    "phos_tumor_gene[\"__gene__\"] = phos_gene_tumor_map.values\n",
    "phos_tumor_gene = (\n",
    "    phos_tumor_gene\n",
    "    .dropna(subset=[\"__gene__\"])\n",
    "    .groupby(\"__gene__\")\n",
    "    .median(numeric_only=True)\n",
    ")\n",
    "\n",
    "phos_norm_gene = phos_norm.copy()\n",
    "phos_norm_gene[\"__gene__\"] = phos_gene_norm_map.values\n",
    "phos_norm_gene = (\n",
    "    phos_norm_gene\n",
    "    .dropna(subset=[\"__gene__\"])\n",
    "    .groupby(\"__gene__\")\n",
    "    .median(numeric_only=True)\n",
    ")\n",
    "\n",
    "# Align tumor vs normal within each modality (intersection within modality)\n",
    "prot_genes   = prot_tumor.index.intersection(prot_norm.index)\n",
    "prot_tumor2  = prot_tumor.loc[prot_genes].copy()\n",
    "prot_norm2   = prot_norm.loc[prot_genes].copy()\n",
    "\n",
    "phos_genes   = phos_tumor_gene.index.intersection(phos_norm_gene.index)\n",
    "phos_tumor2  = phos_tumor_gene.loc[phos_genes].copy()\n",
    "phos_norm2   = phos_norm_gene.loc[phos_genes].copy()\n",
    "\n",
    "# Filter by missingness across tumor+normal (modality-wise)\n",
    "def filter_by_missingness(df_tum: pd.DataFrame, df_norm: pd.DataFrame, prot_miss_max=0.20):\n",
    "    df_all = pd.concat([df_tum, df_norm], axis=1)\n",
    "    keep_prot = df_all.isna().mean(axis=1) <= prot_miss_max\n",
    "    return df_tum.loc[keep_prot], df_norm.loc[keep_prot]\n",
    "\n",
    "prot_tumor_f, prot_norm_f = filter_by_missingness(prot_tumor2, prot_norm2, prot_miss_max=PROT_MISS_MAX)\n",
    "phos_tumor_f, phos_norm_f = filter_by_missingness(phos_tumor2, phos_norm2, prot_miss_max=PROT_MISS_MAX)\n",
    "\n",
    "# Keep CMS-labeled proteomics tumor patients (proteomics is the patient anchor)\n",
    "prot_tumor_f = prot_tumor_f.loc[:, prot_tumor_f.columns.intersection(cms_in_prot)]\n",
    "\n",
    "# Proteomics missingness mask BEFORE imputation (tumor-only)\n",
    "# (We keep this aligned to prot_tumor_f columns; final mask computed after union below)\n",
    "# prot_mask_pre_raw = prot_tumor_f.isna().astype(np.float32)  # optional sanity, not used later\n",
    "\n",
    "# Sanitize and KNN-impute each modality (adaptive k)\n",
    "def sanitize_for_impute(df: pd.DataFrame, clip_abs: float | None = None) -> pd.DataFrame:\n",
    "    df2 = df.apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "    if clip_abs is not None:\n",
    "        df2 = df2.clip(lower=-clip_abs, upper=clip_abs)\n",
    "    return df2\n",
    "\n",
    "def knn_impute(df: pd.DataFrame, max_k: int = 5) -> pd.DataFrame:\n",
    "    n_samples = df.shape[1]\n",
    "    k = min(max_k, max(1, n_samples - 1))\n",
    "    if n_samples < 2:\n",
    "        row_med = df.median(axis=1, skipna=True)\n",
    "        return df.apply(lambda col: col.fillna(row_med), axis=0)\n",
    "    imp = KNNImputer(n_neighbors=k, weights='distance')\n",
    "    vals = imp.fit_transform(df.T.values)  # [samples, features]\n",
    "    return pd.DataFrame(vals.T, index=df.index, columns=df.columns)\n",
    "\n",
    "# Choose patient lists for imputation\n",
    "tumor_patients_for_prot = prot_tumor_f.columns.tolist()\n",
    "tumor_patients_for_phos = phos_tumor_f.columns.intersection(cms_in_prot).tolist()\n",
    "if len(tumor_patients_for_phos) == 0:\n",
    "    tumor_patients_for_phos = phos_tumor_f.columns.tolist()\n",
    "\n",
    "# Sanitize\n",
    "prot_tumor_f_san = sanitize_for_impute(prot_tumor_f.loc[:, tumor_patients_for_prot], clip_abs=1e6)\n",
    "prot_norm_f_san  = sanitize_for_impute(prot_norm_f,                                  clip_abs=1e6)\n",
    "phos_tumor_f_san = sanitize_for_impute(phos_tumor_f.loc[:, tumor_patients_for_phos], clip_abs=1e6)\n",
    "phos_norm_f_san  = sanitize_for_impute(phos_norm_f,                                  clip_abs=1e6)\n",
    "\n",
    "# Impute\n",
    "prot_tumor_imp = knn_impute(prot_tumor_f_san, max_k=5)\n",
    "prot_norm_imp  = knn_impute(prot_norm_f_san,  max_k=5)\n",
    "phos_tumor_imp = knn_impute(phos_tumor_f_san, max_k=5)\n",
    "phos_norm_imp  = knn_impute(phos_norm_f_san,  max_k=5)\n",
    "\n",
    "# Baseline-normalize with NORMALS ONLY (no leakage)\n",
    "def baseline_z(tum: pd.DataFrame, norm: pd.DataFrame, clip: float = 5.0):\n",
    "    mu = norm.mean(axis=1)\n",
    "    sd = norm.std(axis=1, ddof=0).replace(0, np.nan)\n",
    "    z  = (tum.sub(mu, axis=0)).div(sd, axis=0)\n",
    "    z  = z.replace([np.inf, -np.inf], np.nan).fillna(0.0).clip(-clip, clip)\n",
    "    return z, mu, sd\n",
    "\n",
    "prot_z, prot_mu, prot_sd = baseline_z(prot_tumor_imp, prot_norm_imp, clip=5.0)\n",
    "phos_z, phos_mu, phos_sd = baseline_z(phos_tumor_imp, phos_norm_imp, clip=5.0)\n",
    "\n",
    "# Patients → align columns to CMS proteomics set\n",
    "patient_ids = list(prot_z.columns.intersection(cms_in_prot))\n",
    "prot_z = prot_z.loc[:, patient_ids]\n",
    "phos_z = phos_z.reindex(columns=patient_ids, fill_value=np.nan)\n",
    "\n",
    "# GENES → ALIGN BY UNION (not intersection)\n",
    "union_proteins = prot_z.index.union(phos_z.index)\n",
    "\n",
    "# Reindex to union; keep NaN in phospho for presence mask, fill proteo with zeros where absent\n",
    "prot_z_u = prot_z.reindex(union_proteins, fill_value=0.0)\n",
    "phos_z_u = phos_z.reindex(union_proteins)  # keep NaN now\n",
    "\n",
    "# Masks (pre-impute, tumor-only), aligned to union\n",
    "prot_mask_pre = prot_tumor_f.reindex(union_proteins).loc[:, patient_ids].isna().astype(np.float32)\n",
    "phos_present  = phos_tumor_f.reindex(union_proteins).loc[:, patient_ids].notna().astype(np.float32)  # optional\n",
    "\n",
    "# Fill phospho NaNs with 0 for features (presence mask retains availability info)\n",
    "phos_z_u = phos_z_u.fillna(0.0)\n",
    "\n",
    "prot_z        = prot_z_u\n",
    "phos_z        = phos_z_u\n",
    "prot_mask_pre = prot_mask_pre\n",
    "protein_ids   = list(union_proteins)\n",
    "patient_ids   = list(patient_ids)\n",
    "\n",
    "print(\"Proteins kept (UNION):\", len(protein_ids), \"| Patients kept:\", len(patient_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2c876b54-e56e-4bc3-84f7-c6a01eba213c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T03:08:12.470676Z",
     "iopub.status.busy": "2025-10-03T03:08:12.470346Z",
     "iopub.status.idle": "2025-10-03T03:08:12.497165Z",
     "shell.execute_reply": "2025-10-03T03:08:12.496087Z",
     "shell.execute_reply.started": "2025-10-03T03:08:12.470651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mut_lists] patients=76 | mut_table_cols_found=76 | with_any=76 | avg_mut_proteins/patient=205.4\n"
     ]
    }
   ],
   "source": [
    "# ----- Cell X: align mutations to protein_id space & build per-patient lists -----\n",
    "\n",
    "# 1) Collapse duplicate gene rows (still binary)\n",
    "if mut_bin.index.has_duplicates:\n",
    "    mut_bin = mut_bin.groupby(level=0).max(numeric_only=True)\n",
    "\n",
    "# 2) Keep only mutation columns for patients we're actually using (order = patient_ids)\n",
    "mut_cols = [p for p in patient_ids if p in mut_bin.columns]\n",
    "mut_sub  = mut_bin.reindex(columns=mut_cols)\n",
    "\n",
    "# 3) Align rows to protein_ids (already Ensembl base IDs/symbols from your pipeline)\n",
    "#    Missing rows -> 0 (no mutation recorded for that protein)\n",
    "mut_aligned_df = mut_sub.reindex(index=protein_ids).fillna(0)\n",
    "\n",
    "# 4) Ensure strictly binary int8 (DataFrame -> ndarray)\n",
    "mut_aligned = (mut_aligned_df.to_numpy() > 0).astype(np.int8)  # shape [N_prot, len(mut_cols)]\n",
    "\n",
    "# 5) (Optional) Map protein id -> row index (handy if you need lookups later)\n",
    "prot_idx_map = {g: i for i, g in enumerate(protein_ids)}\n",
    "\n",
    "# 6) Build mut_lists in EXACT patient_ids order (empty for patients with no mut column)\n",
    "mut_lists = []\n",
    "present_cols = {c: j for j, c in enumerate(mut_cols)}  # patient_id -> column idx in mut_aligned\n",
    "for p in patient_ids:\n",
    "    j = present_cols.get(p, None)\n",
    "    if j is None:\n",
    "        mut_lists.append(np.array([], dtype=np.int64))\n",
    "    else:\n",
    "        prot_indices = np.nonzero(mut_aligned[:, j])[0].astype(np.int64)\n",
    "        mut_lists.append(prot_indices)\n",
    "\n",
    "# 7) Diagnostics\n",
    "n_with_any = sum(arr.size > 0 for arr in mut_lists)\n",
    "avg_muts   = float(np.mean([arr.size for arr in mut_lists])) if len(mut_lists) else 0.0\n",
    "print(f\"[mut_lists] patients={len(mut_lists)} | mut_table_cols_found={len(mut_cols)} \"\n",
    "      f\"| with_any={n_with_any} | avg_mut_proteins/patient={avg_muts:.1f}\")\n",
    "\n",
    "# 8) Sanity checks\n",
    "assert len(mut_lists) == len(patient_ids), \"mut_lists must align 1:1 with patient_ids\"\n",
    "if len(mut_cols) == 0:\n",
    "    print(\"WARNING: No mutation columns matched current patient_ids. \"\n",
    "          \"Downstream mutation edges will all be empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "758c7366-1e94-4808-aa0e-f6daa353df23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T03:11:30.409704Z",
     "iopub.status.busy": "2025-10-03T03:11:30.409385Z",
     "iopub.status.idle": "2025-10-03T03:11:30.639608Z",
     "shell.execute_reply": "2025-10-03T03:11:30.638497Z",
     "shell.execute_reply.started": "2025-10-03T03:11:30.409683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients kept (proteomics-anchored): 76\n",
      "Protein features: (7102, 76) | Phospho: (7102, 76)\n",
      "COMPACT gene space: 7093\n",
      "RNA raw (compacted): (7093, 76) | CNV raw (compacted): (7093, 76)\n"
     ]
    }
   ],
   "source": [
    "# ---------- proteomics-anchored patients; compact gene space; dedup-safe ----------\n",
    "\n",
    "# Inputs expected from earlier cells:\n",
    "# - prot_z, phos_z, prot_mask_pre  (proteo/phospho union; columns already tumor patients)\n",
    "# - patient_ids                    (CMS-aligned proteomics tumor patients)\n",
    "# - rna_tumor, cnv_log2            (raw RNA/CNV tables with gene rows, patient columns)\n",
    "\n",
    "# --- Helpers to collapse duplicates on rows/columns (median across dups) ---\n",
    "def collapse_duplicate_rows(df: pd.DataFrame, how=\"median\"):\n",
    "    if not df.index.is_unique:\n",
    "        if how == \"median\":\n",
    "            df = df.groupby(level=0).median(numeric_only=True)\n",
    "        elif how == \"mean\":\n",
    "            df = df.groupby(level=0).mean(numeric_only=True)\n",
    "        else:\n",
    "            raise ValueError(\"how must be 'median' or 'mean'\")\n",
    "    return df\n",
    "\n",
    "def collapse_duplicate_cols(df: pd.DataFrame, how=\"median\"):\n",
    "    if not df.columns.is_unique:\n",
    "        if how == \"median\":\n",
    "            df = df.T.groupby(level=0).median(numeric_only=True).T\n",
    "        elif how == \"mean\":\n",
    "            df = df.T.groupby(level=0).mean(numeric_only=True).T\n",
    "        else:\n",
    "            raise ValueError(\"how must be 'median' or 'mean'\")\n",
    "    return df\n",
    "\n",
    "def strip_index_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.index   = df.index.map(lambda x: str(x).strip())\n",
    "    df.columns = df.columns.map(lambda x: str(x).strip())\n",
    "    return df\n",
    "\n",
    "# --- Proteomics-anchored patients (preserve order, de-dup) ---\n",
    "patient_ids = list(dict.fromkeys(patient_ids))  # de-dup while preserving order\n",
    "\n",
    "# Sync proteo/phospho/mask to these patients\n",
    "prot_z = prot_z.loc[:, [p for p in patient_ids if p in prot_z.columns]]\n",
    "phos_z = phos_z.reindex(columns=prot_z.columns, fill_value=0.0)  # align to proteomics anchor\n",
    "prot_mask_pre = prot_mask_pre.reindex(columns=prot_z.columns)\n",
    "\n",
    "prot_patients = prot_z.columns\n",
    "assert prot_patients.is_unique, \"Proteomics patient IDs must be unique.\"\n",
    "\n",
    "# --- 9.2) Prepare RNA/CNV for these patients (allow missing) with robust de-dup ---\n",
    "rna_tumor = strip_index_columns(rna_tumor)\n",
    "cnv_log2  = strip_index_columns(cnv_log2)\n",
    "\n",
    "# Keep only the columns we can match (but we will reindex to prot_patients later)\n",
    "rna_cols = [p for p in prot_patients if p in rna_tumor.columns]\n",
    "cnv_cols = [p for p in prot_patients if p in cnv_log2.columns]\n",
    "\n",
    "rna_tumor_sub = rna_tumor.loc[:, rna_cols] if len(rna_cols) else pd.DataFrame(index=rna_tumor.index)\n",
    "cnv_log2_sub = cnv_log2.loc[:,  cnv_cols] if len(cnv_cols) else pd.DataFrame(index=cnv_log2.index)\n",
    "\n",
    "# Collapse duplicates on BOTH axes (avoid reindex errors)\n",
    "rna_tumor_sub = collapse_duplicate_rows(collapse_duplicate_cols(rna_tumor_sub), how=\"median\")\n",
    "cnv_log2_sub = collapse_duplicate_rows(collapse_duplicate_cols(cnv_log2_sub),  how=\"median\")\n",
    "\n",
    "# Force numeric (non-numeric -> NaN) to keep downstream ops stable\n",
    "rna_tumor_sub = rna_tumor_sub.apply(pd.to_numeric, errors=\"coerce\")\n",
    "cnv_log2_sub = cnv_log2_sub.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# --- Compact gene space = protein ids ∩ RNA genes ∩ CNV genes ---\n",
    "protein_index = pd.Index(prot_z.index).map(lambda x: str(x).strip())\n",
    "rna_index = rna_tumor_sub.index\n",
    "cnv_index = cnv_log2_sub.index\n",
    "\n",
    "gene_space = protein_index.intersection(rna_index).intersection(cnv_index)\n",
    "\n",
    "# If intersection is unexpectedly tiny, you can relax here (optional):\n",
    "if len(gene_space) == 0:\n",
    "    # fallback: intersect protein with whichever modality has more overlap\n",
    "    inter_pr_rna = protein_index.intersection(rna_index)\n",
    "    inter_pr_cnv = protein_index.intersection(cnv_index)\n",
    "    gene_space = inter_pr_rna if len(inter_pr_rna) >= len(inter_pr_cnv) else inter_pr_cnv\n",
    "    print(f\"WARNING: 3-way intersection empty; falling back to 2-way size={len(gene_space)}\")\n",
    "\n",
    "# --- Reindex RNA/CNV to compact genes and proteomics patient order (dedup-safe) ---\n",
    "# Columns we reindex to are unique by assert above\n",
    "rna_full = rna_tumor_sub.reindex(index=gene_space, columns=prot_patients)\n",
    "cnv_full = cnv_log2_sub.reindex(index=gene_space, columns=prot_patients)\n",
    "\n",
    "# Availability masks BEFORE any z-scaling\n",
    "rna_avl = rna_full.notna().astype(np.float32)\n",
    "cnv_avl = cnv_full.notna().astype(np.float32)\n",
    "\n",
    "# Final sanity prints\n",
    "print(\"Patients kept (proteomics-anchored):\", len(prot_patients))\n",
    "print(\"Protein features:\", prot_z.shape, \"| Phospho:\", phos_z.shape)\n",
    "print(\"COMPACT gene space:\", len(gene_space))\n",
    "print(\"RNA raw (compacted):\", rna_full.shape, \"| CNV raw (compacted):\", cnv_full.shape)\n",
    "\n",
    "# Expose for next cells\n",
    "patient_ids = list(prot_patients)\n",
    "gene_ids    = list(gene_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f604434-aab8-42a0-b462-80154cf5b74c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T03:17:07.486978Z",
     "iopub.status.busy": "2025-10-03T03:17:07.486603Z",
     "iopub.status.idle": "2025-10-03T03:17:07.512404Z",
     "shell.execute_reply": "2025-10-03T03:17:07.511353Z",
     "shell.execute_reply.started": "2025-10-03T03:17:07.486952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genes with ≥1 phospho measurement: 3144\n",
      "Genes with no phospho measurement: 3958\n",
      "Total genes (union): 7102\n",
      "Phospho coverage per patient (mean±sd): 0.42903780937194824 +/- 0.009626339189708233\n",
      "Phospho entries that were originally missing but now have imputed |z|>0: 7370\n",
      "Present phospho entries that are ~0 (valid, just FYI): 0\n",
      "Patient 0: mask-present phospho features = 3091\n",
      "Lowest-coverage patients: ['05CO026', '11CO045', '11CO030', '21CO006', '11CO031'] -> [0.39819768, 0.39819768, 0.3984793, 0.3984793, 0.40805408]\n"
     ]
    }
   ],
   "source": [
    "# Alignment checks (must be True)\n",
    "assert phos_present.index.equals(phos_z.index),  \"Row (gene) order mismatch\"\n",
    "assert phos_present.columns.equals(phos_z.columns), \"Column (patient) order mismatch\"\n",
    "assert len(protein_ids) == phos_present.shape[0] == phos_z.shape[0], \"Gene dimension mismatch\"\n",
    "\n",
    "# 1) How many genes have ANY phospho measured across patients?\n",
    "genes_with_any_phos = int((phos_present.sum(axis=1) > 0).sum())\n",
    "genes_with_no_phos  = int((phos_present.sum(axis=1) == 0).sum())\n",
    "print(\"Genes with ≥1 phospho measurement:\", genes_with_any_phos)\n",
    "print(\"Genes with no phospho measurement:\",  genes_with_no_phos)\n",
    "print(\"Total genes (union):\", len(protein_ids))\n",
    "\n",
    "# 2) Per-patient phospho coverage (fraction of genes with any measured phospho in that patient)\n",
    "per_patient_phos_cov = phos_present.sum(axis=0) / len(protein_ids)\n",
    "print(\"Phospho coverage per patient (mean±sd):\",\n",
    "      float(per_patient_phos_cov.mean()), \"+/-\", float(per_patient_phos_cov.std()))\n",
    "\n",
    "# 3) Strong sanity: features must be ~zero wherever mask==0\n",
    "Z = phos_z.to_numpy()                          # shape [N_genes, N_patients]\n",
    "M = phos_present.to_numpy(dtype=bool)          # same shape\n",
    "eps = 1e-8\n",
    "imputed_slots_used = int(((~M) & (np.abs(Z) > eps)).sum())\n",
    "print(\"Phospho entries that were originally missing but now have imputed |z|>0:\", imputed_slots_used)\n",
    "\n",
    "# 4) Informational: how often present-but-numerically-zero (not an error)\n",
    "present_but_zeroish = int((M & (np.abs(Z) <= eps)).sum())\n",
    "print(\"Present phospho entries that are ~0 (valid, just FYI):\", present_but_zeroish)\n",
    "\n",
    "# 5) Example patient summary (use mask, not nonzero)\n",
    "p0 = 0\n",
    "mask_present_p0 = int(M[:, p0].sum())\n",
    "print(f\"Patient {p0}: mask-present phospho features =\", mask_present_p0)\n",
    "\n",
    "# (Optional) Show patients with lowest coverage\n",
    "cov_sorted = per_patient_phos_cov.sort_values()\n",
    "print(\"Lowest-coverage patients:\", list(cov_sorted.index[:5]), \"->\", list(cov_sorted.values[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43eb484f-4908-44c6-8652-06beb26f74e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T03:20:16.569181Z",
     "iopub.status.busy": "2025-10-03T03:20:16.568729Z",
     "iopub.status.idle": "2025-10-03T03:20:16.583926Z",
     "shell.execute_reply": "2025-10-03T03:20:16.582740Z",
     "shell.execute_reply.started": "2025-10-03T03:20:16.569152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero phospho values where mask==0 (should be 0): 0\n"
     ]
    }
   ],
   "source": [
    "# HARD MASK phospho: zero out imputed values where mask==0\n",
    "phos_z = phos_z.where(phos_present.astype(bool), 0.0)\n",
    "\n",
    "# Re-run the consistency check:\n",
    "Z = phos_z.to_numpy()\n",
    "M = phos_present.to_numpy(dtype=bool)\n",
    "eps = 1e-8\n",
    "bad_nonzero_when_missing = int(((~M) & (np.abs(Z) > eps)).sum())\n",
    "print(\"Non-zero phospho values where mask==0 (should be 0):\", bad_nonzero_when_missing)  # expect 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ca04fadd-2ea5-4033-bf0f-fafa721a23ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T03:22:37.839741Z",
     "iopub.status.busy": "2025-10-03T03:22:37.839418Z",
     "iopub.status.idle": "2025-10-03T03:22:37.860085Z",
     "shell.execute_reply": "2025-10-03T03:22:37.858967Z",
     "shell.execute_reply.started": "2025-10-03T03:22:37.839700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_splits=5 | sizes → train=45  val=16  test=15\n",
      "Train class counts: Counter({1: 18, 3: 13, 2: 8, 0: 6})\n",
      "Val class counts:   Counter({1: 6, 3: 4, 0: 3, 2: 3})\n",
      "Test class counts:  Counter({1: 5, 3: 4, 0: 3, 2: 3})\n"
     ]
    }
   ],
   "source": [
    "# ---------- CELL 10: StratifiedKFold split (train/val/test) ----------\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 10.1) Align CMS labels to patient_ids (from Cell 8/9 proteomics anchor)\n",
    "cms_pid2lab = {p: l for p, l in zip(cms_in_prot, labels_in_prot.tolist())}\n",
    "labels_aligned = torch.tensor([cms_pid2lab[p] for p in patient_ids], dtype=torch.long)\n",
    "y = labels_aligned.numpy()\n",
    "\n",
    "# 10.2) Drop classes with <2 samples (Stratified splits require ≥2 per class)\n",
    "counts = np.bincount(y, minlength=int(y.max() + 1))\n",
    "rare_classes = np.where(counts < 2)[0].tolist()\n",
    "if rare_classes:\n",
    "    keep_mask = ~np.isin(y, rare_classes)\n",
    "    patient_ids = [p for p, m in zip(patient_ids, keep_mask) if m]\n",
    "    labels_aligned = labels_aligned[keep_mask]\n",
    "    y = labels_aligned.numpy()\n",
    "    prot_z        = prot_z.loc[:, patient_ids]\n",
    "    phos_z        = phos_z.loc[:, patient_ids]\n",
    "    prot_mask_pre = prot_mask_pre.loc[:, patient_ids]\n",
    "    rna_full      = rna_full.loc[:, patient_ids]\n",
    "    cnv_full      = cnv_full.loc[:, patient_ids]\n",
    "    rna_avl       = rna_avl.loc[:, patient_ids]\n",
    "    cnv_avl       = cnv_avl.loc[:, patient_ids]\n",
    "    print(\"Dropped rare CMS classes:\", rare_classes)\n",
    "\n",
    "# 10.3) StratifiedKFold → pick one fold for VAL, one for TEST; rest = TRAIN\n",
    "# Choose folds so each is ~N/n_splits. For ~75 pts, n_splits=5 ⇒ ~15 per fold (nice for your target 12–16).\n",
    "counts = np.bincount(y, minlength=int(y.max() + 1))\n",
    "min_per_class = int(counts[counts > 0].min())\n",
    "n_splits = int(min(5, max(2, min_per_class)))   # cap at 5 by default; never below 2\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "folds = list(skf.split(np.zeros_like(y), y))\n",
    "\n",
    "# Nominally use fold 0 for VAL and fold 1 for TEST (disjoint)\n",
    "VAL_FOLD  = 0\n",
    "TEST_FOLD = 1 if n_splits >= 3 else 0  # if only 2 folds, fallback handled below\n",
    "\n",
    "if n_splits >= 3:\n",
    "    val_idx  = folds[VAL_FOLD][1]\n",
    "    test_idx = folds[TEST_FOLD][1]\n",
    "    all_idx = np.arange(len(y))\n",
    "    train_mask = np.ones_like(all_idx, dtype=bool)\n",
    "    train_mask[val_idx] = False\n",
    "    train_mask[test_idx] = False\n",
    "    train_idx = all_idx[train_mask]\n",
    "else:\n",
    "    # n_splits == 2 → create TEST from one fold's test, then carve VAL from the remaining via SSS\n",
    "    test_idx = folds[1][1]              # ~50% as test\n",
    "    rest_idx = folds[1][0]              # complement used for train+val\n",
    "    # carve a small stratified val (e.g., 20% of rest)\n",
    "    sss_val = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=43)\n",
    "    tr_sub, va_sub = next(sss_val.split(np.zeros_like(y[rest_idx]), y[rest_idx]))\n",
    "    train_idx = rest_idx[tr_sub]\n",
    "    val_idx   = rest_idx[va_sub]\n",
    "\n",
    "# Final reporting\n",
    "print(f\"n_splits={n_splits} | sizes → train={len(train_idx)}  val={len(val_idx)}  test={len(test_idx)}\")\n",
    "print(\"Train class counts:\", Counter(y[train_idx]))\n",
    "print(\"Val class counts:  \", Counter(y[val_idx]))\n",
    "print(\"Test class counts: \", Counter(y[test_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f95fd23-e358-4293-a91f-2a2fb850503e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T02:25:28.993512Z",
     "iopub.status.busy": "2025-10-03T02:25:28.993202Z",
     "iopub.status.idle": "2025-10-03T02:25:29.009247Z",
     "shell.execute_reply": "2025-10-03T02:25:29.007929Z",
     "shell.execute_reply.started": "2025-10-03T02:25:28.993490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: train=57 val=15 test=4\n",
      "Train class counts: Counter({1: 22, 3: 16, 2: 10, 0: 9})\n",
      "Val class counts:   Counter({1: 6, 3: 4, 2: 3, 0: 2})\n",
      "Test class counts:  Counter({0: 1, 1: 1, 2: 1, 3: 1})\n"
     ]
    }
   ],
   "source": [
    "# Labels aligned to patient_ids\n",
    "cms_pid2lab = {p:l for p,l in zip(cms_in_prot, labels_in_prot.tolist())}\n",
    "labels_aligned = torch.tensor([cms_pid2lab[p] for p in patient_ids], dtype=torch.long)\n",
    "y = labels_aligned.numpy()\n",
    "\n",
    "# Drop classes with <2 members\n",
    "counts = np.bincount(y, minlength=int(y.max()+1))\n",
    "rare = np.where(counts < 2)[0].tolist()\n",
    "if rare:\n",
    "    keep_mask = ~np.isin(y, rare)\n",
    "    patient_ids = [p for p,m in zip(patient_ids, keep_mask) if m]\n",
    "    labels_aligned = labels_aligned[keep_mask]\n",
    "    y = labels_aligned.numpy()\n",
    "    prot_z   = prot_z.loc[:, patient_ids]\n",
    "    phos_z   = phos_z.loc[:, patient_ids]\n",
    "    prot_mask_pre = prot_mask_pre.loc[:, patient_ids]\n",
    "    rna_tumor_sub = rna_tumor_sub.loc[:, patient_ids]\n",
    "    cnv_log2_sub  = cnv_log2_sub.loc[:, patient_ids]\n",
    "    print(\"Dropped rare classes:\", rare)\n",
    "\n",
    "# Robust split: 1 per class in test, small val if possible\n",
    "rng = np.random.default_rng(42)\n",
    "classes = np.unique(y)\n",
    "cls_to_idx = {c: np.where(y==c)[0].tolist() for c in classes}\n",
    "test_idx = [int(rng.choice(idx)) for idx in cls_to_idx.values()]\n",
    "test_mask = np.zeros_like(y, dtype=bool); test_mask[test_idx] = True\n",
    "rest_idx = np.where(~test_mask)[0]\n",
    "y_rest = y[rest_idx]\n",
    "\n",
    "# Try a small stratified val, otherwise pick one per class if possible\n",
    "val_idx = []\n",
    "if len(rest_idx) >= 4 and all([(y_rest==c).sum()>=2 for c in classes]):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=43)\n",
    "    tr_sub, va_sub = next(sss.split(np.zeros_like(y_rest), y_rest))\n",
    "    train_idx = rest_idx[tr_sub]; val_idx = rest_idx[va_sub]\n",
    "else:\n",
    "    # 1 per class for val if possible\n",
    "    for c in classes:\n",
    "        pool = [i for i in rest_idx if y[i]==c]\n",
    "        if len(pool)>=2:\n",
    "            val_idx.append(int(rng.choice(pool)))\n",
    "    val_idx = np.array(sorted(set(val_idx)), dtype=int)\n",
    "    train_mask = (~test_mask).copy()\n",
    "    train_mask[val_idx] = False\n",
    "    train_idx = np.where(train_mask)[0]\n",
    "\n",
    "print(f\"Split sizes: train={len(train_idx)} val={len(val_idx)} test={len(test_idx)}\")\n",
    "print(\"Train class counts:\", Counter(y[train_idx]))\n",
    "print(\"Val class counts:  \", Counter(y[val_idx]))\n",
    "print(\"Test class counts: \", Counter(y[test_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "71ed0e9a-8431-4b3b-a7fa-aed3c3e7cad0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T03:26:45.169226Z",
     "iopub.status.busy": "2025-10-03T03:26:45.168831Z",
     "iopub.status.idle": "2025-10-03T03:26:45.286143Z",
     "shell.execute_reply": "2025-10-03T03:26:45.285096Z",
     "shell.execute_reply.started": "2025-10-03T03:26:45.169199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein channels: torch.Size([76, 7102]) torch.Size([76, 7102]) torch.Size([76, 7102]) | phos_avl\n",
      "Gene channels: torch.Size([76, 7093]) torch.Size([76, 7093]) torch.Size([76, 7093]) torch.Size([76, 7093])\n"
     ]
    }
   ],
   "source": [
    "# ---------- CELL 11: Train-only normalization for RNA/CNV + tensorization ----------\n",
    "\n",
    "# 11.1) Train-only z-scales for RNA & CNV (fit on *train* patients only, ignore NaN)\n",
    "def z_by_train_only(df_full: pd.DataFrame, train_cols: list[str], clip=5.0):\n",
    "    if len(train_cols) == 0:\n",
    "        # no training data for this modality; return zeros\n",
    "        Z = pd.DataFrame(0.0, index=df_full.index, columns=df_full.columns)\n",
    "        mu = pd.Series(0.0, index=df_full.index)\n",
    "        sd = pd.Series(1.0, index=df_full.index)\n",
    "        return Z, mu, sd\n",
    "\n",
    "    train_df = df_full.loc[:, train_cols]\n",
    "    mu = train_df.mean(axis=1, skipna=True)\n",
    "    sd = train_df.std(axis=1, ddof=0, skipna=True).replace(0, np.nan)\n",
    "\n",
    "    Z = (df_full.sub(mu, axis=0)).div(sd, axis=0)\n",
    "    # guard against inf, then clip; leave NaN to be filled later\n",
    "    Z = Z.replace([np.inf, -np.inf], np.nan).clip(-clip, clip)\n",
    "    return Z, mu, sd\n",
    "\n",
    "# Which train patients actually have RNA/CNV?\n",
    "train_cols_rna = [patient_ids[i] for i in train_idx if patient_ids[i] in rna_full.columns]\n",
    "train_cols_cnv = [patient_ids[i] for i in train_idx if patient_ids[i] in cnv_full.columns]\n",
    "\n",
    "rna_z_df, rna_mu, rna_sd = z_by_train_only(rna_full, train_cols_rna, clip=5.0)\n",
    "cnv_z_df, cnv_mu, cnv_sd = z_by_train_only(cnv_full, train_cols_cnv, clip=5.0)\n",
    "\n",
    "# 11.2) Fill NaNs with 0.0 for features; keep availability masks separately\n",
    "rna_z = rna_z_df.fillna(0.0)\n",
    "cnv_z = cnv_z_df.fillna(0.0)\n",
    "\n",
    "# (Optional) If you decided to *hard-mask* phospho to zero where originally missing:\n",
    "HARD_MASK_PHOS = False\n",
    "if HARD_MASK_PHOS and 'phos_present' in globals():\n",
    "    phos_z = phos_z.where(phos_present.astype(bool), 0.0)\n",
    "\n",
    "# 11.3) Tensorize ALL channels (alignments assumed from prior cells)\n",
    "# Protein channels (proteomics union space × patient_ids)\n",
    "X_prot      = torch.tensor(prot_z.T.values,        dtype=torch.float32)   # [P, N_prot]\n",
    "X_phos      = torch.tensor(phos_z.T.values,        dtype=torch.float32)   # [P, N_prot]\n",
    "X_mask_prot = torch.tensor(prot_mask_pre.T.values, dtype=torch.float32)   # [P, N_prot]\n",
    "\n",
    "# If you kept a phospho availability mask, expose it too (lets the GNN know measured vs imputed)\n",
    "X_phos_avl = None\n",
    "if 'phos_present' in globals():\n",
    "    X_phos_avl = torch.tensor(phos_present.T.values, dtype=torch.float32) # [P, N_prot]\n",
    "\n",
    "# Gene channels (value + availability) in the compact gene space\n",
    "X_rna      = torch.tensor(rna_z.T.values,    dtype=torch.float32)         # [P, N_gene]\n",
    "X_cnv      = torch.tensor(cnv_z.T.values,    dtype=torch.float32)         # [P, N_gene]\n",
    "X_rna_avl  = torch.tensor(rna_avl.T.values,  dtype=torch.float32)         # [P, N_gene]\n",
    "X_cnv_avl  = torch.tensor(cnv_avl.T.values,  dtype=torch.float32)         # [P, N_gene]\n",
    "\n",
    "# Final IDs\n",
    "protein_ids = list(prot_z.index)                   # union protein list from Cell 8\n",
    "gene_ids    = list(rna_z.index.union(cnv_z.index)) # equals the compact gene_space\n",
    "\n",
    "# Quick finite checks (will raise if something slipped through)\n",
    "for name, X in [\n",
    "    (\"X_prot\", X_prot), (\"X_phos\", X_phos), (\"X_mask_prot\", X_mask_prot),\n",
    "    (\"X_rna\", X_rna), (\"X_cnv\", X_cnv), (\"X_rna_avl\", X_rna_avl), (\"X_cnv_avl\", X_cnv_avl)\n",
    "]:\n",
    "    if not torch.isfinite(X).all():\n",
    "        raise ValueError(f\"{name} contains non-finite values\")\n",
    "\n",
    "if X_phos_avl is not None and not torch.isfinite(X_phos_avl).all():\n",
    "    raise ValueError(\"X_phos_avl contains non-finite values\")\n",
    "\n",
    "print(\"Protein channels:\", X_prot.shape, X_phos.shape, X_mask_prot.shape, \n",
    "      \"| phos_avl\" if X_phos_avl is not None else \"| phos_avl (not provided)\")\n",
    "print(\"Gene channels:\",    X_rna.shape,  X_cnv.shape,  X_rna_avl.shape, X_cnv_avl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a689fe1c-b745-4c7b-9c10-45082c78b808",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T03:28:18.495529Z",
     "iopub.status.busy": "2025-10-03T03:28:18.495209Z",
     "iopub.status.idle": "2025-10-03T03:28:18.530040Z",
     "shell.execute_reply": "2025-10-03T03:28:18.528856Z",
     "shell.execute_reply.started": "2025-10-03T03:28:18.495506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[codes edges] 7093 (gene→protein); overlap genes=7093/7093 (100.0%)\n",
      "protein_ids: 7102 total | Ensembl-like=7102 | non-Ensembl=0\n",
      "gene_ids   : 7093 total   | Ensembl-like=7093 | non-Ensembl=0\n"
     ]
    }
   ],
   "source": [
    "# ---------- CELL X: Build gene <-> protein cross-edges (name-matched) ----------\n",
    "def is_ensembl(g):\n",
    "    return isinstance(g, str) and g.upper().startswith(\"ENSG\")\n",
    "\n",
    "# 1) Build index maps\n",
    "prot_idx = {g: i for i, g in enumerate(protein_ids)}  # protein (union) IDs\n",
    "gene_idx = {g: i for i, g in enumerate(gene_ids)}     # compact gene space (RNA∩CNV∩proteo)\n",
    "\n",
    "# 2) Exact-name matches → edges gene->protein\n",
    "src, dst = [], []\n",
    "overlap = 0\n",
    "for g in gene_ids:\n",
    "    if g in prot_idx:         # exact string match\n",
    "        src.append(gene_idx[g])\n",
    "        dst.append(prot_idx[g])\n",
    "        overlap += 1\n",
    "\n",
    "codes_edge_index     = torch.tensor([src, dst], dtype=torch.long)\n",
    "codes_rev_edge_index = torch.tensor([dst, src], dtype=torch.long)\n",
    "\n",
    "print(f\"[codes edges] {codes_edge_index.shape[1]} (gene→protein); overlap genes={overlap}/{len(gene_ids)} \"\n",
    "      f\"({overlap/len(gene_ids):.1%})\")\n",
    "\n",
    "# 3) Quick diagnostics: how many of your protein_ids are Ensembl vs symbols?\n",
    "n_prot_ens  = sum(is_ensembl(p) for p in protein_ids)\n",
    "n_gene_ens  = sum(is_ensembl(g) for g in gene_ids)\n",
    "print(f\"protein_ids: {len(protein_ids)} total | Ensembl-like={n_prot_ens} | non-Ensembl={len(protein_ids)-n_prot_ens}\")\n",
    "print(f\"gene_ids   : {len(gene_ids)} total   | Ensembl-like={n_gene_ens} | non-Ensembl={len(gene_ids)-n_gene_ens}\")\n",
    "\n",
    "# 4) Optional: warn if overlap is low (often caused by symbol-vs-Ensembl mix)\n",
    "if overlap < 0.6 * len(gene_ids):\n",
    "    print(\"WARNING: Low gene↔protein overlap. Likely ID convention mismatch (symbols vs Ensembl).\")\n",
    "    print(\"Tip: ensure phospho aggregation produced Ensembl IDs when available (your function prefers ENSG).\")\n",
    "    print(\"If many protein rows are symbols only, consider restricting protein_ids to Ensembl-like to improve alignment.\")\n",
    "\n",
    "# --- OPTIONAL TIGHTENING (commented) ---\n",
    "# If you decide to restrict to Ensembl-only proteins to maximize overlap, do it BEFORE tensorization:\n",
    "# ONLY_ENSG_PROTEINS = False\n",
    "# if ONLY_ENSG_PROTEINS:\n",
    "#     keep_mask = [is_ensembl(p) for p in protein_ids]\n",
    "#     prot_keep = [p for p, k in zip(protein_ids, keep_mask) if k]\n",
    "#     # Reindex proteomics matrices (and any masks) to prot_keep, then rebuild protein_ids and codes_edge_index:\n",
    "#     prot_z        = prot_z.loc[prot_keep]\n",
    "#     phos_z        = phos_z.loc[prot_keep]\n",
    "#     prot_mask_pre = prot_mask_pre.loc[prot_keep]\n",
    "#     protein_ids   = prot_keep\n",
    "#     # Rebuild the maps and edges after this restriction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f6ee22aa-3a27-48f2-aacd-80b9df7d72b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T03:48:04.051728Z",
     "iopub.status.busy": "2025-10-03T03:48:04.051436Z",
     "iopub.status.idle": "2025-10-03T03:48:07.048477Z",
     "shell.execute_reply": "2025-10-03T03:48:07.047412Z",
     "shell.execute_reply.started": "2025-10-03T03:48:04.051706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[kNN] k=15 -> edges=190398 (directed), isolates=0\n",
      "Nodes=7102 | Edges=190398 (directed)\n",
      "Degree: mean=26.81, min=15, max=191\n",
      "Isolated proteins: 0\n",
      "Components=1, Giant component fraction=1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch_geometric.utils import to_undirected  # is_undirected not needed\n",
    "\n",
    "def build_ppi_knn_from_train_robust(X_train_prot: torch.Tensor,\n",
    "                                    X_train_phos: torch.Tensor | None = None,\n",
    "                                    k: int = 15,\n",
    "                                    k_step: int = 5,\n",
    "                                    k_max: int = 40,\n",
    "                                    tiny_jitter: float = 1e-8):\n",
    "    Vp = X_train_prot.cpu().numpy().T  # [N_prot, P_train]\n",
    "    if X_train_phos is not None:\n",
    "        Vh = X_train_phos.cpu().numpy().T\n",
    "        V  = np.concatenate([Vp, Vh], axis=1)\n",
    "    else:\n",
    "        V = Vp\n",
    "\n",
    "    V = np.nan_to_num(V, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    row_norm = np.linalg.norm(V, axis=1)\n",
    "    zero_rows = (row_norm == 0)\n",
    "    if zero_rows.any():\n",
    "        V[zero_rows, 0] = tiny_jitter\n",
    "\n",
    "    N = V.shape[0]\n",
    "    cur_k = min(k, max(1, N - 1))\n",
    "\n",
    "    def make_edges(cur_k: int) -> torch.Tensor:\n",
    "        S = cosine_similarity(V)               # [N, N]\n",
    "        np.fill_diagonal(S, -np.inf)\n",
    "        S = np.nan_to_num(S, nan=-1.0)\n",
    "        kk = min(cur_k, N - 1)\n",
    "        idx = np.argpartition(-S, kth=kk, axis=1)[:, :kk]\n",
    "\n",
    "        pairs = set()\n",
    "        for i in range(N):\n",
    "            for j in idx[i]:\n",
    "                if i == j: \n",
    "                    continue\n",
    "                a, b = (i, j) if i < j else (j, i)\n",
    "                pairs.add((a, b))\n",
    "\n",
    "        if not pairs:\n",
    "            return torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "        E = np.array(sorted(list(pairs)), dtype=np.int64)  # [M, 2]\n",
    "        src = np.concatenate([E[:, 0], E[:, 1]], axis=0)\n",
    "        dst = np.concatenate([E[:, 1], E[:, 0]], axis=0)\n",
    "        edge_index = torch.tensor(np.stack([src, dst], axis=0), dtype=torch.long)\n",
    "\n",
    "        # Always force canonical undirected (idempotent) and drop self-loops\n",
    "        edge_index = to_undirected(edge_index, num_nodes=N)\n",
    "        mask = edge_index[0] != edge_index[1]\n",
    "        return edge_index[:, mask]\n",
    "\n",
    "    while True:\n",
    "        edge_index = make_edges(cur_k)\n",
    "        deg = torch.bincount(edge_index[0], minlength=N)\n",
    "        isolates = int((deg == 0).sum().item())\n",
    "        print(f\"[kNN] k={cur_k} -> edges={edge_index.size(1)} (directed), isolates={isolates}\")\n",
    "        if isolates == 0 or cur_k >= k_max:\n",
    "            break\n",
    "        cur_k = min(k_max, cur_k + k_step)\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "\n",
    "# ---- Build PPI from TRAIN patients (uses your existing splits) ----\n",
    "X_train_prot = X_prot[train_idx]   # [P_train, N_prot]\n",
    "X_train_phos = X_phos[train_idx]   # [P_train, N_prot]\n",
    "ppi_edge_index = build_ppi_knn_from_train_robust(X_train_prot, X_train_phos, k=15, k_step=5, k_max=40)\n",
    "\n",
    "# Diagnostics\n",
    "N = X_prot.shape[1]\n",
    "deg = torch.bincount(ppi_edge_index[0], minlength=N)\n",
    "print(f\"Nodes={N} | Edges={ppi_edge_index.size(1)} (directed)\")\n",
    "print(f\"Degree: mean={deg.float().mean():.2f}, min={int(deg.min())}, max={int(deg.max())}\")\n",
    "iso = (deg == 0).sum().item()\n",
    "print(\"Isolated proteins:\", iso)\n",
    "\n",
    "# Connectedness (requires networkx)\n",
    "import networkx as nx\n",
    "E = list(zip(ppi_edge_index[0].tolist(), ppi_edge_index[1].tolist()))\n",
    "G = nx.Graph(); G.add_edges_from(E)\n",
    "n_comp = nx.number_connected_components(G)\n",
    "giant = len(max(nx.connected_components(G), key=len)) / G.number_of_nodes()\n",
    "print(f\"Components={n_comp}, Giant component fraction={giant:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "23294164-b947-4c43-bee6-3121f7c730c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T03:48:25.066431Z",
     "iopub.status.busy": "2025-10-03T03:48:25.066151Z",
     "iopub.status.idle": "2025-10-03T03:48:25.710888Z",
     "shell.execute_reply": "2025-10-03T03:48:25.708044Z",
     "shell.execute_reply.started": "2025-10-03T03:48:25.066411Z"
    }
   },
   "outputs": [],
   "source": [
    "# Canonicalize each edge as (min, max), drop duplicates\n",
    "E = torch.sort(ppi_edge_index, dim=0)[0].t()      # [E, 2]\n",
    "E = torch.unique(E, dim=0).t()                    # [2, E_unique]\n",
    "ppi_edge_index = torch.stack([E[0], E[1]], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3dd31de5-a24b-4975-aeb9-03f1e13fbe5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T03:49:15.704151Z",
     "iopub.status.busy": "2025-10-03T03:49:15.703712Z",
     "iopub.status.idle": "2025-10-03T03:49:18.288179Z",
     "shell.execute_reply": "2025-10-03T03:49:18.287046Z",
     "shell.execute_reply.started": "2025-10-03T03:49:15.704127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[kNN] k=15 -> edges=190398 (directed), isolates=0\n",
      "Nodes=7102 | Edges=190398 (directed)\n",
      "Degree: mean=26.81, min=15, max=191\n",
      "Isolated proteins: 0\n",
      "Degree histogram (degree:count): {15: 646, 16: 673, 17: 601, 18: 490, 19: 468, 20: 437, 21: 339, 22: 305, 23: 267, 24: 250, 25: 234, 26: 189, 27: 150, 28: 163, 29: 147, 30: 109, 31: 103, 32: 101, 33: 90, 34: 98, 35: 88, 36: 57, 37: 62, 38: 63, 39: 52, 40: 44, 41: 46, 42: 51, 43: 44, 44: 35, 45: 32, 46: 36, 47: 22, 48: 23, 49: 22, 50: 35, 51: 24, 52: 27, 53: 17, 54: 22, 55: 16, 56: 20, 57: 23, 58: 16, 59: 20, 60: 21, 61: 17, 62: 12, 63: 9, 64: 14, 65: 11, 66: 14, 67: 15, 68: 6, 69: 7, 70: 12, 71: 8, 72: 4, 73: 9, 74: 9, 75: 7, 76: 10, 77: 4, 78: 4, 79: 8, 80: 9, 81: 4, 82: 5, 83: 9, 84: 6, 85: 3, 86: 6, 87: 4, 88: 4, 89: 3, 90: 5, 91: 6, 92: 4, 93: 3, 94: 2, 96: 2, 97: 6, 98: 1, 99: 1, 100: 2, 101: 3, 102: 1, 103: 1, 104: 2, 105: 2, 106: 6, 107: 3, 108: 1, 109: 1, 110: 3, 111: 4, 113: 1, 115: 1, 117: 3, 118: 1, 120: 1, 122: 1, 124: 1, 128: 3, 131: 2, 136: 2, 141: 2, 143: 1, 144: 1, 145: 1, 146: 1, 148: 1, 152: 1, 153: 1, 156: 1, 159: 1, 160: 1, 170: 1, 172: 1, 181: 1, 191: 1}\n"
     ]
    }
   ],
   "source": [
    "X_train_prot = X_prot[train_idx]\n",
    "X_train_phos = X_phos[train_idx]\n",
    "ppi_edge_index = build_ppi_knn_from_train_robust(X_train_prot, X_train_phos, k=15, k_step=5, k_max=40)\n",
    "\n",
    "N = X_prot.shape[1]\n",
    "deg = torch.bincount(ppi_edge_index[0], minlength=N)\n",
    "print(f\"Nodes={N} | Edges={ppi_edge_index.size(1)} (directed)\")\n",
    "print(f\"Degree: mean={deg.float().mean():.2f}, min={int(deg.min())}, max={int(deg.max())}\")\n",
    "print(\"Isolated proteins:\", int((deg==0).sum()))\n",
    "\n",
    "vals, counts = torch.unique(deg, return_counts=True)\n",
    "print(\"Degree histogram (degree:count):\", dict(zip(vals.tolist(), counts.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4d7189f9-f67e-4768-ac2d-99a9de36f4b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T03:51:50.893792Z",
     "iopub.status.busy": "2025-10-03T03:51:50.893494Z",
     "iopub.status.idle": "2025-10-03T03:51:51.955879Z",
     "shell.execute_reply": "2025-10-03T03:51:51.954946Z",
     "shell.execute_reply.started": "2025-10-03T03:51:50.893770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 0: graph_deg=38 sym_topk_deg=38 overlap=38\n",
      "Jaccard(graph, sym_topk) = 1.0\n",
      "Top-10 by cosine: [2250 3571 4738 3589 1301 5349 1603 1666 6539 6972] scores: [0.795 0.788 0.785 0.783 0.776 0.774 0.771 0.77  0.764 0.76 ]\n",
      "Top-10 protein IDs: ['ENSG00000115541', 'ENSG00000137288', 'ENSG00000160124', 'ENSG00000137563', 'ENSG00000101346', 'ENSG00000167600', 'ENSG00000105388', 'ENSG00000106028', 'ENSG00000197142', 'ENSG00000242110']\n",
      "Graph neighbors (IDs): ['ENSG00000047230', 'ENSG00000070019', 'ENSG00000099800', 'ENSG00000101019', 'ENSG00000101346', 'ENSG00000101421', 'ENSG00000105388', 'ENSG00000106028', 'ENSG00000115541', 'ENSG00000118939', 'ENSG00000126432', 'ENSG00000130234', 'ENSG00000132541', 'ENSG00000136270', 'ENSG00000137288', 'ENSG00000137563', 'ENSG00000140092', 'ENSG00000145824', 'ENSG00000147044', 'ENSG00000147202', 'ENSG00000154639', 'ENSG00000160124', 'ENSG00000162910', 'ENSG00000164182', 'ENSG00000164576', 'ENSG00000164924', 'ENSG00000165215', 'ENSG00000165280', 'ENSG00000165556', 'ENSG00000167600', 'ENSG00000169398', 'ENSG00000174173', 'ENSG00000176532', 'ENSG00000180900', 'ENSG00000187720', 'ENSG00000197142', 'ENSG00000198001', 'ENSG00000242110']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# pick the same patient subset & feature construction used to build the PPI\n",
    "Vp = X_train_prot.cpu().numpy().T                    # [N_prot, P_train]\n",
    "V  = Vp\n",
    "if 'X_train_phos' in globals() and X_train_phos is not None:\n",
    "    Vh = X_train_phos.cpu().numpy().T               # [N_prot, P_train]\n",
    "    V  = np.concatenate([Vp, Vh], axis=1)           # multi-omics as in builder\n",
    "\n",
    "# cosine sim like the builder\n",
    "S = cosine_similarity(V)                             # [N, N]\n",
    "np.fill_diagonal(S, -np.inf)\n",
    "\n",
    "def topk_neighbors(i, k):\n",
    "    kk = min(k, S.shape[1]-1)\n",
    "    idx = np.argpartition(-S[i], kth=kk)[:kk]\n",
    "    return set(idx.tolist())\n",
    "\n",
    "def sym_topk_neighbors(i, k):\n",
    "    \"\"\"Neighbors after symmetrizing: j in topk(i) OR i in topk(j).\"\"\"\n",
    "    tki = topk_neighbors(i, k)\n",
    "    sym = set(tki)\n",
    "    for j in range(S.shape[0]):\n",
    "        if j == i: \n",
    "            continue\n",
    "        # is i among j's top-k?\n",
    "        kk = min(k, S.shape[1]-1)\n",
    "        idx_j = np.argpartition(-S[j], kth=kk)[:kk]\n",
    "        if i in idx_j:\n",
    "            sym.add(j)\n",
    "    return sym\n",
    "\n",
    "# neighbors from the built graph\n",
    "def graph_neighbors(i, edge_index):\n",
    "    ei = edge_index.numpy()\n",
    "    return set(ei[1, ei[0]==i].tolist())\n",
    "\n",
    "prot = 0                       # change as needed\n",
    "k_used = 15                    # the k you passed to the builder\n",
    "\n",
    "g_nbrs   = graph_neighbors(prot, ppi_edge_index)\n",
    "sym_nbrs = sym_topk_neighbors(prot, k_used)\n",
    "\n",
    "overlap  = g_nbrs & sym_nbrs\n",
    "print(f\"Node {prot}: graph_deg={len(g_nbrs)} sym_topk_deg={len(sym_nbrs)} overlap={len(overlap)}\")\n",
    "print(\"Jaccard(graph, sym_topk) =\",\n",
    "      len(overlap) / max(1, len(g_nbrs | sym_nbrs)))\n",
    "\n",
    "# (Optional) show the top-10 most similar indices and their sims\n",
    "top10 = np.argsort(-S[prot])[:10]\n",
    "print(\"Top-10 by cosine:\", top10, \"scores:\", np.round(S[prot, top10], 3))\n",
    "\n",
    "# (Optional) map to protein IDs for readability\n",
    "if 'protein_ids' in globals():\n",
    "    print(\"Top-10 protein IDs:\", [protein_ids[i] for i in top10])\n",
    "    print(\"Graph neighbors (IDs):\", [protein_ids[i] for i in sorted(g_nbrs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "31a1cb9b-4d7b-4eec-8e2f-6bd5d5c81281",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T03:57:38.455220Z",
     "iopub.status.busy": "2025-10-03T03:57:38.454863Z",
     "iopub.status.idle": "2025-10-03T03:57:38.472663Z",
     "shell.execute_reply": "2025-10-03T03:57:38.471160Z",
     "shell.execute_reply.started": "2025-10-03T03:57:38.455194Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "class MultiOmicsPatientDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, patient_ids, y,\n",
    "                 X_prot, X_phos, X_mask_prot,         # [P, N_prot]\n",
    "                 X_rna,  X_cnv,  X_rna_avl, X_cnv_avl, # [P, N_gene]\n",
    "                 mut_lists, protein_ids, gene_ids,\n",
    "                 ppi_edge_index, codes_edge_index, codes_rev_edge_index,\n",
    "                 use_masks: bool = True):\n",
    "        super().__init__()\n",
    "        self.pids = list(patient_ids)\n",
    "        self.y = y\n",
    "\n",
    "        # ---- basic shape guards\n",
    "        Pp, Np = X_prot.shape\n",
    "        assert X_phos.shape == (Pp, Np), \"X_phos must match X_prot shape\"\n",
    "        assert X_mask_prot.shape == (Pp, Np), \"X_mask_prot must match X_prot shape\"\n",
    "\n",
    "        Pg, Ng = X_rna.shape\n",
    "        assert Pg == Pp, \"X_rna must have same #patients as X_prot\"\n",
    "        assert X_cnv.shape      == (Pg, Ng), \"X_cnv must match X_rna shape\"\n",
    "        assert X_rna_avl.shape  == (Pg, Ng), \"X_rna_avl must match X_rna shape\"\n",
    "        assert X_cnv_avl.shape  == (Pg, Ng), \"X_cnv_avl must match X_rna shape\"\n",
    "        assert len(mut_lists)   == Pp,       \"mut_lists must align with patients\"\n",
    "\n",
    "        # ---- store (make contiguous for speed)\n",
    "        self.X_prot = X_prot.contiguous()\n",
    "        self.X_phos = X_phos.contiguous()\n",
    "        self.X_mask_prot = (X_mask_prot if use_masks else torch.zeros_like(X_prot)).contiguous()\n",
    "\n",
    "        self.X_rna = X_rna.contiguous()\n",
    "        self.X_cnv = X_cnv.contiguous()\n",
    "        self.X_rna_avl = (X_rna_avl if use_masks else torch.zeros_like(X_rna)).contiguous()\n",
    "        self.X_cnv_avl = (X_cnv_avl if use_masks else torch.zeros_like(X_cnv)).contiguous()\n",
    "\n",
    "        self.mut = mut_lists\n",
    "        self.prot_ids = list(protein_ids)\n",
    "        self.gene_ids = list(gene_ids)\n",
    "\n",
    "        # ---- static edges\n",
    "        self.ppi       = ppi_edge_index\n",
    "        self.codes     = codes_edge_index\n",
    "        self.codes_rev = codes_rev_edge_index\n",
    "\n",
    "        # edge dtype/range checks\n",
    "        for name, ei, n0, n1 in [\n",
    "            (\"ppi\",       self.ppi,       Np, Np),\n",
    "            (\"codes\",     self.codes,     Ng, Np),\n",
    "            (\"rev_codes\", self.codes_rev, Np, Ng),\n",
    "        ]:\n",
    "            assert ei.dtype == torch.long and ei.dim() == 2 and ei.size(0) == 2, f\"{name} must be [2,E] long\"\n",
    "            if ei.numel() > 0:\n",
    "                max0 = int(ei[0].max())\n",
    "                max1 = int(ei[1].max())\n",
    "                assert max0 < n0 and max1 < n1, f\"{name} edge index out of range\"\n",
    "\n",
    "        self.nP = Np  # #protein nodes\n",
    "        self.nG = Ng  # #gene nodes\n",
    "\n",
    "        # ---- static template graph (copied per __getitem__)\n",
    "        self.template = HeteroData()\n",
    "        self.template['protein'].x = torch.zeros(self.nP, 3)  # prot_z, phos_z, prot_missing_mask\n",
    "        self.template['gene'].x    = torch.zeros(self.nG, 4)  # rna_z, cnv_z, rna_avl, cnv_avl\n",
    "        self.template['patient'].x = torch.zeros(1, 1)\n",
    "\n",
    "        self.template[('protein','ppi','protein')].edge_index   = self.ppi\n",
    "        self.template[('gene','codes','protein')].edge_index    = self.codes\n",
    "        self.template[('protein','rev_codes','gene')].edge_index= self.codes_rev\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pids)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        g = self.template.clone()\n",
    "\n",
    "        # protein channels\n",
    "        g['protein'].x[:, 0] = self.X_prot[i]\n",
    "        g['protein'].x[:, 1] = self.X_phos[i]\n",
    "        g['protein'].x[:, 2] = self.X_mask_prot[i]\n",
    "\n",
    "        # gene channels\n",
    "        g['gene'].x[:, 0] = self.X_rna[i]\n",
    "        g['gene'].x[:, 1] = self.X_cnv[i]\n",
    "        g['gene'].x[:, 2] = self.X_rna_avl[i]\n",
    "        g['gene'].x[:, 3] = self.X_cnv_avl[i]\n",
    "\n",
    "        # patient↔protein mutation edges (sparse, per-patient)\n",
    "        mi = self.mut[i]\n",
    "        if isinstance(mi, np.ndarray):\n",
    "            mi = mi.tolist()\n",
    "        if len(mi) > 0:\n",
    "            src = torch.zeros(len(mi), dtype=torch.long)     # single patient node index 0\n",
    "            dst = torch.tensor(mi, dtype=torch.long)         # protein indices\n",
    "            g[('patient','mutated','protein')].edge_index   = torch.stack([src, dst], dim=0)\n",
    "            g[('protein','rev_mutated','patient')].edge_index = torch.stack([dst, src], dim=0)\n",
    "        else:\n",
    "            g[('patient','mutated','protein')].edge_index     = torch.empty((2,0), dtype=torch.long)\n",
    "            g[('protein','rev_mutated','patient')].edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "\n",
    "        g['patient'].y = torch.tensor([int(self.y[i].item())], dtype=torch.long)\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "805c39be-ddde-4924-aa6b-296985838b82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T03:58:10.968030Z",
     "iopub.status.busy": "2025-10-03T03:58:10.967718Z",
     "iopub.status.idle": "2025-10-03T03:58:10.975137Z",
     "shell.execute_reply": "2025-10-03T03:58:10.974057Z",
     "shell.execute_reply.started": "2025-10-03T03:58:10.968008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protein feat dims: torch.Size([6259, 3])\n",
      "gene feat dims: torch.Size([6250, 4])\n",
      "ppi edges: 159842\n",
      "codes edges: 6250\n",
      "mut edges: 19\n",
      "label: 1\n"
     ]
    }
   ],
   "source": [
    "g0 = train_ds[0]\n",
    "print(\"protein feat dims:\", g0['protein'].x.shape)\n",
    "print(\"gene feat dims:\",    g0['gene'].x.shape)\n",
    "print(\"ppi edges:\",         g0[('protein','ppi','protein')].edge_index.shape[1])\n",
    "print(\"codes edges:\",       g0[('gene','codes','protein')].edge_index.shape[1])\n",
    "print(\"mut edges:\",         g0[('patient','mutated','protein')].edge_index.shape[1])\n",
    "print(\"label:\",             g0['patient'].y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9b8a132d-5e3d-47a9-aeb0-d979735e5b28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T03:59:39.175310Z",
     "iopub.status.busy": "2025-10-03T03:59:39.174787Z",
     "iopub.status.idle": "2025-10-03T03:59:39.187514Z",
     "shell.execute_reply": "2025-10-03T03:59:39.186249Z",
     "shell.execute_reply.started": "2025-10-03T03:59:39.175284Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiOmicsKGNN(nn.Module):\n",
    "    def __init__(self, in_protein=3, in_gene=4, in_patient=1, hidden=128, n_classes=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lin_prot = nn.Linear(in_protein, hidden)\n",
    "        self.lin_gene = nn.Linear(in_gene, hidden)\n",
    "        self.lin_pat  = nn.Linear(in_patient, hidden)\n",
    "\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('protein','ppi','protein'):           SAGEConv((-1, -1), hidden),\n",
    "            ('gene','codes','protein'):            SAGEConv((-1, -1), hidden),\n",
    "            ('protein','rev_codes','gene'):        SAGEConv((-1, -1), hidden),\n",
    "            ('patient','mutated','protein'):       SAGEConv((-1, -1), hidden),\n",
    "            ('protein','rev_mutated','patient'):   SAGEConv((-1, -1), hidden),\n",
    "        }, aggr='sum')\n",
    "\n",
    "        self.conv2 = HeteroConv({\n",
    "            ('protein','ppi','protein'):           SAGEConv((-1, -1), hidden),\n",
    "            ('gene','codes','protein'):            SAGEConv((-1, -1), hidden),\n",
    "            ('protein','rev_codes','gene'):        SAGEConv((-1, -1), hidden),\n",
    "            ('patient','mutated','protein'):       SAGEConv((-1, -1), hidden),\n",
    "            ('protein','rev_mutated','patient'):   SAGEConv((-1, -1), hidden),\n",
    "        }, aggr='sum')\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lin_fuse = nn.Linear(3 * hidden, hidden)  # concat patient + protein + gene\n",
    "        self.cls = nn.Linear(hidden, n_classes)\n",
    "\n",
    "    def forward(self, g: HeteroData):\n",
    "        # Project raw features to hidden\n",
    "        x = {\n",
    "            'protein': F.relu(self.lin_prot(g['protein'].x)),\n",
    "            'gene':    F.relu(self.lin_gene(g['gene'].x)),\n",
    "            'patient': F.relu(self.lin_pat(g['patient'].x)),\n",
    "        }\n",
    "\n",
    "        # Two hetero SAGE layers\n",
    "        x = self.conv1(x, g.edge_index_dict); x = {k: F.relu(v) for k, v in x.items()}\n",
    "        x = self.conv2(x, g.edge_index_dict); x = {k: F.relu(v) for k, v in x.items()}\n",
    "\n",
    "        # Batch-aware pooling\n",
    "        def bvec(name):\n",
    "            return g[name].batch if 'batch' in g[name] else torch.zeros(\n",
    "                x[name].size(0), dtype=torch.long, device=x[name].device)\n",
    "\n",
    "        p_batch = bvec('protein')\n",
    "        g_batch = bvec('gene')\n",
    "        t_batch = bvec('patient')\n",
    "\n",
    "        z_prot = global_mean_pool(x['protein'], p_batch)   # [B, H]\n",
    "        z_gene = global_mean_pool(x['gene'],    g_batch)   # [B, H]\n",
    "        z_pat  = global_mean_pool(x['patient'], t_batch)   # [B, H]\n",
    "\n",
    "        z = torch.cat([z_pat, z_prot, z_gene], dim=-1)     # [B, 3H]\n",
    "        z = self.dropout(F.relu(self.lin_fuse(z)))\n",
    "        logits = self.cls(z)                                # [B, C]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bad7c42e-0600-438a-b837-f45a89744068",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T04:01:08.111918Z",
     "iopub.status.busy": "2025-10-03T04:01:08.111581Z",
     "iopub.status.idle": "2025-10-03T04:01:09.076953Z",
     "shell.execute_reply": "2025-10-03T04:01:09.075533Z",
     "shell.execute_reply.started": "2025-10-03T04:01:08.111894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mut_lists] patients=76 | with_any=76 | avg_muted_proteins_per_patient=205.4\n",
      "Coverage: mutation columns matched = 76/76 patients | row overlap (mut→protein base ids with any 1s) = 5407/7102\n"
     ]
    }
   ],
   "source": [
    "## ---------- Build mut_lists from gene-level binary (robust to Ensembl versions) ----------\n",
    "\n",
    "def ensg_base(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    m = re.match(r'^(ENSG[0-9]+)', s)   # keep only ENSG base (drop .version)\n",
    "    return m.group(1) if m else s       # if not ENSG, return as-is (e.g., TP53)\n",
    "\n",
    "# 1) Load\n",
    "mut_url = URLS[\"mut_gene_bin\"]\n",
    "mut_df = pd.read_csv(mut_url, sep=\"\\t\", header=0)\n",
    "\n",
    "# 2) Identify gene column (first col) and normalize to ENSG base\n",
    "gene_col = mut_df.columns[0]\n",
    "mut_df = mut_df.rename(columns={gene_col: \"gene\"})\n",
    "mut_df[\"gene_base\"] = mut_df[\"gene\"].map(ensg_base)\n",
    "mut_df = mut_df.drop(columns=[\"gene\"]).set_index(\"gene_base\")\n",
    "\n",
    "# 2b) Coerce mutation values to numeric and collapse duplicate patient columns (if any)\n",
    "mut_df = mut_df.apply(pd.to_numeric, errors=\"coerce\")      # non-numeric -> NaN\n",
    "mut_df = mut_df.fillna(0)                                  # treat NaN as 0 (no mutation)\n",
    "if mut_df.columns.has_duplicates:\n",
    "    mut_df = mut_df.T.groupby(level=0).max(numeric_only=True).T  # binary OR across dup cols\n",
    "\n",
    "# 3) Collapse duplicate Ensembl rows (binary OR across dup rows)\n",
    "if mut_df.index.has_duplicates:\n",
    "    mut_df = mut_df.groupby(level=0).max(numeric_only=True)\n",
    "\n",
    "# 4) Normalize current protein node IDs to the same base\n",
    "protein_ids_base = [ensg_base(g) for g in protein_ids]\n",
    "prot_base_to_idx = {}\n",
    "for i, b in enumerate(protein_ids_base):\n",
    "    # keep first occurrence so union order -> node index is stable\n",
    "    if b not in prot_base_to_idx:\n",
    "        prot_base_to_idx[b] = i\n",
    "\n",
    "# 5) Keep only mutation columns for our current patient_ids (preserve order)\n",
    "mut_cols = [p for p in patient_ids if p in mut_df.columns]\n",
    "mut_df = mut_df.reindex(columns=mut_cols)\n",
    "\n",
    "# 6) Align mutation rows to *base* protein IDs; missing -> 0\n",
    "mut_df = mut_df.reindex(index=list(prot_base_to_idx.keys())).fillna(0)\n",
    "\n",
    "# 7) Ensure strictly binary int8\n",
    "mut_df = (mut_df > 0).astype(np.int8)\n",
    "\n",
    "# 8) Build mut_lists in EXACT patient_ids order (empty if patient not present in file)\n",
    "present_cols = set(mut_df.columns)\n",
    "col_pos = {c:i for i, c in enumerate(mut_cols)}  # avoid O(N) .index() calls in loop\n",
    "\n",
    "mut_lists = []\n",
    "for p in patient_ids:\n",
    "    if p in present_cols:\n",
    "        j = col_pos[p]\n",
    "        prot_indices = np.nonzero(mut_df.iloc[:, j].to_numpy(dtype=bool))[0].astype(np.int64)\n",
    "        # map row order (base ID) -> protein node index\n",
    "        base_hits = mut_df.index[prot_indices].tolist()\n",
    "        idx_hits = np.array([prot_base_to_idx[b] for b in base_hits if b in prot_base_to_idx], dtype=np.int64)\n",
    "        mut_lists.append(idx_hits)\n",
    "    else:\n",
    "        mut_lists.append(np.array([], dtype=np.int64))\n",
    "\n",
    "# 9) Diagnostics + sanity checks\n",
    "n_with_any = sum(arr.size > 0 for arr in mut_lists)\n",
    "avg_muts   = float(np.mean([arr.size for arr in mut_lists])) if mut_lists else 0.0\n",
    "\n",
    "print(\n",
    "    f\"[mut_lists] patients={len(mut_lists)} | with_any={n_with_any} | \"\n",
    "    f\"avg_muted_proteins_per_patient={avg_muts:.1f}\"\n",
    ")\n",
    "print(\n",
    "    \"Coverage:\",\n",
    "    f\"mutation columns matched = {len(mut_cols)}/{len(patient_ids)} patients | \"\n",
    "    f\"row overlap (mut→protein base ids with any 1s) = \"\n",
    "    f\"{int((mut_df.sum(axis=1)>0).sum())}/{len(prot_base_to_idx)}\"\n",
    ")\n",
    "\n",
    "assert len(mut_lists) == len(patient_ids), \"mut_lists must align 1:1 with patient_ids\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "786bf00f-cd41-461c-b80c-0c4411ae8d28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T04:02:55.786917Z",
     "iopub.status.busy": "2025-10-03T04:02:55.786614Z",
     "iopub.status.idle": "2025-10-03T04:02:55.808506Z",
     "shell.execute_reply": "2025-10-03T04:02:55.807619Z",
     "shell.execute_reply.started": "2025-10-03T04:02:55.786895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protein feat dims: torch.Size([7102, 3])\n",
      "gene feat dims: torch.Size([7093, 4])\n",
      "ppi edges: 190398\n",
      "codes edges: 7093\n"
     ]
    }
   ],
   "source": [
    "# ---------- Loaders ----------\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def take_rows(X, idx): \n",
    "    return X[idx]\n",
    "\n",
    "# Split tensors by your precomputed indices\n",
    "X_prot_tr, X_prot_va, X_prot_te = take_rows(X_prot, train_idx), take_rows(X_prot, val_idx), take_rows(X_prot, test_idx)\n",
    "X_phos_tr, X_phos_va, X_phos_te = take_rows(X_phos, train_idx), take_rows(X_phos, val_idx), take_rows(X_phos, test_idx)\n",
    "X_mprt_tr, X_mprt_va, X_mprt_te = take_rows(X_mask_prot, train_idx), take_rows(X_mask_prot, val_idx), take_rows(X_mask_prot, test_idx)\n",
    "\n",
    "X_rna_tr,  X_rna_va,  X_rna_te  = take_rows(X_rna, train_idx),  take_rows(X_rna, val_idx),  take_rows(X_rna, test_idx)\n",
    "X_cnv_tr,  X_cnv_va,  X_cnv_te  = take_rows(X_cnv, train_idx),  take_rows(X_cnv, val_idx),  take_rows(X_cnv, test_idx)\n",
    "X_ravl_tr, X_ravl_va, X_ravl_te = take_rows(X_rna_avl, train_idx), take_rows(X_rna_avl, val_idx), take_rows(X_rna_avl, test_idx)\n",
    "X_cavl_tr, X_cavl_va, X_cavl_te = take_rows(X_cnv_avl, train_idx), take_rows(X_cnv_avl, val_idx), take_rows(X_cnv_avl, test_idx)\n",
    "\n",
    "y_tr, y_va, y_te = labels_aligned[train_idx], labels_aligned[val_idx], labels_aligned[test_idx]\n",
    "pids_tr = [patient_ids[i] for i in train_idx]\n",
    "pids_va = [patient_ids[i] for i in val_idx]\n",
    "pids_te = [patient_ids[i] for i in test_idx]\n",
    "\n",
    "train_ds = MultiOmicsPatientDataset(\n",
    "    pids_tr, y_tr,\n",
    "    X_prot_tr, X_phos_tr, X_mprt_tr,\n",
    "    X_rna_tr,  X_cnv_tr,  X_ravl_tr, X_cavl_tr,\n",
    "    [mut_lists[i] for i in train_idx],\n",
    "    protein_ids, gene_ids,\n",
    "    ppi_edge_index, codes_edge_index, codes_rev_edge_index\n",
    ")\n",
    "val_ds = MultiOmicsPatientDataset(\n",
    "    pids_va, y_va,\n",
    "    X_prot_va, X_phos_va, X_mprt_va,\n",
    "    X_rna_va,  X_cnv_va,  X_ravl_va, X_cavl_va,\n",
    "    [mut_lists[i] for i in val_idx],\n",
    "    protein_ids, gene_ids,\n",
    "    ppi_edge_index, codes_edge_index, codes_rev_edge_index\n",
    ")\n",
    "test_ds = MultiOmicsPatientDataset(\n",
    "    pids_te, y_te,\n",
    "    X_prot_te, X_phos_te, X_mprt_te,\n",
    "    X_rna_te,  X_cnv_te,  X_ravl_te, X_cavl_te,\n",
    "    [mut_lists[i] for i in test_idx],\n",
    "    protein_ids, gene_ids,\n",
    "    ppi_edge_index, codes_edge_index, codes_rev_edge_index\n",
    ")\n",
    "\n",
    "# Smaller batch size is safer; set num_workers=0 in notebooks\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True,  num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "val_loader   = DataLoader(val_ds,   batch_size=8, shuffle=False, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "test_loader  = DataLoader(test_ds,  batch_size=8, shuffle=False, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# Quick sanity\n",
    "g0 = train_ds[0]\n",
    "print(\"protein feat dims:\", g0['protein'].x.shape)  # (N_prot, 3)\n",
    "print(\"gene feat dims:\",    g0['gene'].x.shape)     # (N_gene, 4)\n",
    "print(\"ppi edges:\",         g0[('protein','ppi','protein')].edge_index.shape[1])\n",
    "print(\"codes edges:\",       g0[('gene','codes','protein')].edge_index.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "74f94d32-b12b-4def-9018-f6ed6fc6c631",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T04:13:12.840947Z",
     "iopub.status.busy": "2025-10-03T04:13:12.840631Z",
     "iopub.status.idle": "2025-10-03T04:13:12.847881Z",
     "shell.execute_reply": "2025-10-03T04:13:12.847038Z",
     "shell.execute_reply.started": "2025-10-03T04:13:12.840925Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# Build balanced sampler from training labels (y_tr is a 1D tensor of class ids)\n",
    "cls_counts = np.bincount(y_tr.cpu().numpy(), minlength=int(y_tr.max().item())+1)\n",
    "cls_weights = {c: 1.0 / cnt for c, cnt in enumerate(cls_counts) if cnt > 0}\n",
    "sample_weights = torch.tensor([cls_weights[int(c.item())] for c in y_tr], dtype=torch.double)\n",
    "\n",
    "# Optional: reproducible sampling\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "\n",
    "train_sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),   # one epoch ~ same length as dataset\n",
    "    replacement=True,\n",
    "    generator=g\n",
    ")\n",
    "\n",
    "# DataLoaders (note: no shuffle when sampler is set)\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=8, sampler=train_sampler,\n",
    "    num_workers=0, pin_memory=torch.cuda.is_available(), drop_last=False\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=8, shuffle=False,\n",
    "    num_workers=0, pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=8, shuffle=False,\n",
    "    num_workers=0, pin_memory=torch.cuda.is_available()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "70ef9cdd-ea1f-4019-981d-2b2bb92150bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T04:13:15.508755Z",
     "iopub.status.busy": "2025-10-03T04:13:15.508428Z",
     "iopub.status.idle": "2025-10-03T04:13:18.376138Z",
     "shell.execute_reply": "2025-10-03T04:13:18.375353Z",
     "shell.execute_reply.started": "2025-10-03T04:13:15.508729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: (8, 4)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_classes = int(labels_aligned.max().item() + 1)\n",
    "model = MultiOmicsKGNN(in_protein=3, in_gene=4, in_patient=1, hidden=128, n_classes=n_classes).to(device)\n",
    "\n",
    "# Try a forward pass\n",
    "g_test = next(iter(train_loader)).to(device)\n",
    "with torch.no_grad():\n",
    "    out = model(g_test)\n",
    "print(\"Logits shape:\", tuple(out.shape))  # should be (batch_size, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "48186fc4-425b-4e05-86cb-e6bc691f7aa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T04:13:23.961410Z",
     "iopub.status.busy": "2025-10-03T04:13:23.960890Z",
     "iopub.status.idle": "2025-10-03T04:13:23.975371Z",
     "shell.execute_reply": "2025-10-03T04:13:23.974348Z",
     "shell.execute_reply.started": "2025-10-03T04:13:23.961380Z"
    }
   },
   "outputs": [],
   "source": [
    "# ---------- Model ----------\n",
    "#import torch.nn.functional as F\n",
    "#from torch_geometric.nn import HeteroConv, SAGEConv, global_mean_pool\n",
    "\n",
    "class MultiOmicsKGNN(nn.Module):\n",
    "    def __init__(self, in_protein=3, in_gene=4, in_patient=1, hidden=128, n_classes=4, p_drop=0.2):\n",
    "        super().__init__()\n",
    "        self.lin_prot = nn.Linear(in_protein, hidden)\n",
    "        self.lin_gene = nn.Linear(in_gene, hidden)\n",
    "        self.lin_pat  = nn.Linear(in_patient, hidden)\n",
    "\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('protein','ppi','protein'):           SAGEConv((-1,-1), hidden),\n",
    "            ('gene','codes','protein'):            SAGEConv((-1,-1), hidden),\n",
    "            ('protein','rev_codes','gene'):        SAGEConv((-1,-1), hidden),\n",
    "            ('patient','mutated','protein'):       SAGEConv((-1,-1), hidden),\n",
    "            ('protein','rev_mutated','patient'):   SAGEConv((-1,-1), hidden),\n",
    "        }, aggr='sum')\n",
    "\n",
    "        self.conv2 = HeteroConv({\n",
    "            ('protein','ppi','protein'):           SAGEConv((-1,-1), hidden),\n",
    "            ('gene','codes','protein'):            SAGEConv((-1,-1), hidden),\n",
    "            ('protein','rev_codes','gene'):        SAGEConv((-1,-1), hidden),\n",
    "            ('patient','mutated','protein'):       SAGEConv((-1,-1), hidden),\n",
    "            ('protein','rev_mutated','patient'):   SAGEConv((-1,-1), hidden),\n",
    "        }, aggr='sum')\n",
    "\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "        self.lin_fuse = nn.Linear(3*hidden, hidden)\n",
    "        self.cls = nn.Linear(hidden, n_classes)\n",
    "\n",
    "    def forward(self, g: HeteroData):\n",
    "        x = {\n",
    "            'protein': F.relu(self.lin_prot(g['protein'].x)),\n",
    "            'gene':    F.relu(self.lin_gene(g['gene'].x)),\n",
    "            'patient': F.relu(self.lin_pat(g['patient'].x)),\n",
    "        }\n",
    "        x = self.conv1(x, g.edge_index_dict); x = {k: self.dropout(F.relu(v)) for k,v in x.items()}\n",
    "        x = self.conv2(x, g.edge_index_dict); x = {k: self.dropout(F.relu(v)) for k,v in x.items()}\n",
    "\n",
    "        def bvec(tname):\n",
    "            return g[tname].batch if 'batch' in g[tname] else torch.zeros(\n",
    "                x[tname].size(0), dtype=torch.long, device=x[tname].device)\n",
    "\n",
    "        z_prot = global_mean_pool(x['protein'], bvec('protein'))\n",
    "        z_gene = global_mean_pool(x['gene'],    bvec('gene'))\n",
    "        z_pat  = global_mean_pool(x['patient'], bvec('patient'))\n",
    "\n",
    "        z = torch.cat([z_pat, z_prot, z_gene], dim=-1)\n",
    "        z = self.dropout(F.relu(self.lin_fuse(z)))\n",
    "        return self.cls(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942cea4-455a-4e53-9270-374087ceed13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T04:15:37.252820Z",
     "iopub.status.busy": "2025-10-03T04:15:37.252510Z",
     "iopub.status.idle": "2025-10-03T04:35:02.097716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train 1.338/0.200 | val 1.471/0.188 | macroF1 0.079 balAcc 0.250\n",
      "Epoch 02 | train 1.231/0.311 | val 1.592/0.188 | macroF1 0.079 balAcc 0.250\n",
      "Epoch 03 | train 1.301/0.267 | val 1.454/0.188 | macroF1 0.079 balAcc 0.250\n",
      "Epoch 04 | train 1.368/0.200 | val 1.381/0.188 | macroF1 0.079 balAcc 0.250\n",
      "Epoch 05 | train 1.312/0.267 | val 1.393/0.188 | macroF1 0.079 balAcc 0.250\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CMS1      0.188     1.000     0.316         3\n",
      "        CMS2      0.000     0.000     0.000         6\n",
      "        CMS3      0.000     0.000     0.000         3\n",
      "        CMS4      0.000     0.000     0.000         4\n",
      "\n",
      "    accuracy                          0.188        16\n",
      "   macro avg      0.047     0.250     0.079        16\n",
      "weighted avg      0.035     0.188     0.059        16\n",
      "\n",
      "Confusion matrix (val):\n",
      " [[3 0 0 0]\n",
      " [6 0 0 0]\n",
      " [3 0 0 0]\n",
      " [4 0 0 0]]\n",
      "Epoch 06 | train 1.361/0.156 | val 1.405/0.375 | macroF1 0.275 balAcc 0.500\n",
      "Epoch 07 | train 1.349/0.267 | val 1.413/0.188 | macroF1 0.079 balAcc 0.250\n",
      "Epoch 08 | train 1.388/0.178 | val 1.382/0.188 | macroF1 0.079 balAcc 0.250\n",
      "Epoch 09 | train 1.296/0.267 | val 1.380/0.188 | macroF1 0.079 balAcc 0.250\n",
      "Epoch 10 | train 1.379/0.156 | val 1.387/0.188 | macroF1 0.094 balAcc 0.250\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CMS1      0.000     0.000     0.000         3\n",
      "        CMS2      0.000     0.000     0.000         6\n",
      "        CMS3      0.231     1.000     0.375         3\n",
      "        CMS4      0.000     0.000     0.000         4\n",
      "\n",
      "    accuracy                          0.188        16\n",
      "   macro avg      0.058     0.250     0.094        16\n",
      "weighted avg      0.043     0.188     0.070        16\n",
      "\n",
      "Confusion matrix (val):\n",
      " [[0 0 3 0]\n",
      " [0 0 6 0]\n",
      " [0 0 3 0]\n",
      " [3 0 1 0]]\n",
      "Epoch 11 | train 1.407/0.133 | val 1.373/0.188 | macroF1 0.079 balAcc 0.250\n",
      "Epoch 12 | train 1.329/0.222 | val 1.367/0.188 | macroF1 0.083 balAcc 0.250\n",
      "Epoch 13 | train 1.344/0.222 | val 1.373/0.188 | macroF1 0.079 balAcc 0.250\n",
      "Epoch 14 | train 1.333/0.222 | val 1.357/0.188 | macroF1 0.079 balAcc 0.250\n",
      "Epoch 15 | train 1.234/0.267 | val 1.357/0.188 | macroF1 0.079 balAcc 0.250\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CMS1      0.188     1.000     0.316         3\n",
      "        CMS2      0.000     0.000     0.000         6\n",
      "        CMS3      0.000     0.000     0.000         3\n",
      "        CMS4      0.000     0.000     0.000         4\n",
      "\n",
      "    accuracy                          0.188        16\n",
      "   macro avg      0.047     0.250     0.079        16\n",
      "weighted avg      0.035     0.188     0.059        16\n",
      "\n",
      "Confusion matrix (val):\n",
      " [[3 0 0 0]\n",
      " [6 0 0 0]\n",
      " [3 0 0 0]\n",
      " [4 0 0 0]]\n",
      "Epoch 16 | train 1.233/0.222 | val 1.372/0.188 | macroF1 0.079 balAcc 0.250\n",
      "Epoch 17 | train 1.234/0.244 | val 1.322/0.312 | macroF1 0.238 balAcc 0.375\n",
      "Epoch 18 | train 1.022/0.511 | val 1.917/0.188 | macroF1 0.079 balAcc 0.250\n",
      "Epoch 19 | train 1.225/0.333 | val 1.389/0.375 | macroF1 0.369 balAcc 0.458\n",
      "Epoch 20 | train 1.114/0.467 | val 1.339/0.312 | macroF1 0.271 balAcc 0.375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CMS1      0.600     1.000     0.750         3\n",
      "        CMS2      0.000     0.000     0.000         6\n",
      "        CMS3      0.000     0.000     0.000         3\n",
      "        CMS4      0.250     0.500     0.333         4\n",
      "\n",
      "    accuracy                          0.312        16\n",
      "   macro avg      0.212     0.375     0.271        16\n",
      "weighted avg      0.175     0.312     0.224        16\n",
      "\n",
      "Confusion matrix (val):\n",
      " [[3 0 0 0]\n",
      " [0 0 2 4]\n",
      " [1 0 0 2]\n",
      " [1 0 1 2]]\n",
      "Epoch 21 | train 1.110/0.489 | val 1.356/0.312 | macroF1 0.244 balAcc 0.375\n",
      "Epoch 22 | train 1.105/0.467 | val 1.439/0.312 | macroF1 0.244 balAcc 0.375\n",
      "Epoch 23 | train 1.001/0.511 | val 1.548/0.375 | macroF1 0.369 balAcc 0.458\n",
      "Epoch 24 | train 0.977/0.489 | val 1.638/0.312 | macroF1 0.264 balAcc 0.375\n",
      "Epoch 25 | train 1.018/0.511 | val 1.674/0.375 | macroF1 0.342 balAcc 0.417\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CMS1      0.600     1.000     0.750         3\n",
      "        CMS2      1.000     0.167     0.286         6\n",
      "        CMS3      0.000     0.000     0.000         3\n",
      "        CMS4      0.250     0.500     0.333         4\n",
      "\n",
      "    accuracy                          0.375        16\n",
      "   macro avg      0.463     0.417     0.342        16\n",
      "weighted avg      0.550     0.375     0.331        16\n",
      "\n",
      "Confusion matrix (val):\n",
      " [[3 0 0 0]\n",
      " [0 1 1 4]\n",
      " [1 0 0 2]\n",
      " [1 0 1 2]]\n",
      "Epoch 26 | train 0.965/0.533 | val 1.660/0.625 | macroF1 0.596 balAcc 0.625\n",
      "Epoch 27 | train 0.894/0.644 | val 1.634/0.625 | macroF1 0.488 balAcc 0.562\n",
      "Epoch 28 | train 0.855/0.667 | val 1.784/0.625 | macroF1 0.596 balAcc 0.625\n",
      "Epoch 29 | train 0.983/0.644 | val 1.521/0.625 | macroF1 0.523 balAcc 0.583\n",
      "Epoch 30 | train 0.829/0.711 | val 1.525/0.625 | macroF1 0.596 balAcc 0.625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CMS1      0.600     1.000     0.750         3\n",
      "        CMS2      1.000     0.667     0.800         6\n",
      "        CMS3      0.333     0.333     0.333         3\n",
      "        CMS4      0.500     0.500     0.500         4\n",
      "\n",
      "    accuracy                          0.625        16\n",
      "   macro avg      0.608     0.625     0.596        16\n",
      "weighted avg      0.675     0.625     0.628        16\n",
      "\n",
      "Confusion matrix (val):\n",
      " [[3 0 0 0]\n",
      " [0 4 1 1]\n",
      " [1 0 1 1]\n",
      " [1 0 1 2]]\n",
      "TEST  | loss 1.028 acc 0.467\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CMS1      0.600     1.000     0.750         3\n",
      "        CMS2      1.000     0.200     0.333         5\n",
      "        CMS3      0.250     0.333     0.286         3\n",
      "        CMS4      0.400     0.500     0.444         4\n",
      "\n",
      "    accuracy                          0.467        15\n",
      "   macro avg      0.562     0.508     0.453        15\n",
      "weighted avg      0.610     0.467     0.437        15\n",
      "\n",
      "Confusion matrix (test):\n",
      " [[3 0 0 0]\n",
      " [0 1 2 2]\n",
      " [1 0 1 1]\n",
      " [1 0 1 2]]\n"
     ]
    }
   ],
   "source": [
    "# ---------- Training with per-epoch classification report ----------\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score, f1_score, confusion_matrix\n",
    "import torch.nn as nn\n",
    "\n",
    "# Class names (align to label indices). If you have CMS names, you can set them here.\n",
    "n_classes = int(labels_aligned.max().item() + 1)\n",
    "try:\n",
    "    class_names = list(classes)  # from Cell 7 (e.g., [\"CMS1\",\"CMS2\",\"CMS3\",\"CMS4\"])\n",
    "    if len(class_names) != n_classes:\n",
    "        class_names = [f\"c{i}\" for i in range(n_classes)]\n",
    "except NameError:\n",
    "    class_names = [f\"c{i}\" for i in range(n_classes)]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate the model here\n",
    "model = MultiOmicsKGNN(\n",
    "    in_protein=3,   # prot_z, phos_z, prot_missing_mask\n",
    "    in_gene=4,      # rna_z, cnv_z, rna_avl, cnv_avl\n",
    "    in_patient=1,\n",
    "    hidden=128,\n",
    "    n_classes=n_classes\n",
    ").to(device)\n",
    "\n",
    "# Weighted loss from TRAIN ONLY (reuse y_tr from your split)\n",
    "cnt = Counter(y_tr.cpu().numpy().tolist())\n",
    "weights = torch.tensor([1.0 / max(cnt.get(i, 1), 1) for i in range(n_classes)],\n",
    "                       dtype=torch.float32, device=device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    all_y, all_p = [], []\n",
    "    tot_loss, n_batches = 0.0, 0\n",
    "    for g in loader:\n",
    "        g = g.to(device)\n",
    "        logits = model(g)\n",
    "        y = g['patient'].y.view(-1).to(device)\n",
    "        loss = criterion(logits, y)\n",
    "        tot_loss += float(loss.item()); n_batches += 1\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        all_y.extend(y.detach().cpu().numpy().tolist())\n",
    "        all_p.extend(pred.detach().cpu().numpy().tolist())\n",
    "    if n_batches == 0:\n",
    "        return 0.0, 0.0, np.array([]), np.array([])\n",
    "    avg_loss = tot_loss / n_batches\n",
    "    all_y, all_p = np.array(all_y), np.array(all_p)\n",
    "    acc = (all_y == all_p).mean() if all_y.size else 0.0\n",
    "    return avg_loss, acc, all_y, all_p\n",
    "\n",
    "def train_epoch(loader):\n",
    "    model.train()\n",
    "    tot_loss, correct, total, n_batches = 0.0, 0, 0, 0\n",
    "    for g in loader:\n",
    "        g = g.to(device)\n",
    "        logits = model(g)\n",
    "        y = g['patient'].y.view(-1).to(device)\n",
    "        loss = criterion(logits, y)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        tot_loss += float(loss.item()); n_batches += 1\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        correct += int((pred == y).sum().item())\n",
    "        total += y.size(0)\n",
    "    avg_loss = tot_loss / max(n_batches, 1)\n",
    "    acc = correct / max(total, 1)\n",
    "    return avg_loss, acc\n",
    "\n",
    "best_val_macro_f1, best_state = -1.0, None\n",
    "EPOCHS = 30\n",
    "REPORT_EVERY = 5  # print report every N epochs\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_acc = train_epoch(train_loader)\n",
    "    va_loss, va_acc, y_true, y_pred = evaluate(val_loader)\n",
    "\n",
    "    # Metrics robust to small val sets\n",
    "    if y_true.size > 0:\n",
    "        macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        bal_acc  = balanced_accuracy_score(y_true, y_pred) if len(np.unique(y_true)) > 1 else va_acc\n",
    "    else:\n",
    "        macro_f1, bal_acc = 0.0, 0.0\n",
    "\n",
    "    # Track best by macro-F1\n",
    "    if macro_f1 > best_val_macro_f1:\n",
    "        best_val_macro_f1 = macro_f1\n",
    "        best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"train {tr_loss:.3f}/{tr_acc:.3f} | \"\n",
    "          f\"val {va_loss:.3f}/{va_acc:.3f} | \"\n",
    "          f\"macroF1 {macro_f1:.3f} balAcc {bal_acc:.3f}\")\n",
    "\n",
    "    if (epoch % REPORT_EVERY == 0) and (y_true.size > 0):\n",
    "        print(classification_report(\n",
    "            y_true, y_pred,\n",
    "            labels=list(range(n_classes)),\n",
    "            target_names=class_names,\n",
    "            digits=3,\n",
    "            zero_division=0\n",
    "        ))\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=list(range(n_classes)))\n",
    "        print(\"Confusion matrix (val):\\n\", cm)\n",
    "\n",
    "# Load best and evaluate on test\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "te_loss, te_acc, y_true_te, y_pred_te = evaluate(test_loader)\n",
    "print(f\"TEST  | loss {te_loss:.3f} acc {te_acc:.3f}\")\n",
    "if y_true_te.size > 0:\n",
    "    print(classification_report(\n",
    "        y_true_te, y_pred_te,\n",
    "        labels=list(range(n_classes)),\n",
    "        target_names=class_names,\n",
    "        digits=3,\n",
    "        zero_division=0\n",
    "    ))\n",
    "    cm = confusion_matrix(y_true_te, y_pred_te, labels=list(range(n_classes)))\n",
    "    print(\"Confusion matrix (test):\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6cd83231-b6dd-452c-9e6b-479f45723480",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T01:46:54.982929Z",
     "iopub.status.busy": "2025-10-03T01:46:54.982459Z",
     "iopub.status.idle": "2025-10-03T01:46:56.286165Z",
     "shell.execute_reply": "2025-10-03T01:46:56.284946Z",
     "shell.execute_reply.started": "2025-10-03T01:46:54.982892Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000         1\n",
      "           1      0.500     1.000     0.667         1\n",
      "           2      1.000     1.000     1.000         1\n",
      "           3      0.000     0.000     0.000         1\n",
      "\n",
      "    accuracy                          0.750         4\n",
      "   macro avg      0.625     0.750     0.667         4\n",
      "weighted avg      0.625     0.750     0.667         4\n",
      "\n",
      "Confusion matrix:\n",
      " [[1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 1 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "model.eval()\n",
    "for g in test_loader:\n",
    "    g = g.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(g)\n",
    "    y_true.extend(g['patient'].y.view(-1).cpu().numpy().tolist())\n",
    "    y_pred.extend(logits.argmax(dim=-1).cpu().numpy().tolist())\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=3))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
