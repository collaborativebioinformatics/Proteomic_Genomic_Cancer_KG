{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a0ded3a-f8e1-4792-8c5e-6151bd845113",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:13:11.463231Z",
     "iopub.status.busy": "2025-10-03T16:13:11.462945Z",
     "iopub.status.idle": "2025-10-03T16:13:13.677894Z",
     "shell.execute_reply": "2025-10-03T16:13:13.676709Z",
     "shell.execute_reply.started": "2025-10-03T16:13:11.463208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-geometric in /opt/conda/lib/python3.12/site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.12/site-packages (from torch-geometric) (3.12.15)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch-geometric) (2024.12.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch-geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torch-geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.12/site-packages (from torch-geometric) (5.9.8)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.12/site-packages (from torch-geometric) (3.2.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from torch-geometric) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch-geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch-geometric) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch-geometric) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch-geometric) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch-geometric) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch-geometric) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch-geometric) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->torch-geometric) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /opt/conda/lib/python3.12/site-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.14.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch-geometric) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->torch-geometric) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->torch-geometric) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->torch-geometric) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d58dcd48-4d23-4071-a17c-822e9dca30d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:13:13.678970Z",
     "iopub.status.busy": "2025-10-03T16:13:13.678622Z",
     "iopub.status.idle": "2025-10-03T16:13:20.517672Z",
     "shell.execute_reply": "2025-10-03T16:13:20.516492Z",
     "shell.execute_reply.started": "2025-10-03T16:13:13.678925Z"
    }
   },
   "outputs": [],
   "source": [
    "import re, sys, math, json\n",
    "from io import StringIO\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv, global_mean_pool\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import io\n",
    "import gzip\n",
    "from io import StringIO\n",
    "from typing import Optional, Literal, Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb54680e-c859-4ddf-bc7c-a042e1adfe08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:13:20.520549Z",
     "iopub.status.busy": "2025-10-03T16:13:20.520067Z",
     "iopub.status.idle": "2025-10-03T16:13:20.528656Z",
     "shell.execute_reply": "2025-10-03T16:13:20.527596Z",
     "shell.execute_reply.started": "2025-10-03T16:13:20.520521Z"
    }
   },
   "outputs": [],
   "source": [
    "URLS = {\n",
    "    # Proteomics (gene-level)\n",
    "    \"prot_tumor\":  \"https://cptac-pancancer-data.s3.us-west-2.amazonaws.com/data_freeze_v1.2_reorganized/COAD/COAD_proteomics_gene_abundance_log2_reference_intensity_normalized_Tumor.txt\",\n",
    "    \"prot_norm\":   \"https://cptac-pancancer-data.s3.us-west-2.amazonaws.com/data_freeze_v1.2_reorganized/COAD/COAD_proteomics_gene_abundance_log2_reference_intensity_normalized_Normal.txt\",\n",
    "\n",
    "    # Phosphoproteomics (site-level)\n",
    "    \"phos_tumor\":  \"https://cptac-pancancer-data.s3.us-west-2.amazonaws.com/data_freeze_v1.2_reorganized/COAD/COAD_phospho_site_abundance_log2_reference_intensity_normalized_Tumor.txt\",\n",
    "    \"phos_norm\":   \"https://cptac-pancancer-data.s3.us-west-2.amazonaws.com/data_freeze_v1.2_reorganized/COAD/COAD_phospho_site_abundance_log2_reference_intensity_normalized_Normal.txt\",\n",
    "\n",
    "    # Mutations (gene-level binary), CNV, RNA\n",
    "    \"mut_gene_bin\":\"https://cptac-pancancer-data.s3.us-west-2.amazonaws.com/data_freeze_v1.2_reorganized/COAD/COAD_somatic_mutation_gene_level_binary.txt\",\n",
    "    \"cnv_log2\":    \"https://cptac-pancancer-data.s3.us-west-2.amazonaws.com/data_freeze_v1.2_reorganized/COAD/COAD_WES_CNV_gene_ratio_log2.txt\",\n",
    "    # alt CNV: GISTIC discrete\n",
    "    # \"cnv_gistic\":  \"https://cptac-pancancer-data.s3.us-west-2.amazonaws.com/data_freeze_v1.2_reorganized/COAD/COAD_WES_CNV_gene_gistic_level.txt\",\n",
    "\n",
    "    \"rna_gene_tumor\": \"https://cptac-pancancer-data.s3.us-west-2.amazonaws.com/data_freeze_v1.2_reorganized/COAD/COAD_RNAseq_gene_RSEM_coding_UQ_1500_log2_Tumor.txt\",\n",
    "\n",
    "    # CMS labels from Linkedomics clinical .tsi\n",
    "    \"tsi\": \"https://linkedomics.org/cptac-colon/Human__CPTAC_COAD__MS__Clinical__Clinical__03_01_2017__CPTAC__Clinical__BCM.tsi\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765f7e3e-de0e-481d-86a3-860265f6532c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:13:20.530494Z",
     "iopub.status.busy": "2025-10-03T16:13:20.530048Z",
     "iopub.status.idle": "2025-10-03T16:13:20.552283Z",
     "shell.execute_reply": "2025-10-03T16:13:20.551386Z",
     "shell.execute_reply.started": "2025-10-03T16:13:20.530461Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_tsv(url: str, index_col: int | None = 0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust TSV fetcher:\n",
    "      - Handles plain or gzipped content.\n",
    "      - Uses pandas dtype inference but preserves the index as string.\n",
    "      - Does not coerce to numeric here (we sanitize later, modality-specific).\n",
    "    \"\"\"\n",
    "    r = requests.get(url, timeout=180)\n",
    "    r.raise_for_status()\n",
    "    content = r.content\n",
    "    # Detect gzip by header\n",
    "    if content[:2] == b\"\\x1f\\x8b\":\n",
    "        buf = io.BytesIO(content)\n",
    "        with gzip.GzipFile(fileobj=buf, mode=\"rb\") as gz:\n",
    "            text = gz.read().decode(\"utf-8\", errors=\"replace\")\n",
    "        df = pd.read_csv(StringIO(text), sep=\"\\t\", header=0, low_memory=False)\n",
    "    else:\n",
    "        df = pd.read_csv(io.BytesIO(content), sep=\"\\t\", header=0, low_memory=False)\n",
    "\n",
    "    if index_col is not None:\n",
    "        # ensure index is string-like, then set as index\n",
    "        idx_name = df.columns[index_col]\n",
    "        df[idx_name] = df[idx_name].astype(str)\n",
    "        df = df.set_index(idx_name)\n",
    "    return df\n",
    "\n",
    "def strip_ensembl_version(x: str):\n",
    "    \"\"\"Drop trailing .version from Ensembl ids; no-op for non-Ensembl.\"\"\"\n",
    "    if isinstance(x, str) and x.startswith((\"ENS\", \"ens\")) and \".\" in x:\n",
    "        return x.split(\".\", 1)[0]\n",
    "    return x\n",
    "\n",
    "def clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Trim whitespace from column names, preserve order.\"\"\"\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def pick_valid_patients(id_list: List[str]) -> List[str]:\n",
    "    \"\"\"Keep non-empty, non-null patient IDs; trims whitespace.\"\"\"\n",
    "    keep = []\n",
    "    for p in id_list:\n",
    "        if isinstance(p, str):\n",
    "            s = p.strip()\n",
    "            if s and s.upper() not in {\"NA\", \"NAN\", \"NULL\"}:\n",
    "                keep.append(s)\n",
    "    return keep\n",
    "\n",
    "# ---------- Utilities you reuse later ----------\n",
    "\n",
    "def collapse_duplicate_rows(\n",
    "    df: pd.DataFrame,\n",
    "    how: Literal[\"median\", \"mean\", \"max_binary\"] = \"median\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collapse duplicate index rows:\n",
    "      - 'median' or 'mean' for continuous (proteo/phospho/RNA/CNV),\n",
    "      - 'max_binary' for mutation (0/1).\n",
    "    \"\"\"\n",
    "    if not df.index.has_duplicates:\n",
    "        return df\n",
    "    if how == \"max_binary\":\n",
    "        return df.groupby(level=0).max(numeric_only=True)\n",
    "    elif how == \"mean\":\n",
    "        return df.groupby(level=0).mean(numeric_only=True)\n",
    "    else:  # median default\n",
    "        return df.groupby(level=0).median(numeric_only=True)\n",
    "\n",
    "def sanitize_numeric(df: pd.DataFrame, clip_abs: float | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Make all entries numeric; non-numeric -> NaN; replace ±inf with NaN; optional clipping.\n",
    "    Use before KNN impute to avoid distance explosions.\n",
    "    \"\"\"\n",
    "    df2 = df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    df2 = df2.replace([np.inf, -np.inf], np.nan)\n",
    "    if clip_abs is not None:\n",
    "        df2 = df2.clip(lower=-clip_abs, upper=clip_abs)\n",
    "    return df2\n",
    "\n",
    "def knn_impute(df: pd.DataFrame, max_k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    KNN impute treating samples as rows (transpose inside).\n",
    "    Falls back to per-row median if samples < 2.\n",
    "    \"\"\"\n",
    "    from sklearn.impute import KNNImputer\n",
    "    n_samples = df.shape[1]\n",
    "    if n_samples < 2:\n",
    "        row_med = df.median(axis=1, skipna=True)\n",
    "        return df.apply(lambda col: col.fillna(row_med), axis=0)\n",
    "\n",
    "    k = min(max_k, max(1, n_samples - 1))\n",
    "    imp = KNNImputer(n_neighbors=k, weights=\"distance\")\n",
    "    vals = imp.fit_transform(df.T.values)  # [samples, features]\n",
    "    return pd.DataFrame(vals.T, index=df.index, columns=df.columns)\n",
    "\n",
    "def baseline_z_from_normals(\n",
    "    tumor_df: pd.DataFrame,\n",
    "    normal_df: pd.DataFrame,\n",
    "    clip: float = 5.0\n",
    ") -> tuple[pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Baseline-normalize tumor using normals only: (tumor - mean_norm) / std_norm.\n",
    "    Returns (Z, mu_norm, sd_norm). NaNs/±inf -> 0 after z, with clipping.\n",
    "    \"\"\"\n",
    "    mu = normal_df.mean(axis=1)\n",
    "    sd = normal_df.std(axis=1, ddof=0).replace(0, np.nan)\n",
    "    Z = (tumor_df.sub(mu, axis=0)).div(sd, axis=0)\n",
    "    Z = Z.replace([np.inf, -np.inf], np.nan).fillna(0.0).clip(-clip, clip)\n",
    "    return Z, mu, sd\n",
    "\n",
    "def z_by_train_only(\n",
    "    df_all: pd.DataFrame,\n",
    "    train_cols: List[str],\n",
    "    clip: float = 5.0\n",
    ") -> tuple[pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Train-only standardization: z-score each row using mean/std computed on TRAIN columns ONLY.\n",
    "    Returns (Z, mu_train, sd_train). NaNs/±inf -> 0 with clipping.\n",
    "    \"\"\"\n",
    "    mu = df_all[train_cols].mean(axis=1)\n",
    "    sd = df_all[train_cols].std(axis=1, ddof=0).replace(0, np.nan)\n",
    "    Z = (df_all.sub(mu, axis=0)).div(sd, axis=0)\n",
    "    Z = Z.replace([np.inf, -np.inf], np.nan).fillna(0.0).clip(-clip, clip)\n",
    "    return Z, mu, sd\n",
    "\n",
    "def parse_cms_from_tsi(url: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse the Linkedomics .tsi and return {patient_id: CMS_label} for 'Transcriptomic_subtype'.\n",
    "    The .tsi uses a wide format: first column is 'attrib_name', subsequent columns are patients.\n",
    "    \"\"\"\n",
    "    df = fetch_tsv(url, index_col=0)\n",
    "    df.index = df.index.astype(str)\n",
    "    # Choose the transcriptomic subtype row robustly\n",
    "    row_key = None\n",
    "    candidates = [idx for idx in df.index if \"Transcriptomic_subtype\" in str(idx)]\n",
    "    if candidates:\n",
    "        row_key = candidates[0]\n",
    "    else:\n",
    "        # fallback: any row containing CMS-like labels\n",
    "        for idx in df.index:\n",
    "            vals = set(str(v) for v in df.loc[idx].values)\n",
    "            if any(v.startswith(\"CMS\") for v in vals):\n",
    "                row_key = idx\n",
    "                break\n",
    "    if row_key is None:\n",
    "        return {}\n",
    "\n",
    "    row = df.loc[row_key]\n",
    "    # Build mapping: keep only non-empty CMS labels\n",
    "    cms_map = {}\n",
    "    for pid, lab in row.items():\n",
    "        if isinstance(lab, str):\n",
    "            lab_s = lab.strip()\n",
    "            if lab_s and lab_s.upper() != \"NA\":\n",
    "                cms_map[pid.strip()] = lab_s\n",
    "    return cms_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feeea0e1-df5e-483d-b148-22c27a0b1c66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:13:20.555590Z",
     "iopub.status.busy": "2025-10-03T16:13:20.555254Z",
     "iopub.status.idle": "2025-10-03T16:13:20.568926Z",
     "shell.execute_reply": "2025-10-03T16:13:20.567383Z",
     "shell.execute_reply.started": "2025-10-03T16:13:20.555564Z"
    }
   },
   "outputs": [],
   "source": [
    "def phospho_index_to_gene(phos_idx: pd.Index) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Map phosphosite row IDs to gene identifiers.\n",
    "    Tries, in order:\n",
    "      - Regex search for Ensembl ID (ENSG...).\n",
    "      - Split on delimiters and look for ENSG or plausible gene symbol.\n",
    "    Returns a Series of gene IDs aligned to phos_idx.\n",
    "    \"\"\"\n",
    "    genes = []\n",
    "    for s in phos_idx.astype(str):\n",
    "        gene = None\n",
    "        # Prefer explicit Ensembl IDs\n",
    "        m = re.search(r\"(ENSG[0-9]+(?:\\.[0-9]+)?)\", s)\n",
    "        if m:\n",
    "            gene = strip_ensembl_version(m.group(1))\n",
    "        else:\n",
    "            parts = re.split(r\"[|,;:_\\s]+\", s)\n",
    "            for p in parts:\n",
    "                if p.startswith(\"ENSG\"):\n",
    "                    gene = strip_ensembl_version(p)\n",
    "                    break\n",
    "                if re.fullmatch(r\"[A-Za-z][A-Za-z0-9\\-]{0,20}\", p):\n",
    "                    gene = p.upper()\n",
    "                    break\n",
    "        genes.append(gene)\n",
    "    return pd.Series(genes, index=phos_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca465f83-3e29-47be-a1c6-bbf834d879de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:13:20.570881Z",
     "iopub.status.busy": "2025-10-03T16:13:20.570273Z",
     "iopub.status.idle": "2025-10-03T16:13:36.300693Z",
     "shell.execute_reply": "2025-10-03T16:13:36.299704Z",
     "shell.execute_reply.started": "2025-10-03T16:13:20.570845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prot_tumor (9151, 97) | prot_norm (9152, 100) (dup rows removed: T=0, N=0)\n",
      "phos_tumor (35487, 97) | phos_norm (35485, 100) (site-level; will aggregate to genes later)\n",
      "mut_bin    (14783, 96) (dup rows removed: 0)\n",
      "rna_tumor  (60624, 106) (dup rows removed: 45)\n",
      "cnv_log2   (60558, 105) (dup rows removed: 45)\n"
     ]
    }
   ],
   "source": [
    "# Proteomics (gene-level)\n",
    "prot_tumor = fetch_tsv(URLS[\"prot_tumor\"], index_col=0)\n",
    "prot_norm  = fetch_tsv(URLS[\"prot_norm\"],  index_col=0)\n",
    "prot_tumor.index = prot_tumor.index.map(strip_ensembl_version)\n",
    "prot_norm.index  = prot_norm.index.map(strip_ensembl_version)\n",
    "prot_tumor = clean_cols(prot_tumor)\n",
    "prot_norm  = clean_cols(prot_norm)\n",
    "\n",
    "# Phosphoproteomics (site-level) — stays site-level for now; aggregation happens later\n",
    "phos_tumor = fetch_tsv(URLS[\"phos_tumor\"], index_col=0)\n",
    "phos_norm  = fetch_tsv(URLS[\"phos_norm\"],  index_col=0)\n",
    "phos_tumor = clean_cols(phos_tumor)\n",
    "phos_norm  = clean_cols(phos_norm)\n",
    "\n",
    "# Mutations (gene-level binary)\n",
    "mut_bin = fetch_tsv(URLS[\"mut_gene_bin\"], index_col=0)\n",
    "mut_bin.index = mut_bin.index.map(strip_ensembl_version)\n",
    "mut_bin = clean_cols(mut_bin)\n",
    "\n",
    "# RNA (gene-level, tumor)\n",
    "rna_tumor = fetch_tsv(URLS[\"rna_gene_tumor\"], index_col=0)\n",
    "rna_tumor.index = rna_tumor.index.map(strip_ensembl_version)\n",
    "rna_tumor = clean_cols(rna_tumor)\n",
    "\n",
    "# CNV (gene-level log2 ratio, tumor)\n",
    "cnv_log2 = fetch_tsv(URLS[\"cnv_log2\"], index_col=0)\n",
    "cnv_log2.index = cnv_log2.index.map(strip_ensembl_version)\n",
    "cnv_log2 = clean_cols(cnv_log2)\n",
    "\n",
    "# ---- Light, safe cleanup: collapse duplicate gene rows ----\n",
    "dup_prot_t = int(prot_tumor.index.duplicated().sum())\n",
    "dup_prot_n = int(prot_norm.index.duplicated().sum())\n",
    "dup_mut    = int(mut_bin.index.duplicated().sum())\n",
    "dup_rna    = int(rna_tumor.index.duplicated().sum())\n",
    "dup_cnv    = int(cnv_log2.index.duplicated().sum())\n",
    "\n",
    "if dup_prot_t or dup_prot_n:\n",
    "    prot_tumor = collapse_duplicate_rows(prot_tumor, how=\"median\")\n",
    "    prot_norm  = collapse_duplicate_rows(prot_norm,  how=\"median\")\n",
    "\n",
    "if dup_mut:\n",
    "    # Ensure numeric first, then strict binary max across dups\n",
    "    mut_bin = mut_bin.apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "    mut_bin = collapse_duplicate_rows(mut_bin, how=\"max_binary\")\n",
    "\n",
    "if dup_rna:\n",
    "    rna_tumor = collapse_duplicate_rows(rna_tumor, how=\"median\")\n",
    "\n",
    "if dup_cnv:\n",
    "    cnv_log2 = collapse_duplicate_rows(cnv_log2, how=\"median\")\n",
    "\n",
    "# Strict binary for mutation table (robust if source encodes ints/floats/strings)\n",
    "mut_bin = mut_bin.apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "mut_bin = (mut_bin > 0).astype(np.int8)\n",
    "\n",
    "# Optional: drop obviously invalid/blank patient columns across all tables\n",
    "# (keeps order; other alignment happens later)\n",
    "for df_name in [\"prot_tumor\", \"prot_norm\", \"phos_tumor\", \"phos_norm\", \"mut_bin\", \"rna_tumor\", \"cnv_log2\"]:\n",
    "    df = locals()[df_name]\n",
    "    df.columns = pick_valid_patients(df.columns)\n",
    "\n",
    "print(\"prot_tumor\", prot_tumor.shape, \"| prot_norm\", prot_norm.shape, f\"(dup rows removed: T={dup_prot_t}, N={dup_prot_n})\")\n",
    "print(\"phos_tumor\", phos_tumor.shape, \"| phos_norm\", phos_norm.shape, \"(site-level; will aggregate to genes later)\")\n",
    "print(\"mut_bin   \", mut_bin.shape,    f\"(dup rows removed: {dup_mut})\")\n",
    "print(\"rna_tumor \", rna_tumor.shape,  f\"(dup rows removed: {dup_rna})\")\n",
    "print(\"cnv_log2  \", cnv_log2.shape,   f\"(dup rows removed: {dup_cnv})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f4eebfd-6466-4d51-9860-147f24c17128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:13:36.302566Z",
     "iopub.status.busy": "2025-10-03T16:13:36.302130Z",
     "iopub.status.idle": "2025-10-03T16:13:36.656417Z",
     "shell.execute_reply": "2025-10-03T16:13:36.655419Z",
     "shell.execute_reply.started": "2025-10-03T16:13:36.302530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMS classes (label->index): {'CMS1': 0, 'CMS2': 1, 'CMS3': 2, 'CMS4': 3}\n",
      "Total CMS-labeled patients: 85\n",
      "Class counts: Counter({1: 33, 3: 22, 2: 16, 0: 14})\n"
     ]
    }
   ],
   "source": [
    "# ----- Cell 7 (CMS labels from .tsi; robust parsing) -----\n",
    "tsi = fetch_tsv(URLS[\"tsi\"], index_col=None)\n",
    "assert \"attrib_name\" in tsi.columns, \"Unexpected .tsi format: missing 'attrib_name'\"\n",
    "tsi = tsi.set_index(\"attrib_name\")\n",
    "\n",
    "# Find the row containing CMS transcriptomic subtype\n",
    "row_key = None\n",
    "if \"Transcriptomic_subtype\" in tsi.index:\n",
    "    row_key = \"Transcriptomic_subtype\"\n",
    "else:\n",
    "    # try case-insensitive/contains\n",
    "    matches = [ix for ix in tsi.index if \"transcriptomic\" in str(ix).lower() and \"subtype\" in str(ix).lower()]\n",
    "    if matches:\n",
    "        row_key = matches[0]\n",
    "    else:\n",
    "        # fallback: any row whose values look like CMS labels\n",
    "        for ix in tsi.index:\n",
    "            vals = set(str(v) for v in tsi.loc[ix].values)\n",
    "            if any(v.upper().startswith(\"CMS\") for v in vals):\n",
    "                row_key = ix\n",
    "                break\n",
    "\n",
    "assert row_key is not None, \"Could not locate CMS subtype row in the .tsi file.\"\n",
    "\n",
    "cms_row = tsi.loc[row_key].to_dict()\n",
    "\n",
    "# Keep only valid patient → CMS pairs (trim id; accept CMS1..CMS4; drop NA/blank)\n",
    "valid_pairs = []\n",
    "for pid, lab in cms_row.items():\n",
    "    if not isinstance(pid, str) or not isinstance(lab, str):\n",
    "        continue\n",
    "    pid_s = pid.strip()\n",
    "    lab_s = lab.strip().upper()\n",
    "    if not pid_s or lab_s in {\"NA\", \"NAN\", \"\"}:\n",
    "        continue\n",
    "    if not lab_s.startswith(\"CMS\"):  # be strict; ignore non-CMS entries\n",
    "        continue\n",
    "    valid_pairs.append((pid_s, lab_s))\n",
    "\n",
    "assert len(valid_pairs) > 0, \"No valid CMS-labeled patients found.\"\n",
    "\n",
    "kept_patients, labels_str = zip(*valid_pairs)\n",
    "kept_patients = list(kept_patients)\n",
    "labels_str    = list(labels_str)\n",
    "\n",
    "# Stable class order (CMS1..CMS4 if present)\n",
    "classes = sorted(set(labels_str), key=lambda x: (len(x), x))  # CMS1,CMS2,CMS3,CMS4 in order\n",
    "vocab   = {c: i for i, c in enumerate(classes)}               # e.g. {'CMS1':0,'CMS2':1,...}\n",
    "labels_all = torch.tensor([vocab[s] for s in labels_str], dtype=torch.long)\n",
    "\n",
    "# Handy map used later to align labels to proteomics patients\n",
    "cms_pid2lab = {p: vocab[s] for p, s in zip(kept_patients, labels_str)}\n",
    "\n",
    "# Prints\n",
    "from collections import Counter\n",
    "print(\"CMS classes (label->index):\", vocab)\n",
    "print(\"Total CMS-labeled patients:\", len(kept_patients))\n",
    "print(\"Class counts:\", Counter(labels_all.numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bed579c1-c910-463a-ad43-f8bda08a9912",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:13:36.657897Z",
     "iopub.status.busy": "2025-10-03T16:13:36.657545Z",
     "iopub.status.idle": "2025-10-03T16:13:36.666818Z",
     "shell.execute_reply": "2025-10-03T16:13:36.666022Z",
     "shell.execute_reply.started": "2025-10-03T16:13:36.657868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMS in proteomics: 76 of 85\n",
      "Patients per modality: {'prot': 97, 'phos': 97, 'rna': 106, 'cnv': 105, 'mut': 96}\n",
      "Overlap w/ proteomics: {'phos': 97, 'rna': 96, 'cnv': 95, 'mut': 96}\n",
      "CMS in (prot ∩ rna ∩ cnv): 75\n"
     ]
    }
   ],
   "source": [
    "# ----- Cell 8 (normalize patient IDs + anchor CMS to proteomics) -----\n",
    "\n",
    "# 8.1) Normalize patient IDs across all matrices (strip whitespace only)\n",
    "for name in [\"prot_tumor\", \"prot_norm\", \"phos_tumor\", \"phos_norm\", \"mut_bin\", \"rna_tumor\", \"cnv_log2\"]:\n",
    "    df = locals()[name]\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    locals()[name] = df  # rebind explicitly\n",
    "\n",
    "# 8.2) Anchor = proteomics tumor columns\n",
    "prot_patients = set(prot_tumor.columns)\n",
    "\n",
    "# 8.3) Keep CMS patients present in proteomics tumor\n",
    "# (cms_pid2lab and kept_patients come from Cell 7)\n",
    "cms_in_prot = [p for p in kept_patients if p in prot_patients]\n",
    "labels_in_prot = torch.tensor([cms_pid2lab[p] for p in cms_in_prot], dtype=torch.long)\n",
    "\n",
    "print(\"CMS in proteomics:\", len(cms_in_prot), \"of\", len(kept_patients))\n",
    "\n",
    "# Optional quick diagnostics (no filtering performed here)\n",
    "sets = {\n",
    "    \"prot\": set(prot_tumor.columns),\n",
    "    \"phos\": set(phos_tumor.columns),\n",
    "    \"rna\":  set(rna_tumor.columns),\n",
    "    \"cnv\":  set(cnv_log2.columns),\n",
    "    \"mut\":  set(mut_bin.columns),\n",
    "}\n",
    "print(\"Patients per modality:\",\n",
    "      {k: len(v) for k, v in sets.items()})\n",
    "print(\"Overlap w/ proteomics:\",\n",
    "      {k: len(sets[k] & sets[\"prot\"]) for k in [\"phos\", \"rna\", \"cnv\", \"mut\"]})\n",
    "print(\"CMS in (prot ∩ rna ∩ cnv):\",\n",
    "      len(set(cms_in_prot) & sets[\"rna\"] & sets[\"cnv\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3238a7dc-ec60-4a8c-aa66-912fdf28c64b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:13:36.668929Z",
     "iopub.status.busy": "2025-10-03T16:13:36.668015Z",
     "iopub.status.idle": "2025-10-03T16:13:39.742112Z",
     "shell.execute_reply": "2025-10-03T16:13:39.740823Z",
     "shell.execute_reply.started": "2025-10-03T16:13:36.668890Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proteins kept (UNION): 7102 | Patients kept: 76\n"
     ]
    }
   ],
   "source": [
    "# Tunable missingness threshold (across tumor+normal); 0.20–0.40 common in proteomics\n",
    "PROT_MISS_MAX = 0.20\n",
    "\n",
    "# Aggregate phospho sites -> gene level (median)\n",
    "phos_gene_tumor_map = phospho_index_to_gene(phos_tumor.index)\n",
    "phos_gene_norm_map  = phospho_index_to_gene(phos_norm.index)\n",
    "\n",
    "phos_tumor_gene = phos_tumor.copy()\n",
    "phos_tumor_gene[\"__gene__\"] = phos_gene_tumor_map.values\n",
    "phos_tumor_gene = (\n",
    "    phos_tumor_gene\n",
    "    .dropna(subset=[\"__gene__\"])\n",
    "    .groupby(\"__gene__\")\n",
    "    .median(numeric_only=True)\n",
    ")\n",
    "\n",
    "phos_norm_gene = phos_norm.copy()\n",
    "phos_norm_gene[\"__gene__\"] = phos_gene_norm_map.values\n",
    "phos_norm_gene = (\n",
    "    phos_norm_gene\n",
    "    .dropna(subset=[\"__gene__\"])\n",
    "    .groupby(\"__gene__\")\n",
    "    .median(numeric_only=True)\n",
    ")\n",
    "\n",
    "# Align tumor vs normal within each modality (intersection within modality)\n",
    "prot_genes   = prot_tumor.index.intersection(prot_norm.index)\n",
    "prot_tumor2  = prot_tumor.loc[prot_genes].copy()\n",
    "prot_norm2   = prot_norm.loc[prot_genes].copy()\n",
    "\n",
    "phos_genes   = phos_tumor_gene.index.intersection(phos_norm_gene.index)\n",
    "phos_tumor2  = phos_tumor_gene.loc[phos_genes].copy()\n",
    "phos_norm2   = phos_norm_gene.loc[phos_genes].copy()\n",
    "\n",
    "# Filter by missingness across tumor+normal (modality-wise)\n",
    "def filter_by_missingness(df_tum: pd.DataFrame, df_norm: pd.DataFrame, prot_miss_max=0.20):\n",
    "    df_all = pd.concat([df_tum, df_norm], axis=1)\n",
    "    keep_prot = df_all.isna().mean(axis=1) <= prot_miss_max\n",
    "    return df_tum.loc[keep_prot], df_norm.loc[keep_prot]\n",
    "\n",
    "prot_tumor_f, prot_norm_f = filter_by_missingness(prot_tumor2, prot_norm2, prot_miss_max=PROT_MISS_MAX)\n",
    "phos_tumor_f, phos_norm_f = filter_by_missingness(phos_tumor2, phos_norm2, prot_miss_max=PROT_MISS_MAX)\n",
    "\n",
    "# Keep CMS-labeled proteomics tumor patients (proteomics is the patient anchor)\n",
    "prot_tumor_f = prot_tumor_f.loc[:, prot_tumor_f.columns.intersection(cms_in_prot)]\n",
    "\n",
    "# Proteomics missingness mask BEFORE imputation (tumor-only)\n",
    "# (We keep this aligned to prot_tumor_f columns; final mask computed after union below)\n",
    "# prot_mask_pre_raw = prot_tumor_f.isna().astype(np.float32)  # optional sanity, not used later\n",
    "\n",
    "# Sanitize and KNN-impute each modality (adaptive k)\n",
    "def sanitize_for_impute(df: pd.DataFrame, clip_abs: float | None = None) -> pd.DataFrame:\n",
    "    df2 = df.apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "    if clip_abs is not None:\n",
    "        df2 = df2.clip(lower=-clip_abs, upper=clip_abs)\n",
    "    return df2\n",
    "\n",
    "def knn_impute(df: pd.DataFrame, max_k: int = 5) -> pd.DataFrame:\n",
    "    n_samples = df.shape[1]\n",
    "    k = min(max_k, max(1, n_samples - 1))\n",
    "    if n_samples < 2:\n",
    "        row_med = df.median(axis=1, skipna=True)\n",
    "        return df.apply(lambda col: col.fillna(row_med), axis=0)\n",
    "    imp = KNNImputer(n_neighbors=k, weights='distance')\n",
    "    vals = imp.fit_transform(df.T.values)  # [samples, features]\n",
    "    return pd.DataFrame(vals.T, index=df.index, columns=df.columns)\n",
    "\n",
    "# Choose patient lists for imputation\n",
    "tumor_patients_for_prot = prot_tumor_f.columns.tolist()\n",
    "tumor_patients_for_phos = phos_tumor_f.columns.intersection(cms_in_prot).tolist()\n",
    "if len(tumor_patients_for_phos) == 0:\n",
    "    tumor_patients_for_phos = phos_tumor_f.columns.tolist()\n",
    "\n",
    "# Sanitize\n",
    "prot_tumor_f_san = sanitize_for_impute(prot_tumor_f.loc[:, tumor_patients_for_prot], clip_abs=1e6)\n",
    "prot_norm_f_san  = sanitize_for_impute(prot_norm_f,                                  clip_abs=1e6)\n",
    "phos_tumor_f_san = sanitize_for_impute(phos_tumor_f.loc[:, tumor_patients_for_phos], clip_abs=1e6)\n",
    "phos_norm_f_san  = sanitize_for_impute(phos_norm_f,                                  clip_abs=1e6)\n",
    "\n",
    "# Impute\n",
    "prot_tumor_imp = knn_impute(prot_tumor_f_san, max_k=5)\n",
    "prot_norm_imp  = knn_impute(prot_norm_f_san,  max_k=5)\n",
    "phos_tumor_imp = knn_impute(phos_tumor_f_san, max_k=5)\n",
    "phos_norm_imp  = knn_impute(phos_norm_f_san,  max_k=5)\n",
    "\n",
    "# Baseline-normalize with NORMALS ONLY (no leakage)\n",
    "def baseline_z(tum: pd.DataFrame, norm: pd.DataFrame, clip: float = 5.0):\n",
    "    mu = norm.mean(axis=1)\n",
    "    sd = norm.std(axis=1, ddof=0).replace(0, np.nan)\n",
    "    z  = (tum.sub(mu, axis=0)).div(sd, axis=0)\n",
    "    z  = z.replace([np.inf, -np.inf], np.nan).fillna(0.0).clip(-clip, clip)\n",
    "    return z, mu, sd\n",
    "\n",
    "prot_z, prot_mu, prot_sd = baseline_z(prot_tumor_imp, prot_norm_imp, clip=5.0)\n",
    "phos_z, phos_mu, phos_sd = baseline_z(phos_tumor_imp, phos_norm_imp, clip=5.0)\n",
    "\n",
    "# Patients → align columns to CMS proteomics set\n",
    "patient_ids = list(prot_z.columns.intersection(cms_in_prot))\n",
    "prot_z = prot_z.loc[:, patient_ids]\n",
    "phos_z = phos_z.reindex(columns=patient_ids, fill_value=np.nan)\n",
    "\n",
    "# GENES → ALIGN BY UNION (not intersection)\n",
    "union_proteins = prot_z.index.union(phos_z.index)\n",
    "\n",
    "# Reindex to union; keep NaN in phospho for presence mask, fill proteo with zeros where absent\n",
    "prot_z_u = prot_z.reindex(union_proteins, fill_value=0.0)\n",
    "phos_z_u = phos_z.reindex(union_proteins)  # keep NaN now\n",
    "\n",
    "# Masks (pre-impute, tumor-only), aligned to union\n",
    "prot_mask_pre = prot_tumor_f.reindex(union_proteins).loc[:, patient_ids].isna().astype(np.float32)\n",
    "phos_present  = phos_tumor_f.reindex(union_proteins).loc[:, patient_ids].notna().astype(np.float32)  # optional\n",
    "\n",
    "# Fill phospho NaNs with 0 for features (presence mask retains availability info)\n",
    "phos_z_u = phos_z_u.fillna(0.0)\n",
    "\n",
    "prot_z        = prot_z_u\n",
    "phos_z        = phos_z_u\n",
    "prot_mask_pre = prot_mask_pre\n",
    "protein_ids   = list(union_proteins)\n",
    "patient_ids   = list(patient_ids)\n",
    "\n",
    "print(\"Proteins kept (UNION):\", len(protein_ids), \"| Patients kept:\", len(patient_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c876b54-e56e-4bc3-84f7-c6a01eba213c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:13:39.744650Z",
     "iopub.status.busy": "2025-10-03T16:13:39.743332Z",
     "iopub.status.idle": "2025-10-03T16:13:39.774220Z",
     "shell.execute_reply": "2025-10-03T16:13:39.773179Z",
     "shell.execute_reply.started": "2025-10-03T16:13:39.744608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mut_lists] patients=76 | mut_table_cols_found=76 | with_any=76 | avg_mut_proteins/patient=205.4\n"
     ]
    }
   ],
   "source": [
    "# ----- Cell X: align mutations to protein_id space & build per-patient lists -----\n",
    "\n",
    "# 1) Collapse duplicate gene rows (still binary)\n",
    "if mut_bin.index.has_duplicates:\n",
    "    mut_bin = mut_bin.groupby(level=0).max(numeric_only=True)\n",
    "\n",
    "# 2) Keep only mutation columns for patients we're actually using (order = patient_ids)\n",
    "mut_cols = [p for p in patient_ids if p in mut_bin.columns]\n",
    "mut_sub  = mut_bin.reindex(columns=mut_cols)\n",
    "\n",
    "# 3) Align rows to protein_ids (already Ensembl base IDs/symbols from your pipeline)\n",
    "#    Missing rows -> 0 (no mutation recorded for that protein)\n",
    "mut_aligned_df = mut_sub.reindex(index=protein_ids).fillna(0)\n",
    "\n",
    "# 4) Ensure strictly binary int8 (DataFrame -> ndarray)\n",
    "mut_aligned = (mut_aligned_df.to_numpy() > 0).astype(np.int8)  # shape [N_prot, len(mut_cols)]\n",
    "\n",
    "# 5) (Optional) Map protein id -> row index (handy if you need lookups later)\n",
    "prot_idx_map = {g: i for i, g in enumerate(protein_ids)}\n",
    "\n",
    "# 6) Build mut_lists in EXACT patient_ids order (empty for patients with no mut column)\n",
    "mut_lists = []\n",
    "present_cols = {c: j for j, c in enumerate(mut_cols)}  # patient_id -> column idx in mut_aligned\n",
    "for p in patient_ids:\n",
    "    j = present_cols.get(p, None)\n",
    "    if j is None:\n",
    "        mut_lists.append(np.array([], dtype=np.int64))\n",
    "    else:\n",
    "        prot_indices = np.nonzero(mut_aligned[:, j])[0].astype(np.int64)\n",
    "        mut_lists.append(prot_indices)\n",
    "\n",
    "# 7) Diagnostics\n",
    "n_with_any = sum(arr.size > 0 for arr in mut_lists)\n",
    "avg_muts   = float(np.mean([arr.size for arr in mut_lists])) if len(mut_lists) else 0.0\n",
    "print(f\"[mut_lists] patients={len(mut_lists)} | mut_table_cols_found={len(mut_cols)} \"\n",
    "      f\"| with_any={n_with_any} | avg_mut_proteins/patient={avg_muts:.1f}\")\n",
    "\n",
    "# 8) Sanity checks\n",
    "assert len(mut_lists) == len(patient_ids), \"mut_lists must align 1:1 with patient_ids\"\n",
    "if len(mut_cols) == 0:\n",
    "    print(\"WARNING: No mutation columns matched current patient_ids. \"\n",
    "          \"Downstream mutation edges will all be empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "758c7366-1e94-4808-aa0e-f6daa353df23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:13:39.776346Z",
     "iopub.status.busy": "2025-10-03T16:13:39.775920Z",
     "iopub.status.idle": "2025-10-03T16:13:40.238077Z",
     "shell.execute_reply": "2025-10-03T16:13:40.236541Z",
     "shell.execute_reply.started": "2025-10-03T16:13:39.776306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients kept (proteomics-anchored): 76\n",
      "Protein features: (7102, 76) | Phospho: (7102, 76)\n",
      "COMPACT gene space: 7093\n",
      "RNA raw (compacted): (7093, 76) | CNV raw (compacted): (7093, 76)\n"
     ]
    }
   ],
   "source": [
    "# ---------- proteomics-anchored patients; compact gene space; dedup-safe ----------\n",
    "\n",
    "# Inputs expected from earlier cells:\n",
    "# - prot_z, phos_z, prot_mask_pre  (proteo/phospho union; columns already tumor patients)\n",
    "# - patient_ids                    (CMS-aligned proteomics tumor patients)\n",
    "# - rna_tumor, cnv_log2            (raw RNA/CNV tables with gene rows, patient columns)\n",
    "\n",
    "# --- Helpers to collapse duplicates on rows/columns (median across dups) ---\n",
    "def collapse_duplicate_rows(df: pd.DataFrame, how=\"median\"):\n",
    "    if not df.index.is_unique:\n",
    "        if how == \"median\":\n",
    "            df = df.groupby(level=0).median(numeric_only=True)\n",
    "        elif how == \"mean\":\n",
    "            df = df.groupby(level=0).mean(numeric_only=True)\n",
    "        else:\n",
    "            raise ValueError(\"how must be 'median' or 'mean'\")\n",
    "    return df\n",
    "\n",
    "def collapse_duplicate_cols(df: pd.DataFrame, how=\"median\"):\n",
    "    if not df.columns.is_unique:\n",
    "        if how == \"median\":\n",
    "            df = df.T.groupby(level=0).median(numeric_only=True).T\n",
    "        elif how == \"mean\":\n",
    "            df = df.T.groupby(level=0).mean(numeric_only=True).T\n",
    "        else:\n",
    "            raise ValueError(\"how must be 'median' or 'mean'\")\n",
    "    return df\n",
    "\n",
    "def strip_index_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.index   = df.index.map(lambda x: str(x).strip())\n",
    "    df.columns = df.columns.map(lambda x: str(x).strip())\n",
    "    return df\n",
    "\n",
    "# --- Proteomics-anchored patients (preserve order, de-dup) ---\n",
    "patient_ids = list(dict.fromkeys(patient_ids))  # de-dup while preserving order\n",
    "\n",
    "# Sync proteo/phospho/mask to these patients\n",
    "prot_z = prot_z.loc[:, [p for p in patient_ids if p in prot_z.columns]]\n",
    "phos_z = phos_z.reindex(columns=prot_z.columns, fill_value=0.0)  # align to proteomics anchor\n",
    "prot_mask_pre = prot_mask_pre.reindex(columns=prot_z.columns)\n",
    "\n",
    "prot_patients = prot_z.columns\n",
    "assert prot_patients.is_unique, \"Proteomics patient IDs must be unique.\"\n",
    "\n",
    "# --- 9.2) Prepare RNA/CNV for these patients (allow missing) with robust de-dup ---\n",
    "rna_tumor = strip_index_columns(rna_tumor)\n",
    "cnv_log2  = strip_index_columns(cnv_log2)\n",
    "\n",
    "# Keep only the columns we can match (but we will reindex to prot_patients later)\n",
    "rna_cols = [p for p in prot_patients if p in rna_tumor.columns]\n",
    "cnv_cols = [p for p in prot_patients if p in cnv_log2.columns]\n",
    "\n",
    "rna_tumor_sub = rna_tumor.loc[:, rna_cols] if len(rna_cols) else pd.DataFrame(index=rna_tumor.index)\n",
    "cnv_log2_sub = cnv_log2.loc[:,  cnv_cols] if len(cnv_cols) else pd.DataFrame(index=cnv_log2.index)\n",
    "\n",
    "# Collapse duplicates on BOTH axes (avoid reindex errors)\n",
    "rna_tumor_sub = collapse_duplicate_rows(collapse_duplicate_cols(rna_tumor_sub), how=\"median\")\n",
    "cnv_log2_sub = collapse_duplicate_rows(collapse_duplicate_cols(cnv_log2_sub),  how=\"median\")\n",
    "\n",
    "# Force numeric (non-numeric -> NaN) to keep downstream ops stable\n",
    "rna_tumor_sub = rna_tumor_sub.apply(pd.to_numeric, errors=\"coerce\")\n",
    "cnv_log2_sub = cnv_log2_sub.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# --- Compact gene space = protein ids ∩ RNA genes ∩ CNV genes ---\n",
    "protein_index = pd.Index(prot_z.index).map(lambda x: str(x).strip())\n",
    "rna_index = rna_tumor_sub.index\n",
    "cnv_index = cnv_log2_sub.index\n",
    "\n",
    "gene_space = protein_index.intersection(rna_index).intersection(cnv_index)\n",
    "\n",
    "# If intersection is unexpectedly tiny, you can relax here (optional):\n",
    "if len(gene_space) == 0:\n",
    "    # fallback: intersect protein with whichever modality has more overlap\n",
    "    inter_pr_rna = protein_index.intersection(rna_index)\n",
    "    inter_pr_cnv = protein_index.intersection(cnv_index)\n",
    "    gene_space = inter_pr_rna if len(inter_pr_rna) >= len(inter_pr_cnv) else inter_pr_cnv\n",
    "    print(f\"WARNING: 3-way intersection empty; falling back to 2-way size={len(gene_space)}\")\n",
    "\n",
    "# --- Reindex RNA/CNV to compact genes and proteomics patient order (dedup-safe) ---\n",
    "# Columns we reindex to are unique by assert above\n",
    "rna_full = rna_tumor_sub.reindex(index=gene_space, columns=prot_patients)\n",
    "cnv_full = cnv_log2_sub.reindex(index=gene_space, columns=prot_patients)\n",
    "\n",
    "# Availability masks BEFORE any z-scaling\n",
    "rna_avl = rna_full.notna().astype(np.float32)\n",
    "cnv_avl = cnv_full.notna().astype(np.float32)\n",
    "\n",
    "# Final sanity prints\n",
    "print(\"Patients kept (proteomics-anchored):\", len(prot_patients))\n",
    "print(\"Protein features:\", prot_z.shape, \"| Phospho:\", phos_z.shape)\n",
    "print(\"COMPACT gene space:\", len(gene_space))\n",
    "print(\"RNA raw (compacted):\", rna_full.shape, \"| CNV raw (compacted):\", cnv_full.shape)\n",
    "\n",
    "# Expose for next cells\n",
    "patient_ids = list(prot_patients)\n",
    "gene_ids    = list(gene_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f604434-aab8-42a0-b462-80154cf5b74c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:13:40.239943Z",
     "iopub.status.busy": "2025-10-03T16:13:40.239633Z",
     "iopub.status.idle": "2025-10-03T16:13:40.267506Z",
     "shell.execute_reply": "2025-10-03T16:13:40.266326Z",
     "shell.execute_reply.started": "2025-10-03T16:13:40.239918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genes with ≥1 phospho measurement: 3144\n",
      "Genes with no phospho measurement: 3958\n",
      "Total genes (union): 7102\n",
      "Phospho coverage per patient (mean±sd): 0.42903780937194824 +/- 0.009626339189708233\n",
      "Phospho entries that were originally missing but now have imputed |z|>0: 7370\n",
      "Present phospho entries that are ~0 (valid, just FYI): 0\n",
      "Patient 0: mask-present phospho features = 3091\n",
      "Lowest-coverage patients: ['05CO026', '11CO045', '11CO030', '21CO006', '11CO031'] -> [0.39819768, 0.39819768, 0.3984793, 0.3984793, 0.40805408]\n"
     ]
    }
   ],
   "source": [
    "# Alignment checks (must be True)\n",
    "assert phos_present.index.equals(phos_z.index),  \"Row (gene) order mismatch\"\n",
    "assert phos_present.columns.equals(phos_z.columns), \"Column (patient) order mismatch\"\n",
    "assert len(protein_ids) == phos_present.shape[0] == phos_z.shape[0], \"Gene dimension mismatch\"\n",
    "\n",
    "# 1) How many genes have ANY phospho measured across patients?\n",
    "genes_with_any_phos = int((phos_present.sum(axis=1) > 0).sum())\n",
    "genes_with_no_phos  = int((phos_present.sum(axis=1) == 0).sum())\n",
    "print(\"Genes with ≥1 phospho measurement:\", genes_with_any_phos)\n",
    "print(\"Genes with no phospho measurement:\",  genes_with_no_phos)\n",
    "print(\"Total genes (union):\", len(protein_ids))\n",
    "\n",
    "# 2) Per-patient phospho coverage (fraction of genes with any measured phospho in that patient)\n",
    "per_patient_phos_cov = phos_present.sum(axis=0) / len(protein_ids)\n",
    "print(\"Phospho coverage per patient (mean±sd):\",\n",
    "      float(per_patient_phos_cov.mean()), \"+/-\", float(per_patient_phos_cov.std()))\n",
    "\n",
    "# 3) Strong sanity: features must be ~zero wherever mask==0\n",
    "Z = phos_z.to_numpy()                          # shape [N_genes, N_patients]\n",
    "M = phos_present.to_numpy(dtype=bool)          # same shape\n",
    "eps = 1e-8\n",
    "imputed_slots_used = int(((~M) & (np.abs(Z) > eps)).sum())\n",
    "print(\"Phospho entries that were originally missing but now have imputed |z|>0:\", imputed_slots_used)\n",
    "\n",
    "# 4) Informational: how often present-but-numerically-zero (not an error)\n",
    "present_but_zeroish = int((M & (np.abs(Z) <= eps)).sum())\n",
    "print(\"Present phospho entries that are ~0 (valid, just FYI):\", present_but_zeroish)\n",
    "\n",
    "# 5) Example patient summary (use mask, not nonzero)\n",
    "p0 = 0\n",
    "mask_present_p0 = int(M[:, p0].sum())\n",
    "print(f\"Patient {p0}: mask-present phospho features =\", mask_present_p0)\n",
    "\n",
    "# (Optional) Show patients with lowest coverage\n",
    "cov_sorted = per_patient_phos_cov.sort_values()\n",
    "print(\"Lowest-coverage patients:\", list(cov_sorted.index[:5]), \"->\", list(cov_sorted.values[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43eb484f-4908-44c6-8652-06beb26f74e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:13:40.269317Z",
     "iopub.status.busy": "2025-10-03T16:13:40.268783Z",
     "iopub.status.idle": "2025-10-03T16:13:40.284314Z",
     "shell.execute_reply": "2025-10-03T16:13:40.283133Z",
     "shell.execute_reply.started": "2025-10-03T16:13:40.269277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero phospho values where mask==0 (should be 0): 0\n"
     ]
    }
   ],
   "source": [
    "# HARD MASK phospho: zero out imputed values where mask==0\n",
    "phos_z = phos_z.where(phos_present.astype(bool), 0.0)\n",
    "\n",
    "# Re-run the consistency check:\n",
    "Z = phos_z.to_numpy()\n",
    "M = phos_present.to_numpy(dtype=bool)\n",
    "eps = 1e-8\n",
    "bad_nonzero_when_missing = int(((~M) & (np.abs(Z) > eps)).sum())\n",
    "print(\"Non-zero phospho values where mask==0 (should be 0):\", bad_nonzero_when_missing)  # expect 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca04fadd-2ea5-4033-bf0f-fafa721a23ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:13:40.286240Z",
     "iopub.status.busy": "2025-10-03T16:13:40.285731Z",
     "iopub.status.idle": "2025-10-03T16:13:40.306397Z",
     "shell.execute_reply": "2025-10-03T16:13:40.304844Z",
     "shell.execute_reply.started": "2025-10-03T16:13:40.286205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_splits=5 | sizes → train=45  val=16  test=15\n",
      "Train class counts: Counter({1: 18, 3: 13, 2: 8, 0: 6})\n",
      "Val class counts:   Counter({1: 6, 3: 4, 0: 3, 2: 3})\n",
      "Test class counts:  Counter({1: 5, 3: 4, 0: 3, 2: 3})\n"
     ]
    }
   ],
   "source": [
    "# ---------- CELL 10: StratifiedKFold split (train/val/test) ----------\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 10.1) Align CMS labels to patient_ids (from Cell 8/9 proteomics anchor)\n",
    "cms_pid2lab = {p: l for p, l in zip(cms_in_prot, labels_in_prot.tolist())}\n",
    "labels_aligned = torch.tensor([cms_pid2lab[p] for p in patient_ids], dtype=torch.long)\n",
    "y = labels_aligned.numpy()\n",
    "\n",
    "# 10.2) Drop classes with <2 samples (Stratified splits require ≥2 per class)\n",
    "counts = np.bincount(y, minlength=int(y.max() + 1))\n",
    "rare_classes = np.where(counts < 2)[0].tolist()\n",
    "if rare_classes:\n",
    "    keep_mask = ~np.isin(y, rare_classes)\n",
    "    patient_ids = [p for p, m in zip(patient_ids, keep_mask) if m]\n",
    "    labels_aligned = labels_aligned[keep_mask]\n",
    "    y = labels_aligned.numpy()\n",
    "    prot_z        = prot_z.loc[:, patient_ids]\n",
    "    phos_z        = phos_z.loc[:, patient_ids]\n",
    "    prot_mask_pre = prot_mask_pre.loc[:, patient_ids]\n",
    "    rna_full      = rna_full.loc[:, patient_ids]\n",
    "    cnv_full      = cnv_full.loc[:, patient_ids]\n",
    "    rna_avl       = rna_avl.loc[:, patient_ids]\n",
    "    cnv_avl       = cnv_avl.loc[:, patient_ids]\n",
    "    print(\"Dropped rare CMS classes:\", rare_classes)\n",
    "\n",
    "# 10.3) StratifiedKFold → pick one fold for VAL, one for TEST; rest = TRAIN\n",
    "# Choose folds so each is ~N/n_splits. For ~75 pts, n_splits=5 ⇒ ~15 per fold (nice for your target 12–16).\n",
    "counts = np.bincount(y, minlength=int(y.max() + 1))\n",
    "min_per_class = int(counts[counts > 0].min())\n",
    "n_splits = int(min(5, max(2, min_per_class)))   # cap at 5 by default; never below 2\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "folds = list(skf.split(np.zeros_like(y), y))\n",
    "\n",
    "# Nominally use fold 0 for VAL and fold 1 for TEST (disjoint)\n",
    "VAL_FOLD  = 0\n",
    "TEST_FOLD = 1 if n_splits >= 3 else 0  # if only 2 folds, fallback handled below\n",
    "\n",
    "if n_splits >= 3:\n",
    "    val_idx  = folds[VAL_FOLD][1]\n",
    "    test_idx = folds[TEST_FOLD][1]\n",
    "    all_idx = np.arange(len(y))\n",
    "    train_mask = np.ones_like(all_idx, dtype=bool)\n",
    "    train_mask[val_idx] = False\n",
    "    train_mask[test_idx] = False\n",
    "    train_idx = all_idx[train_mask]\n",
    "else:\n",
    "    # n_splits == 2 → create TEST from one fold's test, then carve VAL from the remaining via SSS\n",
    "    test_idx = folds[1][1]              # ~50% as test\n",
    "    rest_idx = folds[1][0]              # complement used for train+val\n",
    "    # carve a small stratified val (e.g., 20% of rest)\n",
    "    sss_val = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=43)\n",
    "    tr_sub, va_sub = next(sss_val.split(np.zeros_like(y[rest_idx]), y[rest_idx]))\n",
    "    train_idx = rest_idx[tr_sub]\n",
    "    val_idx   = rest_idx[va_sub]\n",
    "\n",
    "# Final reporting\n",
    "print(f\"n_splits={n_splits} | sizes → train={len(train_idx)}  val={len(val_idx)}  test={len(test_idx)}\")\n",
    "print(\"Train class counts:\", Counter(y[train_idx]))\n",
    "print(\"Val class counts:  \", Counter(y[val_idx]))\n",
    "print(\"Test class counts: \", Counter(y[test_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f95fd23-e358-4293-a91f-2a2fb850503e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:13:40.309129Z",
     "iopub.status.busy": "2025-10-03T16:13:40.308737Z",
     "iopub.status.idle": "2025-10-03T16:13:40.326309Z",
     "shell.execute_reply": "2025-10-03T16:13:40.324935Z",
     "shell.execute_reply.started": "2025-10-03T16:13:40.309100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: train=57 val=15 test=4\n",
      "Train class counts: Counter({1: 22, 3: 16, 2: 10, 0: 9})\n",
      "Val class counts:   Counter({1: 6, 3: 4, 2: 3, 0: 2})\n",
      "Test class counts:  Counter({0: 1, 1: 1, 2: 1, 3: 1})\n"
     ]
    }
   ],
   "source": [
    "# Labels aligned to patient_ids\n",
    "cms_pid2lab = {p:l for p,l in zip(cms_in_prot, labels_in_prot.tolist())}\n",
    "labels_aligned = torch.tensor([cms_pid2lab[p] for p in patient_ids], dtype=torch.long)\n",
    "y = labels_aligned.numpy()\n",
    "\n",
    "# Drop classes with <2 members\n",
    "counts = np.bincount(y, minlength=int(y.max()+1))\n",
    "rare = np.where(counts < 2)[0].tolist()\n",
    "if rare:\n",
    "    keep_mask = ~np.isin(y, rare)\n",
    "    patient_ids = [p for p,m in zip(patient_ids, keep_mask) if m]\n",
    "    labels_aligned = labels_aligned[keep_mask]\n",
    "    y = labels_aligned.numpy()\n",
    "    prot_z   = prot_z.loc[:, patient_ids]\n",
    "    phos_z   = phos_z.loc[:, patient_ids]\n",
    "    prot_mask_pre = prot_mask_pre.loc[:, patient_ids]\n",
    "    rna_tumor_sub = rna_tumor_sub.loc[:, patient_ids]\n",
    "    cnv_log2_sub  = cnv_log2_sub.loc[:, patient_ids]\n",
    "    print(\"Dropped rare classes:\", rare)\n",
    "\n",
    "# Robust split: 1 per class in test, small val if possible\n",
    "rng = np.random.default_rng(42)\n",
    "classes = np.unique(y)\n",
    "cls_to_idx = {c: np.where(y==c)[0].tolist() for c in classes}\n",
    "test_idx = [int(rng.choice(idx)) for idx in cls_to_idx.values()]\n",
    "test_mask = np.zeros_like(y, dtype=bool); test_mask[test_idx] = True\n",
    "rest_idx = np.where(~test_mask)[0]\n",
    "y_rest = y[rest_idx]\n",
    "\n",
    "# Try a small stratified val, otherwise pick one per class if possible\n",
    "val_idx = []\n",
    "if len(rest_idx) >= 4 and all([(y_rest==c).sum()>=2 for c in classes]):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=43)\n",
    "    tr_sub, va_sub = next(sss.split(np.zeros_like(y_rest), y_rest))\n",
    "    train_idx = rest_idx[tr_sub]; val_idx = rest_idx[va_sub]\n",
    "else:\n",
    "    # 1 per class for val if possible\n",
    "    for c in classes:\n",
    "        pool = [i for i in rest_idx if y[i]==c]\n",
    "        if len(pool)>=2:\n",
    "            val_idx.append(int(rng.choice(pool)))\n",
    "    val_idx = np.array(sorted(set(val_idx)), dtype=int)\n",
    "    train_mask = (~test_mask).copy()\n",
    "    train_mask[val_idx] = False\n",
    "    train_idx = np.where(train_mask)[0]\n",
    "\n",
    "print(f\"Split sizes: train={len(train_idx)} val={len(val_idx)} test={len(test_idx)}\")\n",
    "print(\"Train class counts:\", Counter(y[train_idx]))\n",
    "print(\"Val class counts:  \", Counter(y[val_idx]))\n",
    "print(\"Test class counts: \", Counter(y[test_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71ed0e9a-8431-4b3b-a7fa-aed3c3e7cad0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:13:40.329796Z",
     "iopub.status.busy": "2025-10-03T16:13:40.329433Z",
     "iopub.status.idle": "2025-10-03T16:13:40.479561Z",
     "shell.execute_reply": "2025-10-03T16:13:40.478562Z",
     "shell.execute_reply.started": "2025-10-03T16:13:40.329759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein channels: torch.Size([76, 7102]) torch.Size([76, 7102]) torch.Size([76, 7102]) | phos_avl\n",
      "Gene channels: torch.Size([76, 7093]) torch.Size([76, 7093]) torch.Size([76, 7093]) torch.Size([76, 7093])\n"
     ]
    }
   ],
   "source": [
    "# ---------- Train-only normalization for RNA/CNV + tensorization ----------\n",
    "\n",
    "# 11.1) Train-only z-scales for RNA & CNV (fit on *train* patients only, ignore NaN)\n",
    "def z_by_train_only(df_full: pd.DataFrame, train_cols: list[str], clip=5.0):\n",
    "    if len(train_cols) == 0:\n",
    "        # no training data for this modality; return zeros\n",
    "        Z = pd.DataFrame(0.0, index=df_full.index, columns=df_full.columns)\n",
    "        mu = pd.Series(0.0, index=df_full.index)\n",
    "        sd = pd.Series(1.0, index=df_full.index)\n",
    "        return Z, mu, sd\n",
    "\n",
    "    train_df = df_full.loc[:, train_cols]\n",
    "    mu = train_df.mean(axis=1, skipna=True)\n",
    "    sd = train_df.std(axis=1, ddof=0, skipna=True).replace(0, np.nan)\n",
    "\n",
    "    Z = (df_full.sub(mu, axis=0)).div(sd, axis=0)\n",
    "    # guard against inf, then clip; leave NaN to be filled later\n",
    "    Z = Z.replace([np.inf, -np.inf], np.nan).clip(-clip, clip)\n",
    "    return Z, mu, sd\n",
    "\n",
    "# Which train patients actually have RNA/CNV?\n",
    "train_cols_rna = [patient_ids[i] for i in train_idx if patient_ids[i] in rna_full.columns]\n",
    "train_cols_cnv = [patient_ids[i] for i in train_idx if patient_ids[i] in cnv_full.columns]\n",
    "\n",
    "rna_z_df, rna_mu, rna_sd = z_by_train_only(rna_full, train_cols_rna, clip=5.0)\n",
    "cnv_z_df, cnv_mu, cnv_sd = z_by_train_only(cnv_full, train_cols_cnv, clip=5.0)\n",
    "\n",
    "# 11.2) Fill NaNs with 0.0 for features; keep availability masks separately\n",
    "rna_z = rna_z_df.fillna(0.0)\n",
    "cnv_z = cnv_z_df.fillna(0.0)\n",
    "\n",
    "# (Optional) If you decided to *hard-mask* phospho to zero where originally missing:\n",
    "HARD_MASK_PHOS = False\n",
    "if HARD_MASK_PHOS and 'phos_present' in globals():\n",
    "    phos_z = phos_z.where(phos_present.astype(bool), 0.0)\n",
    "\n",
    "# 11.3) Tensorize ALL channels (alignments assumed from prior cells)\n",
    "# Protein channels (proteomics union space × patient_ids)\n",
    "X_prot      = torch.tensor(prot_z.T.values,        dtype=torch.float32)   # [P, N_prot]\n",
    "X_phos      = torch.tensor(phos_z.T.values,        dtype=torch.float32)   # [P, N_prot]\n",
    "X_mask_prot = torch.tensor(prot_mask_pre.T.values, dtype=torch.float32)   # [P, N_prot]\n",
    "\n",
    "# If you kept a phospho availability mask, expose it too (lets the GNN know measured vs imputed)\n",
    "X_phos_avl = None\n",
    "if 'phos_present' in globals():\n",
    "    X_phos_avl = torch.tensor(phos_present.T.values, dtype=torch.float32) # [P, N_prot]\n",
    "\n",
    "# Gene channels (value + availability) in the compact gene space\n",
    "X_rna      = torch.tensor(rna_z.T.values,    dtype=torch.float32)         # [P, N_gene]\n",
    "X_cnv      = torch.tensor(cnv_z.T.values,    dtype=torch.float32)         # [P, N_gene]\n",
    "X_rna_avl  = torch.tensor(rna_avl.T.values,  dtype=torch.float32)         # [P, N_gene]\n",
    "X_cnv_avl  = torch.tensor(cnv_avl.T.values,  dtype=torch.float32)         # [P, N_gene]\n",
    "\n",
    "# Final IDs\n",
    "protein_ids = list(prot_z.index)                   # union protein list from Cell 8\n",
    "gene_ids    = list(rna_z.index.union(cnv_z.index)) # equals the compact gene_space\n",
    "\n",
    "# Quick finite checks (will raise if something slipped through)\n",
    "for name, X in [\n",
    "    (\"X_prot\", X_prot), (\"X_phos\", X_phos), (\"X_mask_prot\", X_mask_prot),\n",
    "    (\"X_rna\", X_rna), (\"X_cnv\", X_cnv), (\"X_rna_avl\", X_rna_avl), (\"X_cnv_avl\", X_cnv_avl)\n",
    "]:\n",
    "    if not torch.isfinite(X).all():\n",
    "        raise ValueError(f\"{name} contains non-finite values\")\n",
    "\n",
    "if X_phos_avl is not None and not torch.isfinite(X_phos_avl).all():\n",
    "    raise ValueError(\"X_phos_avl contains non-finite values\")\n",
    "\n",
    "print(\"Protein channels:\", X_prot.shape, X_phos.shape, X_mask_prot.shape, \n",
    "      \"| phos_avl\" if X_phos_avl is not None else \"| phos_avl (not provided)\")\n",
    "print(\"Gene channels:\",    X_rna.shape,  X_cnv.shape,  X_rna_avl.shape, X_cnv_avl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a689fe1c-b745-4c7b-9c10-45082c78b808",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:13:40.481358Z",
     "iopub.status.busy": "2025-10-03T16:13:40.480848Z",
     "iopub.status.idle": "2025-10-03T16:13:40.505258Z",
     "shell.execute_reply": "2025-10-03T16:13:40.504135Z",
     "shell.execute_reply.started": "2025-10-03T16:13:40.481324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[codes edges] 7093 (gene→protein); overlap genes=7093/7093 (100.0%)\n",
      "protein_ids: 7102 total | Ensembl-like=7102 | non-Ensembl=0\n",
      "gene_ids   : 7093 total   | Ensembl-like=7093 | non-Ensembl=0\n"
     ]
    }
   ],
   "source": [
    "# ---------- Build gene <-> protein cross-edges (name-matched) ----------\n",
    "def is_ensembl(g):\n",
    "    return isinstance(g, str) and g.upper().startswith(\"ENSG\")\n",
    "\n",
    "# 1) Build index maps\n",
    "prot_idx = {g: i for i, g in enumerate(protein_ids)}  # protein (union) IDs\n",
    "gene_idx = {g: i for i, g in enumerate(gene_ids)}     # compact gene space (RNA∩CNV∩proteo)\n",
    "\n",
    "# 2) Exact-name matches → edges gene->protein\n",
    "src, dst = [], []\n",
    "overlap = 0\n",
    "for g in gene_ids:\n",
    "    if g in prot_idx:         # exact string match\n",
    "        src.append(gene_idx[g])\n",
    "        dst.append(prot_idx[g])\n",
    "        overlap += 1\n",
    "\n",
    "codes_edge_index     = torch.tensor([src, dst], dtype=torch.long)\n",
    "codes_rev_edge_index = torch.tensor([dst, src], dtype=torch.long)\n",
    "\n",
    "print(f\"[codes edges] {codes_edge_index.shape[1]} (gene→protein); overlap genes={overlap}/{len(gene_ids)} \"\n",
    "      f\"({overlap/len(gene_ids):.1%})\")\n",
    "\n",
    "# 3) Quick diagnostics: how many of your protein_ids are Ensembl vs symbols?\n",
    "n_prot_ens  = sum(is_ensembl(p) for p in protein_ids)\n",
    "n_gene_ens  = sum(is_ensembl(g) for g in gene_ids)\n",
    "print(f\"protein_ids: {len(protein_ids)} total | Ensembl-like={n_prot_ens} | non-Ensembl={len(protein_ids)-n_prot_ens}\")\n",
    "print(f\"gene_ids   : {len(gene_ids)} total   | Ensembl-like={n_gene_ens} | non-Ensembl={len(gene_ids)-n_gene_ens}\")\n",
    "\n",
    "# 4) Optional: warn if overlap is low (often caused by symbol-vs-Ensembl mix)\n",
    "if overlap < 0.6 * len(gene_ids):\n",
    "    print(\"WARNING: Low gene↔protein overlap. Likely ID convention mismatch (symbols vs Ensembl).\")\n",
    "    print(\"Tip: ensure phospho aggregation produced Ensembl IDs when available (your function prefers ENSG).\")\n",
    "    print(\"If many protein rows are symbols only, consider restricting protein_ids to Ensembl-like to improve alignment.\")\n",
    "\n",
    "# --- OPTIONAL TIGHTENING (commented) ---\n",
    "# If you decide to restrict to Ensembl-only proteins to maximize overlap, do it BEFORE tensorization:\n",
    "# ONLY_ENSG_PROTEINS = False\n",
    "# if ONLY_ENSG_PROTEINS:\n",
    "#     keep_mask = [is_ensembl(p) for p in protein_ids]\n",
    "#     prot_keep = [p for p, k in zip(protein_ids, keep_mask) if k]\n",
    "#     # Reindex proteomics matrices (and any masks) to prot_keep, then rebuild protein_ids and codes_edge_index:\n",
    "#     prot_z        = prot_z.loc[prot_keep]\n",
    "#     phos_z        = phos_z.loc[prot_keep]\n",
    "#     prot_mask_pre = prot_mask_pre.loc[prot_keep]\n",
    "#     protein_ids   = prot_keep\n",
    "#     # Rebuild the maps and edges after this restriction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6ee22aa-3a27-48f2-aacd-80b9df7d72b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:13:40.506615Z",
     "iopub.status.busy": "2025-10-03T16:13:40.506263Z",
     "iopub.status.idle": "2025-10-03T16:13:43.497474Z",
     "shell.execute_reply": "2025-10-03T16:13:43.496509Z",
     "shell.execute_reply.started": "2025-10-03T16:13:40.506582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[kNN] k=15 -> edges=191304 (directed), isolates=0\n",
      "Nodes=7102 | Edges=191304 (directed)\n",
      "Degree: mean=26.94, min=15, max=219\n",
      "Isolated proteins: 0\n",
      "Components=1, Giant component fraction=1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch_geometric.utils import to_undirected  # is_undirected not needed\n",
    "\n",
    "def build_ppi_knn_from_train_robust(X_train_prot: torch.Tensor,\n",
    "                                    X_train_phos: torch.Tensor | None = None,\n",
    "                                    k: int = 15,\n",
    "                                    k_step: int = 5,\n",
    "                                    k_max: int = 40,\n",
    "                                    tiny_jitter: float = 1e-8):\n",
    "    Vp = X_train_prot.cpu().numpy().T  # [N_prot, P_train]\n",
    "    if X_train_phos is not None:\n",
    "        Vh = X_train_phos.cpu().numpy().T\n",
    "        V  = np.concatenate([Vp, Vh], axis=1)\n",
    "    else:\n",
    "        V = Vp\n",
    "\n",
    "    V = np.nan_to_num(V, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    row_norm = np.linalg.norm(V, axis=1)\n",
    "    zero_rows = (row_norm == 0)\n",
    "    if zero_rows.any():\n",
    "        V[zero_rows, 0] = tiny_jitter\n",
    "\n",
    "    N = V.shape[0]\n",
    "    cur_k = min(k, max(1, N - 1))\n",
    "\n",
    "    def make_edges(cur_k: int) -> torch.Tensor:\n",
    "        S = cosine_similarity(V)               # [N, N]\n",
    "        np.fill_diagonal(S, -np.inf)\n",
    "        S = np.nan_to_num(S, nan=-1.0)\n",
    "        kk = min(cur_k, N - 1)\n",
    "        idx = np.argpartition(-S, kth=kk, axis=1)[:, :kk]\n",
    "\n",
    "        pairs = set()\n",
    "        for i in range(N):\n",
    "            for j in idx[i]:\n",
    "                if i == j: \n",
    "                    continue\n",
    "                a, b = (i, j) if i < j else (j, i)\n",
    "                pairs.add((a, b))\n",
    "\n",
    "        if not pairs:\n",
    "            return torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "        E = np.array(sorted(list(pairs)), dtype=np.int64)  # [M, 2]\n",
    "        src = np.concatenate([E[:, 0], E[:, 1]], axis=0)\n",
    "        dst = np.concatenate([E[:, 1], E[:, 0]], axis=0)\n",
    "        edge_index = torch.tensor(np.stack([src, dst], axis=0), dtype=torch.long)\n",
    "\n",
    "        # Always force canonical undirected (idempotent) and drop self-loops\n",
    "        edge_index = to_undirected(edge_index, num_nodes=N)\n",
    "        mask = edge_index[0] != edge_index[1]\n",
    "        return edge_index[:, mask]\n",
    "\n",
    "    while True:\n",
    "        edge_index = make_edges(cur_k)\n",
    "        deg = torch.bincount(edge_index[0], minlength=N)\n",
    "        isolates = int((deg == 0).sum().item())\n",
    "        print(f\"[kNN] k={cur_k} -> edges={edge_index.size(1)} (directed), isolates={isolates}\")\n",
    "        if isolates == 0 or cur_k >= k_max:\n",
    "            break\n",
    "        cur_k = min(k_max, cur_k + k_step)\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "\n",
    "# ---- Build PPI from TRAIN patients (uses your existing splits) ----\n",
    "X_train_prot = X_prot[train_idx]   # [P_train, N_prot]\n",
    "X_train_phos = X_phos[train_idx]   # [P_train, N_prot]\n",
    "ppi_edge_index = build_ppi_knn_from_train_robust(X_train_prot, X_train_phos, k=15, k_step=5, k_max=40)\n",
    "\n",
    "# Diagnostics\n",
    "N = X_prot.shape[1]\n",
    "deg = torch.bincount(ppi_edge_index[0], minlength=N)\n",
    "print(f\"Nodes={N} | Edges={ppi_edge_index.size(1)} (directed)\")\n",
    "print(f\"Degree: mean={deg.float().mean():.2f}, min={int(deg.min())}, max={int(deg.max())}\")\n",
    "iso = (deg == 0).sum().item()\n",
    "print(\"Isolated proteins:\", iso)\n",
    "\n",
    "# Connectedness (requires networkx)\n",
    "import networkx as nx\n",
    "E = list(zip(ppi_edge_index[0].tolist(), ppi_edge_index[1].tolist()))\n",
    "G = nx.Graph(); G.add_edges_from(E)\n",
    "n_comp = nx.number_connected_components(G)\n",
    "giant = len(max(nx.connected_components(G), key=len)) / G.number_of_nodes()\n",
    "print(f\"Components={n_comp}, Giant component fraction={giant:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98ea3c39-e6c6-4dcb-8cf4-cdd4d6effce8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:13:43.499469Z",
     "iopub.status.busy": "2025-10-03T16:13:43.498690Z",
     "iopub.status.idle": "2025-10-03T16:14:15.770606Z",
     "shell.execute_reply": "2025-10-03T16:14:15.769616Z",
     "shell.execute_reply.started": "2025-10-03T16:13:43.499432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping protein IDs to STRING (aliases)...\n",
      "Mapped 7060 / 7102 proteins to STRING IDs.\n",
      "Streaming STRING links and filtering by score...\n",
      "STRING edges kept: 92980\n",
      "[HYBRID PPI] Nodes=7102 | Edges=275450 (directed)\n",
      "Degree: mean=38.78, min=15, max=312\n",
      "Isolated proteins: 0\n"
     ]
    }
   ],
   "source": [
    "# ---------- STRING PPI (streamed, high-score) + union with kNN ----------\n",
    "import gzip, io, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "# ---- Paths to STRING files (download beforehand) ----\n",
    "STRING_ALIASES = \"./9606.protein.aliases.v12.0.txt.gz\"\n",
    "STRING_LINKS   = \"./9606.protein.links.v12.0.txt.gz\"\n",
    "\n",
    "# ---- Utils ----\n",
    "def _open_any(path: str):\n",
    "    return gzip.open(path, \"rt\") if path.endswith(\".gz\") else open(path, \"r\")\n",
    "\n",
    "def ensg_base(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    m = re.match(r\"^(ENSG[0-9]+)\", s)\n",
    "    return m.group(1) if m else s\n",
    "\n",
    "def load_string_aliases(alias_path: str,\n",
    "                        symbol_sources: set[str] | None = None):\n",
    "    \"\"\"\n",
    "    Read STRING aliases (v12) robustly (tab-separated; header may start with '#string_protein_id').\n",
    "    Returns:\n",
    "      ensg_map: dict[ENSG_BASE -> set(STRING protein IDs like 'ENSP...')]\n",
    "      sym_map : dict[SYMBOL    -> set(STRING protein IDs)]\n",
    "    \"\"\"\n",
    "    if symbol_sources is None:\n",
    "        symbol_sources = {\n",
    "            \"BioMart_HUGO\", \"Ensembl_HGNC_symbol\", \"Ensembl_HGNC\",\n",
    "            \"HGNC\", \"HUGO\", \"GeneCards\", \"UniProt_gene\", \"Ensembl_EntrezGene\"\n",
    "        }\n",
    "\n",
    "    with _open_any(alias_path) as f:\n",
    "        df = pd.read_csv(f, sep=\"\\t\", header=0, dtype=str, keep_default_na=False)\n",
    "\n",
    "    # normalize column names; first may be \"#string_protein_id\"\n",
    "    df.columns = [str(c).lstrip(\"#\").strip() for c in df.columns]\n",
    "    if \"string_protein_id\" not in df.columns or \"alias\" not in df.columns:\n",
    "        # fallback if header missing for some reason\n",
    "        with _open_any(alias_path) as f:\n",
    "            df = pd.read_csv(f, sep=\"\\t\", header=None, dtype=str, keep_default_na=False)\n",
    "        cols = [\"string_protein_id\", \"alias\"] + ([\"source\"] if df.shape[1] >= 3 else [])\n",
    "        df = df.iloc[:, :len(cols)]\n",
    "        df.columns = cols\n",
    "\n",
    "    for c in df.columns:\n",
    "        df[c] = df[c].astype(str).str.strip()\n",
    "    if \"source\" not in df.columns:\n",
    "        df[\"source\"] = \"\"\n",
    "\n",
    "    ensg_map, sym_map = defaultdict(set), defaultdict(set)\n",
    "    for sid, alias, src in zip(df[\"string_protein_id\"], df[\"alias\"], df[\"source\"]):\n",
    "        # drop '9606.' prefix so we store 'ENSP...' only\n",
    "        if sid.startswith(\"9606.\"):\n",
    "            sid = sid.split(\".\", 1)[1]\n",
    "\n",
    "        if alias.startswith(\"ENSG\"):\n",
    "            ensg_map[ensg_base(alias)].add(sid)\n",
    "        else:\n",
    "            if src in symbol_sources and re.fullmatch(r\"[A-Za-z][A-Za-z0-9\\-]{0,20}\", alias):\n",
    "                sym_map[alias.upper()].add(sid)\n",
    "    return ensg_map, sym_map\n",
    "\n",
    "def map_protein_ids_to_string_ids(protein_ids, ensg_map, sym_map):\n",
    "    \"\"\"\n",
    "    Map your proteomics protein_ids (Ensembl base IDs or symbols) to STRING SIDs (ENSP...).\n",
    "    \"\"\"\n",
    "    prot_to_sid = {}   # your protein index -> chosen STRING SID\n",
    "    sid_to_idx  = {}   # STRING SID -> your protein index\n",
    "\n",
    "    # First try Ensembl bases (preferred), then symbols\n",
    "    for i, pid in enumerate(protein_ids):\n",
    "        base = ensg_base(pid)\n",
    "        sid = None\n",
    "\n",
    "        # prefer Ensembl matches\n",
    "        if base in ensg_map and len(ensg_map[base]) > 0:\n",
    "            sid = next(iter(ensg_map[base]))\n",
    "        else:\n",
    "            # fall back to symbol if pid looks like a symbol\n",
    "            if re.fullmatch(r\"[A-Za-z][A-Za-z0-9\\-]{0,20}\", pid):\n",
    "                sym = pid.upper()\n",
    "                if sym in sym_map and len(sym_map[sym]) > 0:\n",
    "                    sid = next(iter(sym_map[sym]))\n",
    "\n",
    "        if sid is not None and sid not in sid_to_idx:\n",
    "            prot_to_sid[i] = sid\n",
    "            sid_to_idx[sid] = i\n",
    "\n",
    "    return prot_to_sid, sid_to_idx\n",
    "\n",
    "def build_string_edges_streamed(links_path: str,\n",
    "                                sid_to_idx: dict[str,int],\n",
    "                                min_score: int = 900,   # 900 = highest confidence\n",
    "                                keep_species_prefix: bool = False):\n",
    "    \"\"\"\n",
    "    Stream the STRING links file and keep only high-score edges where BOTH nodes are in sid_to_idx.\n",
    "    Returns a 2xE torch.LongTensor (undirected, no self-loops).\n",
    "    \"\"\"\n",
    "    keep_pairs = set()\n",
    "    with _open_any(links_path) as f:\n",
    "        header = f.readline().strip().split()\n",
    "        # Expect: protein1, protein2, combined_score\n",
    "        # But don't trust positions; find them by name\n",
    "        # Some releases use 'combined_score'; others may use 'combined_score' consistently.\n",
    "        try:\n",
    "            i1 = header.index(\"protein1\"); i2 = header.index(\"protein2\"); iscore = header.index(\"combined_score\")\n",
    "        except ValueError:\n",
    "            # fallback: assume the first 3 columns are the fields\n",
    "            i1, i2, iscore = 0, 1, 2\n",
    "\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) <= iscore: \n",
    "                continue\n",
    "            p1 = parts[i1]; p2 = parts[i2]; sc = parts[iscore]\n",
    "            # remove species prefix\n",
    "            if p1.startswith(\"9606.\"): p1 = p1.split(\".\",1)[1]\n",
    "            if p2.startswith(\"9606.\"): p2 = p2.split(\".\",1)[1]\n",
    "            try:\n",
    "                score = int(sc)\n",
    "            except:\n",
    "                # sometimes it's float-as-string; coerce\n",
    "                try:\n",
    "                    score = int(float(sc))\n",
    "                except:\n",
    "                    continue\n",
    "            if score < min_score:\n",
    "                continue\n",
    "            if p1 in sid_to_idx and p2 in sid_to_idx:\n",
    "                a = sid_to_idx[p1]; b = sid_to_idx[p2]\n",
    "                if a == b: \n",
    "                    continue\n",
    "                if a > b: \n",
    "                    a, b = b, a\n",
    "                keep_pairs.add((a, b))  # undirected canonical pair\n",
    "\n",
    "    if not keep_pairs:\n",
    "        return torch.empty((2,0), dtype=torch.long)\n",
    "\n",
    "    E = np.asarray(sorted(list(keep_pairs)), dtype=np.int64)\n",
    "    src = np.concatenate([E[:,0], E[:,1]], axis=0)\n",
    "    dst = np.concatenate([E[:,1], E[:,0]], axis=0)\n",
    "    edge_index = torch.tensor(np.stack([src, dst], axis=0), dtype=torch.long)\n",
    "\n",
    "    # final safety: canonical undirected & drop self-loops\n",
    "    edge_index = to_undirected(edge_index, num_nodes=len(protein_ids))\n",
    "    mask = edge_index[0] != edge_index[1]\n",
    "    return edge_index[:, mask]\n",
    "\n",
    "def union_undirected_edge_indices(a: torch.Tensor, b: torch.Tensor, num_nodes: int) -> torch.Tensor:\n",
    "    \"\"\"Union two undirected edge_index tensors; return undirected, de-duplicated, no self-loops.\"\"\"\n",
    "    if a.numel() == 0: \n",
    "        ei = b.clone()\n",
    "    elif b.numel() == 0:\n",
    "        ei = a.clone()\n",
    "    else:\n",
    "        ei = torch.cat([a, b], dim=1)\n",
    "\n",
    "    if ei.numel() == 0:\n",
    "        return ei\n",
    "\n",
    "    # Drop self-loops\n",
    "    mask = ei[0] != ei[1]\n",
    "    ei = ei[:, mask]\n",
    "\n",
    "    # Canonical undirected pairs\n",
    "    lo = torch.minimum(ei[0], ei[1]).cpu().numpy()\n",
    "    hi = torch.maximum(ei[0], ei[1]).cpu().numpy()\n",
    "    pairs = np.stack([lo, hi], axis=1)\n",
    "\n",
    "    # Deduplicate\n",
    "    pairs = np.unique(pairs, axis=0)\n",
    "\n",
    "    # Rebuild symmetric\n",
    "    src = np.concatenate([pairs[:,0], pairs[:,1]], axis=0)\n",
    "    dst = np.concatenate([pairs[:,1], pairs[:,0]], axis=0)\n",
    "    out = torch.tensor(np.stack([src, dst], axis=0), dtype=torch.long)\n",
    "\n",
    "    # Ensure undirected & in-range\n",
    "    out = to_undirected(out, num_nodes=num_nodes)\n",
    "    mask = out[0] != out[1]\n",
    "    return out[:, mask]\n",
    "\n",
    "\n",
    "# ---- Build STRING PPI aligned to your protein_ids ----\n",
    "print(\"Mapping protein IDs to STRING (aliases)...\")\n",
    "ensg_map, sym_map = load_string_aliases(STRING_ALIASES)\n",
    "prot_to_sid, sid_to_idx = map_protein_ids_to_string_ids(protein_ids, ensg_map, sym_map)\n",
    "\n",
    "print(f\"Mapped {len(prot_to_sid)} / {len(protein_ids)} proteins to STRING IDs.\")\n",
    "\n",
    "# Keep only VERY high confidence STRING edges (min_score=900 by default).\n",
    "# You can drop to 800 if this is too sparse.\n",
    "print(\"Streaming STRING links and filtering by score...\")\n",
    "string_edge_index = build_string_edges_streamed(\n",
    "    STRING_LINKS, sid_to_idx,\n",
    "    min_score=900\n",
    ")\n",
    "\n",
    "print(\"STRING edges kept:\", string_edge_index.size(1))\n",
    "\n",
    "# ---- Hybrid union: kNN PPI ∪ STRING PPI ----\n",
    "# Assumes you already built kNN PPI as `ppi_edge_index` (from train set coexpression).\n",
    "# If you haven't yet, you can set ppi_edge_index = torch.empty((2,0), dtype=torch.long).\n",
    "num_nodes = len(protein_ids)\n",
    "ppi_edge_index = ppi_edge_index if 'ppi_edge_index' in globals() else torch.empty((2,0), dtype=torch.long)\n",
    "ppi_edge_index = to_undirected(ppi_edge_index, num_nodes=num_nodes)\n",
    "\n",
    "hybrid_ppi_edge_index = union_undirected_edge_indices(ppi_edge_index, string_edge_index, num_nodes=num_nodes)\n",
    "\n",
    "# ---- Diagnostics ----\n",
    "deg = torch.bincount(hybrid_ppi_edge_index[0], minlength=num_nodes)\n",
    "print(f\"[HYBRID PPI] Nodes={num_nodes} | Edges={hybrid_ppi_edge_index.size(1)} (directed)\")\n",
    "print(f\"Degree: mean={deg.float().mean():.2f}, min={int(deg.min())}, max={int(deg.max())}\")\n",
    "iso = int((deg == 0).sum().item())\n",
    "print(f\"Isolated proteins: {iso}\")\n",
    "\n",
    "# Replace your model’s PPI with the hybrid one:\n",
    "ppi_edge_index = hybrid_ppi_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23294164-b947-4c43-bee6-3121f7c730c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:14:15.773992Z",
     "iopub.status.busy": "2025-10-03T16:14:15.773671Z",
     "iopub.status.idle": "2025-10-03T16:14:16.752331Z",
     "shell.execute_reply": "2025-10-03T16:14:16.751418Z",
     "shell.execute_reply.started": "2025-10-03T16:14:15.773965Z"
    }
   },
   "outputs": [],
   "source": [
    "# Canonicalize each edge as (min, max), drop duplicates\n",
    "E = torch.sort(ppi_edge_index, dim=0)[0].t()      # [E, 2]\n",
    "E = torch.unique(E, dim=0).t()                    # [2, E_unique]\n",
    "ppi_edge_index = torch.stack([E[0], E[1]], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dd31de5-a24b-4975-aeb9-03f1e13fbe5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:14:16.753571Z",
     "iopub.status.busy": "2025-10-03T16:14:16.753182Z",
     "iopub.status.idle": "2025-10-03T16:14:18.755387Z",
     "shell.execute_reply": "2025-10-03T16:14:18.754450Z",
     "shell.execute_reply.started": "2025-10-03T16:14:16.753547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[kNN] k=15 -> edges=191304 (directed), isolates=0\n",
      "Nodes=7102 | Edges=191304 (directed)\n",
      "Degree: mean=26.94, min=15, max=219\n",
      "Isolated proteins: 0\n",
      "Degree histogram (degree:count): {15: 737, 16: 666, 17: 678, 18: 513, 19: 452, 20: 387, 21: 355, 22: 269, 23: 275, 24: 238, 25: 225, 26: 184, 27: 148, 28: 143, 29: 120, 30: 121, 31: 112, 32: 85, 33: 82, 34: 80, 35: 72, 36: 59, 37: 68, 38: 44, 39: 41, 40: 43, 41: 39, 42: 36, 43: 40, 44: 41, 45: 36, 46: 37, 47: 24, 48: 30, 49: 23, 50: 20, 51: 27, 52: 30, 53: 21, 54: 21, 55: 15, 56: 18, 57: 32, 58: 13, 59: 19, 60: 18, 61: 14, 62: 16, 63: 11, 64: 18, 65: 17, 66: 13, 67: 11, 68: 12, 69: 10, 70: 8, 71: 7, 72: 13, 73: 12, 74: 4, 75: 5, 76: 9, 77: 5, 78: 10, 79: 5, 80: 9, 81: 4, 82: 5, 83: 5, 84: 8, 85: 4, 86: 5, 87: 3, 88: 7, 89: 4, 90: 4, 91: 1, 92: 6, 93: 4, 94: 4, 95: 1, 97: 2, 98: 2, 99: 1, 100: 4, 101: 3, 102: 1, 103: 3, 104: 5, 105: 3, 106: 2, 107: 3, 108: 2, 109: 6, 110: 2, 111: 1, 112: 2, 113: 3, 114: 2, 116: 2, 117: 2, 118: 1, 119: 3, 120: 2, 121: 1, 122: 1, 123: 1, 124: 2, 126: 4, 127: 1, 128: 1, 129: 1, 130: 1, 133: 1, 138: 1, 142: 1, 146: 2, 149: 1, 153: 1, 157: 1, 159: 1, 160: 1, 163: 1, 165: 1, 173: 1, 174: 1, 179: 1, 182: 3, 198: 1, 212: 1, 219: 1}\n"
     ]
    }
   ],
   "source": [
    "X_train_prot = X_prot[train_idx]\n",
    "X_train_phos = X_phos[train_idx]\n",
    "ppi_edge_index = build_ppi_knn_from_train_robust(X_train_prot, X_train_phos, k=15, k_step=5, k_max=40)\n",
    "\n",
    "N = X_prot.shape[1]\n",
    "deg = torch.bincount(ppi_edge_index[0], minlength=N)\n",
    "print(f\"Nodes={N} | Edges={ppi_edge_index.size(1)} (directed)\")\n",
    "print(f\"Degree: mean={deg.float().mean():.2f}, min={int(deg.min())}, max={int(deg.max())}\")\n",
    "print(\"Isolated proteins:\", int((deg==0).sum()))\n",
    "\n",
    "vals, counts = torch.unique(deg, return_counts=True)\n",
    "print(\"Degree histogram (degree:count):\", dict(zip(vals.tolist(), counts.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d7189f9-f67e-4768-ac2d-99a9de36f4b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:14:18.756920Z",
     "iopub.status.busy": "2025-10-03T16:14:18.756502Z",
     "iopub.status.idle": "2025-10-03T16:14:19.899637Z",
     "shell.execute_reply": "2025-10-03T16:14:19.898532Z",
     "shell.execute_reply.started": "2025-10-03T16:14:18.756884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 0: graph_deg=38 sym_topk_deg=38 overlap=38\n",
      "Jaccard(graph, sym_topk) = 1.0\n",
      "Top-10 by cosine: [3589 2600 1095 4116 6972 1409 2862 1301 1666 1603] scores: [0.812 0.803 0.784 0.783 0.777 0.774 0.758 0.757 0.757 0.752]\n",
      "Top-10 protein IDs: ['ENSG00000137563', 'ENSG00000122033', 'ENSG00000099800', 'ENSG00000145569', 'ENSG00000242110', 'ENSG00000103018', 'ENSG00000126953', 'ENSG00000101346', 'ENSG00000106028', 'ENSG00000105388']\n",
      "Graph neighbors (IDs): ['ENSG00000015413', 'ENSG00000066777', 'ENSG00000070019', 'ENSG00000083642', 'ENSG00000099800', 'ENSG00000101152', 'ENSG00000101346', 'ENSG00000101421', 'ENSG00000102572', 'ENSG00000103018', 'ENSG00000105388', 'ENSG00000105778', 'ENSG00000106028', 'ENSG00000107566', 'ENSG00000118939', 'ENSG00000120925', 'ENSG00000121039', 'ENSG00000122033', 'ENSG00000126953', 'ENSG00000130234', 'ENSG00000137288', 'ENSG00000137563', 'ENSG00000137648', 'ENSG00000138074', 'ENSG00000145569', 'ENSG00000147202', 'ENSG00000156709', 'ENSG00000164924', 'ENSG00000165215', 'ENSG00000165556', 'ENSG00000169398', 'ENSG00000176532', 'ENSG00000187720', 'ENSG00000188559', 'ENSG00000189403', 'ENSG00000197142', 'ENSG00000198001', 'ENSG00000242110']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# pick the same patient subset & feature construction used to build the PPI\n",
    "Vp = X_train_prot.cpu().numpy().T                    # [N_prot, P_train]\n",
    "V  = Vp\n",
    "if 'X_train_phos' in globals() and X_train_phos is not None:\n",
    "    Vh = X_train_phos.cpu().numpy().T               # [N_prot, P_train]\n",
    "    V  = np.concatenate([Vp, Vh], axis=1)           # multi-omics as in builder\n",
    "\n",
    "# cosine sim like the builder\n",
    "S = cosine_similarity(V)                             # [N, N]\n",
    "np.fill_diagonal(S, -np.inf)\n",
    "\n",
    "def topk_neighbors(i, k):\n",
    "    kk = min(k, S.shape[1]-1)\n",
    "    idx = np.argpartition(-S[i], kth=kk)[:kk]\n",
    "    return set(idx.tolist())\n",
    "\n",
    "def sym_topk_neighbors(i, k):\n",
    "    \"\"\"Neighbors after symmetrizing: j in topk(i) OR i in topk(j).\"\"\"\n",
    "    tki = topk_neighbors(i, k)\n",
    "    sym = set(tki)\n",
    "    for j in range(S.shape[0]):\n",
    "        if j == i: \n",
    "            continue\n",
    "        # is i among j's top-k?\n",
    "        kk = min(k, S.shape[1]-1)\n",
    "        idx_j = np.argpartition(-S[j], kth=kk)[:kk]\n",
    "        if i in idx_j:\n",
    "            sym.add(j)\n",
    "    return sym\n",
    "\n",
    "# neighbors from the built graph\n",
    "def graph_neighbors(i, edge_index):\n",
    "    ei = edge_index.numpy()\n",
    "    return set(ei[1, ei[0]==i].tolist())\n",
    "\n",
    "prot = 0                       # change as needed\n",
    "k_used = 15                    # the k you passed to the builder\n",
    "\n",
    "g_nbrs   = graph_neighbors(prot, ppi_edge_index)\n",
    "sym_nbrs = sym_topk_neighbors(prot, k_used)\n",
    "\n",
    "overlap  = g_nbrs & sym_nbrs\n",
    "print(f\"Node {prot}: graph_deg={len(g_nbrs)} sym_topk_deg={len(sym_nbrs)} overlap={len(overlap)}\")\n",
    "print(\"Jaccard(graph, sym_topk) =\",\n",
    "      len(overlap) / max(1, len(g_nbrs | sym_nbrs)))\n",
    "\n",
    "# (Optional) show the top-10 most similar indices and their sims\n",
    "top10 = np.argsort(-S[prot])[:10]\n",
    "print(\"Top-10 by cosine:\", top10, \"scores:\", np.round(S[prot, top10], 3))\n",
    "\n",
    "# (Optional) map to protein IDs for readability\n",
    "if 'protein_ids' in globals():\n",
    "    print(\"Top-10 protein IDs:\", [protein_ids[i] for i in top10])\n",
    "    print(\"Graph neighbors (IDs):\", [protein_ids[i] for i in sorted(g_nbrs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31a1cb9b-4d7b-4eec-8e2f-6bd5d5c81281",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:14:19.901575Z",
     "iopub.status.busy": "2025-10-03T16:14:19.900784Z",
     "iopub.status.idle": "2025-10-03T16:14:19.917610Z",
     "shell.execute_reply": "2025-10-03T16:14:19.916736Z",
     "shell.execute_reply.started": "2025-10-03T16:14:19.901538Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "class MultiOmicsPatientDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, patient_ids, y,\n",
    "                 X_prot, X_phos, X_mask_prot,         # [P, N_prot]\n",
    "                 X_rna,  X_cnv,  X_rna_avl, X_cnv_avl, # [P, N_gene]\n",
    "                 mut_lists, protein_ids, gene_ids,\n",
    "                 ppi_edge_index, codes_edge_index, codes_rev_edge_index,\n",
    "                 use_masks: bool = True):\n",
    "        super().__init__()\n",
    "        self.pids = list(patient_ids)\n",
    "        self.y = y\n",
    "\n",
    "        # ---- basic shape guards\n",
    "        Pp, Np = X_prot.shape\n",
    "        assert X_phos.shape == (Pp, Np), \"X_phos must match X_prot shape\"\n",
    "        assert X_mask_prot.shape == (Pp, Np), \"X_mask_prot must match X_prot shape\"\n",
    "\n",
    "        Pg, Ng = X_rna.shape\n",
    "        assert Pg == Pp, \"X_rna must have same #patients as X_prot\"\n",
    "        assert X_cnv.shape      == (Pg, Ng), \"X_cnv must match X_rna shape\"\n",
    "        assert X_rna_avl.shape  == (Pg, Ng), \"X_rna_avl must match X_rna shape\"\n",
    "        assert X_cnv_avl.shape  == (Pg, Ng), \"X_cnv_avl must match X_rna shape\"\n",
    "        assert len(mut_lists)   == Pp,       \"mut_lists must align with patients\"\n",
    "\n",
    "        # ---- store (make contiguous for speed)\n",
    "        self.X_prot = X_prot.contiguous()\n",
    "        self.X_phos = X_phos.contiguous()\n",
    "        self.X_mask_prot = (X_mask_prot if use_masks else torch.zeros_like(X_prot)).contiguous()\n",
    "\n",
    "        self.X_rna = X_rna.contiguous()\n",
    "        self.X_cnv = X_cnv.contiguous()\n",
    "        self.X_rna_avl = (X_rna_avl if use_masks else torch.zeros_like(X_rna)).contiguous()\n",
    "        self.X_cnv_avl = (X_cnv_avl if use_masks else torch.zeros_like(X_cnv)).contiguous()\n",
    "\n",
    "        self.mut = mut_lists\n",
    "        self.prot_ids = list(protein_ids)\n",
    "        self.gene_ids = list(gene_ids)\n",
    "\n",
    "        # ---- static edges\n",
    "        self.ppi       = ppi_edge_index\n",
    "        self.codes     = codes_edge_index\n",
    "        self.codes_rev = codes_rev_edge_index\n",
    "\n",
    "        # edge dtype/range checks\n",
    "        for name, ei, n0, n1 in [\n",
    "            (\"ppi\",       self.ppi,       Np, Np),\n",
    "            (\"codes\",     self.codes,     Ng, Np),\n",
    "            (\"rev_codes\", self.codes_rev, Np, Ng),\n",
    "        ]:\n",
    "            assert ei.dtype == torch.long and ei.dim() == 2 and ei.size(0) == 2, f\"{name} must be [2,E] long\"\n",
    "            if ei.numel() > 0:\n",
    "                max0 = int(ei[0].max())\n",
    "                max1 = int(ei[1].max())\n",
    "                assert max0 < n0 and max1 < n1, f\"{name} edge index out of range\"\n",
    "\n",
    "        self.nP = Np  # #protein nodes\n",
    "        self.nG = Ng  # #gene nodes\n",
    "\n",
    "        # ---- static template graph (copied per __getitem__)\n",
    "        self.template = HeteroData()\n",
    "        self.template['protein'].x = torch.zeros(self.nP, 3)  # prot_z, phos_z, prot_missing_mask\n",
    "        self.template['gene'].x    = torch.zeros(self.nG, 4)  # rna_z, cnv_z, rna_avl, cnv_avl\n",
    "        self.template['patient'].x = torch.zeros(1, 1)\n",
    "\n",
    "        self.template[('protein','ppi','protein')].edge_index   = self.ppi\n",
    "        self.template[('gene','codes','protein')].edge_index    = self.codes\n",
    "        self.template[('protein','rev_codes','gene')].edge_index= self.codes_rev\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pids)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        g = self.template.clone()\n",
    "\n",
    "        # protein channels\n",
    "        g['protein'].x[:, 0] = self.X_prot[i]\n",
    "        g['protein'].x[:, 1] = self.X_phos[i]\n",
    "        g['protein'].x[:, 2] = self.X_mask_prot[i]\n",
    "\n",
    "        # gene channels\n",
    "        g['gene'].x[:, 0] = self.X_rna[i]\n",
    "        g['gene'].x[:, 1] = self.X_cnv[i]\n",
    "        g['gene'].x[:, 2] = self.X_rna_avl[i]\n",
    "        g['gene'].x[:, 3] = self.X_cnv_avl[i]\n",
    "\n",
    "        # patient↔protein mutation edges (sparse, per-patient)\n",
    "        mi = self.mut[i]\n",
    "        if isinstance(mi, np.ndarray):\n",
    "            mi = mi.tolist()\n",
    "        if len(mi) > 0:\n",
    "            src = torch.zeros(len(mi), dtype=torch.long)     # single patient node index 0\n",
    "            dst = torch.tensor(mi, dtype=torch.long)         # protein indices\n",
    "            g[('patient','mutated','protein')].edge_index   = torch.stack([src, dst], dim=0)\n",
    "            g[('protein','rev_mutated','patient')].edge_index = torch.stack([dst, src], dim=0)\n",
    "        else:\n",
    "            g[('patient','mutated','protein')].edge_index     = torch.empty((2,0), dtype=torch.long)\n",
    "            g[('protein','rev_mutated','patient')].edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "\n",
    "        g['patient'].y = torch.tensor([int(self.y[i].item())], dtype=torch.long)\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b8a132d-5e3d-47a9-aeb0-d979735e5b28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:14:19.919126Z",
     "iopub.status.busy": "2025-10-03T16:14:19.918607Z",
     "iopub.status.idle": "2025-10-03T16:14:19.944037Z",
     "shell.execute_reply": "2025-10-03T16:14:19.941782Z",
     "shell.execute_reply.started": "2025-10-03T16:14:19.919092Z"
    }
   },
   "outputs": [],
   "source": [
    "# ---------- BN-safe model ----------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv, global_mean_pool\n",
    "from torch_geometric.utils import dropout_edge\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "class MultiOmicsKGNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_protein: int = 3,\n",
    "        in_gene: int    = 4,\n",
    "        in_patient: int = 1,\n",
    "        hidden: int     = 128,\n",
    "        n_classes: int  = 4,\n",
    "        feat_drop: float = 0.0,\n",
    "        edge_drop: float = 0.0,\n",
    "        use_bn: bool     = True,\n",
    "        residual: bool   = True,\n",
    "        mask_gate: bool  = False,\n",
    "        gate_strength: float = 1.0,\n",
    "        mask_channel_prot: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.feat_drop = float(feat_drop)\n",
    "        self.edge_drop = float(edge_drop)\n",
    "        self.use_bn    = bool(use_bn)\n",
    "        self.residual  = bool(residual)\n",
    "        self.mask_gate = bool(mask_gate)\n",
    "        self.gate_strength = float(gate_strength)\n",
    "        self.mask_channel_prot = int(mask_channel_prot)\n",
    "\n",
    "        # Input projections\n",
    "        self.lin_prot = nn.Linear(in_protein, hidden)\n",
    "        self.lin_gene = nn.Linear(in_gene, hidden)\n",
    "        self.lin_pat  = nn.Linear(in_patient, hidden)\n",
    "\n",
    "        # Norm layers: BN for protein/gene (many nodes), LN for patient (1 node/graph)\n",
    "        if use_bn:\n",
    "            self.bn_prot_in = nn.BatchNorm1d(hidden)\n",
    "            self.bn_gene_in = nn.BatchNorm1d(hidden)\n",
    "            self.ln_pat_in  = nn.LayerNorm(hidden)\n",
    "\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('protein','ppi','protein'):         SAGEConv((-1, -1), hidden),\n",
    "            ('gene','codes','protein'):          SAGEConv((-1, -1), hidden),\n",
    "            ('protein','rev_codes','gene'):      SAGEConv((-1, -1), hidden),\n",
    "            ('patient','mutated','protein'):     SAGEConv((-1, -1), hidden),\n",
    "            ('protein','rev_mutated','patient'): SAGEConv((-1, -1), hidden),\n",
    "        }, aggr='sum')\n",
    "\n",
    "        self.conv2 = HeteroConv({\n",
    "            ('protein','ppi','protein'):         SAGEConv((-1, -1), hidden),\n",
    "            ('gene','codes','protein'):          SAGEConv((-1, -1), hidden),\n",
    "            ('protein','rev_codes','gene'):      SAGEConv((-1, -1), hidden),\n",
    "            ('patient','mutated','protein'):     SAGEConv((-1, -1), hidden),\n",
    "            ('protein','rev_mutated','patient'): SAGEConv((-1, -1), hidden),\n",
    "        }, aggr='sum')\n",
    "\n",
    "        if use_bn:\n",
    "            self.bn_prot_1 = nn.BatchNorm1d(hidden)\n",
    "            self.bn_gene_1 = nn.BatchNorm1d(hidden)\n",
    "            self.ln_pat_1  = nn.LayerNorm(hidden)\n",
    "\n",
    "            self.bn_prot_2 = nn.BatchNorm1d(hidden)\n",
    "            self.bn_gene_2 = nn.BatchNorm1d(hidden)\n",
    "            self.ln_pat_2  = nn.LayerNorm(hidden)\n",
    "\n",
    "        self.lin_fuse = nn.Linear(3 * hidden, hidden)\n",
    "        self.cls = nn.Linear(hidden, n_classes)\n",
    "\n",
    "    def _maybe_gate_protein_input(self, x_prot: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.mask_gate or x_prot.size(1) <= self.mask_channel_prot:\n",
    "            return x_prot\n",
    "        mask = x_prot[:, self.mask_channel_prot:self.mask_channel_prot+1].clamp(0, 1)\n",
    "        scale = 1.0 - self.gate_strength * mask\n",
    "        xg = x_prot.clone()\n",
    "        if self.mask_channel_prot > 0:\n",
    "            xg[:, :self.mask_channel_prot] = xg[:, :self.mask_channel_prot] * scale\n",
    "        return xg\n",
    "\n",
    "    def _bn_relu_drop(self, x: torch.Tensor, bn: nn.Module | None) -> torch.Tensor:\n",
    "        if self.use_bn and bn is not None:\n",
    "            # If it's BatchNorm and we have <2 samples, fallback to LayerNorm\n",
    "            if isinstance(bn, nn.BatchNorm1d) and x.shape[0] < 2:\n",
    "                x = F.layer_norm(x, (x.shape[1],))\n",
    "            else:\n",
    "                x = bn(x)\n",
    "        x = F.relu(x)\n",
    "        if self.feat_drop > 0:\n",
    "            x = F.dropout(x, p=self.feat_drop, training=self.training)\n",
    "        return x\n",
    "\n",
    "    def _edge_dropout(self, g: HeteroData):\n",
    "        if (not self.training) or (self.edge_drop <= 0):\n",
    "            return g.edge_index_dict\n",
    "        out = {}\n",
    "        for etype, ei in g.edge_index_dict.items():\n",
    "            ei_do, _ = dropout_edge(ei, p=self.edge_drop, training=True)\n",
    "            out[etype] = ei_do\n",
    "        return out\n",
    "\n",
    "    def forward(self, g: HeteroData):\n",
    "        # Inputs (with optional mask-gating for protein)\n",
    "        prot_in = self._maybe_gate_protein_input(g['protein'].x)\n",
    "        gene_in = g['gene'].x\n",
    "        pat_in  = g['patient'].x\n",
    "\n",
    "        x = {\n",
    "            'protein': self.lin_prot(prot_in),\n",
    "            'gene':    self.lin_gene(gene_in),\n",
    "            'patient': self.lin_pat(pat_in),\n",
    "        }\n",
    "        # Apply BN/LN appropriately\n",
    "        x['protein'] = self._bn_relu_drop(x['protein'], getattr(self, 'bn_prot_in', None))\n",
    "        x['gene']    = self._bn_relu_drop(x['gene'],   getattr(self, 'bn_gene_in', None))\n",
    "        # patient uses LayerNorm (safe with batch size 1)\n",
    "        if self.use_bn:\n",
    "            x['patient'] = self.ln_pat_in(x['patient'])\n",
    "            x['patient'] = F.relu(x['patient'])\n",
    "            if self.feat_drop > 0:\n",
    "                x['patient'] = F.dropout(x['patient'], p=self.feat_drop, training=self.training)\n",
    "        else:\n",
    "            x['patient'] = F.relu(x['patient'])\n",
    "\n",
    "        # GNN layer 1\n",
    "        eidx1 = self._edge_dropout(g)\n",
    "        x_res = {k: v for k, v in x.items()}\n",
    "        x = self.conv1(x, eidx1)\n",
    "        for k in x.keys():\n",
    "            if self.residual and k in x_res and x[k].shape == x_res[k].shape:\n",
    "                x[k] = x[k] + x_res[k]\n",
    "        x['protein'] = self._bn_relu_drop(x['protein'], getattr(self, 'bn_prot_1', None))\n",
    "        x['gene']    = self._bn_relu_drop(x['gene'],   getattr(self, 'bn_gene_1', None))\n",
    "        if self.use_bn:\n",
    "            x['patient'] = self.ln_pat_1(x['patient']); x['patient'] = F.relu(x['patient'])\n",
    "            if self.feat_drop > 0:\n",
    "                x['patient'] = F.dropout(x['patient'], p=self.feat_drop, training=self.training)\n",
    "        else:\n",
    "            x['patient'] = F.relu(x['patient'])\n",
    "\n",
    "        # GNN layer 2\n",
    "        eidx2 = self._edge_dropout(g)\n",
    "        x_res = {k: v for k, v in x.items()}\n",
    "        x = self.conv2(x, eidx2)\n",
    "        for k in x.keys():\n",
    "            if self.residual and k in x_res and x[k].shape == x_res[k].shape:\n",
    "                x[k] = x[k] + x_res[k]\n",
    "        x['protein'] = self._bn_relu_drop(x['protein'], getattr(self, 'bn_prot_2', None))\n",
    "        x['gene']    = self._bn_relu_drop(x['gene'],   getattr(self, 'bn_gene_2', None))\n",
    "        if self.use_bn:\n",
    "            x['patient'] = self.ln_pat_2(x['patient']); x['patient'] = F.relu(x['patient'])\n",
    "            if self.feat_drop > 0:\n",
    "                x['patient'] = F.dropout(x['patient'], p=self.feat_drop, training=self.training)\n",
    "        else:\n",
    "            x['patient'] = F.relu(x['patient'])\n",
    "\n",
    "        # Pool per batch\n",
    "        def bvec(tname):\n",
    "            return g[tname].batch if 'batch' in g[tname] else torch.zeros(\n",
    "                x[tname].size(0), dtype=torch.long, device=x[tname].device\n",
    "            )\n",
    "\n",
    "        z_prot = global_mean_pool(x['protein'], bvec('protein'))\n",
    "        z_gene = global_mean_pool(x['gene'],    bvec('gene'))\n",
    "        z_pat  = global_mean_pool(x['patient'], bvec('patient'))\n",
    "\n",
    "        z = torch.cat([z_pat, z_prot, z_gene], dim=-1)\n",
    "        z = F.relu(self.lin_fuse(z))\n",
    "        if self.feat_drop > 0:\n",
    "            z = F.dropout(z, p=self.feat_drop, training=self.training)\n",
    "        logits = self.cls(z)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bad7c42e-0600-438a-b837-f45a89744068",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:14:19.947876Z",
     "iopub.status.busy": "2025-10-03T16:14:19.947547Z",
     "iopub.status.idle": "2025-10-03T16:14:20.900554Z",
     "shell.execute_reply": "2025-10-03T16:14:20.899301Z",
     "shell.execute_reply.started": "2025-10-03T16:14:19.947850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mut_lists] patients=76 | with_any=76 | avg_muted_proteins_per_patient=205.4\n",
      "Coverage: mutation columns matched = 76/76 patients | row overlap (mut→protein base ids with any 1s) = 5407/7102\n"
     ]
    }
   ],
   "source": [
    "## ---------- Build mut_lists from gene-level binary (robust to Ensembl versions) ----------\n",
    "\n",
    "def ensg_base(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    m = re.match(r'^(ENSG[0-9]+)', s)   # keep only ENSG base (drop .version)\n",
    "    return m.group(1) if m else s       # if not ENSG, return as-is (e.g., TP53)\n",
    "\n",
    "# 1) Load\n",
    "mut_url = URLS[\"mut_gene_bin\"]\n",
    "mut_df = pd.read_csv(mut_url, sep=\"\\t\", header=0)\n",
    "\n",
    "# 2) Identify gene column (first col) and normalize to ENSG base\n",
    "gene_col = mut_df.columns[0]\n",
    "mut_df = mut_df.rename(columns={gene_col: \"gene\"})\n",
    "mut_df[\"gene_base\"] = mut_df[\"gene\"].map(ensg_base)\n",
    "mut_df = mut_df.drop(columns=[\"gene\"]).set_index(\"gene_base\")\n",
    "\n",
    "# 2b) Coerce mutation values to numeric and collapse duplicate patient columns (if any)\n",
    "mut_df = mut_df.apply(pd.to_numeric, errors=\"coerce\")      # non-numeric -> NaN\n",
    "mut_df = mut_df.fillna(0)                                  # treat NaN as 0 (no mutation)\n",
    "if mut_df.columns.has_duplicates:\n",
    "    mut_df = mut_df.T.groupby(level=0).max(numeric_only=True).T  # binary OR across dup cols\n",
    "\n",
    "# 3) Collapse duplicate Ensembl rows (binary OR across dup rows)\n",
    "if mut_df.index.has_duplicates:\n",
    "    mut_df = mut_df.groupby(level=0).max(numeric_only=True)\n",
    "\n",
    "# 4) Normalize current protein node IDs to the same base\n",
    "protein_ids_base = [ensg_base(g) for g in protein_ids]\n",
    "prot_base_to_idx = {}\n",
    "for i, b in enumerate(protein_ids_base):\n",
    "    # keep first occurrence so union order -> node index is stable\n",
    "    if b not in prot_base_to_idx:\n",
    "        prot_base_to_idx[b] = i\n",
    "\n",
    "# 5) Keep only mutation columns for our current patient_ids (preserve order)\n",
    "mut_cols = [p for p in patient_ids if p in mut_df.columns]\n",
    "mut_df = mut_df.reindex(columns=mut_cols)\n",
    "\n",
    "# 6) Align mutation rows to *base* protein IDs; missing -> 0\n",
    "mut_df = mut_df.reindex(index=list(prot_base_to_idx.keys())).fillna(0)\n",
    "\n",
    "# 7) Ensure strictly binary int8\n",
    "mut_df = (mut_df > 0).astype(np.int8)\n",
    "\n",
    "# 8) Build mut_lists in EXACT patient_ids order (empty if patient not present in file)\n",
    "present_cols = set(mut_df.columns)\n",
    "col_pos = {c:i for i, c in enumerate(mut_cols)}  # avoid O(N) .index() calls in loop\n",
    "\n",
    "mut_lists = []\n",
    "for p in patient_ids:\n",
    "    if p in present_cols:\n",
    "        j = col_pos[p]\n",
    "        prot_indices = np.nonzero(mut_df.iloc[:, j].to_numpy(dtype=bool))[0].astype(np.int64)\n",
    "        # map row order (base ID) -> protein node index\n",
    "        base_hits = mut_df.index[prot_indices].tolist()\n",
    "        idx_hits = np.array([prot_base_to_idx[b] for b in base_hits if b in prot_base_to_idx], dtype=np.int64)\n",
    "        mut_lists.append(idx_hits)\n",
    "    else:\n",
    "        mut_lists.append(np.array([], dtype=np.int64))\n",
    "\n",
    "# 9) Diagnostics + sanity checks\n",
    "n_with_any = sum(arr.size > 0 for arr in mut_lists)\n",
    "avg_muts   = float(np.mean([arr.size for arr in mut_lists])) if mut_lists else 0.0\n",
    "\n",
    "print(\n",
    "    f\"[mut_lists] patients={len(mut_lists)} | with_any={n_with_any} | \"\n",
    "    f\"avg_muted_proteins_per_patient={avg_muts:.1f}\"\n",
    ")\n",
    "print(\n",
    "    \"Coverage:\",\n",
    "    f\"mutation columns matched = {len(mut_cols)}/{len(patient_ids)} patients | \"\n",
    "    f\"row overlap (mut→protein base ids with any 1s) = \"\n",
    "    f\"{int((mut_df.sum(axis=1)>0).sum())}/{len(prot_base_to_idx)}\"\n",
    ")\n",
    "\n",
    "assert len(mut_lists) == len(patient_ids), \"mut_lists must align 1:1 with patient_ids\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3d2b0d9-ef7b-4c92-8ccc-ee74e89fa869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:14:20.902359Z",
     "iopub.status.busy": "2025-10-03T16:14:20.901856Z",
     "iopub.status.idle": "2025-10-03T16:14:20.909343Z",
     "shell.execute_reply": "2025-10-03T16:14:20.908007Z",
     "shell.execute_reply.started": "2025-10-03T16:14:20.902322Z"
    }
   },
   "outputs": [],
   "source": [
    "DO_KFOLD = False\n",
    "N_FOLDS = 5\n",
    "if DO_KFOLD:\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=123)\n",
    "    folds = list(skf.split(np.zeros_like(labels_aligned.numpy()), labels_aligned.numpy()))\n",
    "    # Example: pick one fold and overwrite train_idx/val_idx/test_idx\n",
    "    train_idx, test_idx = folds[0][0], folds[0][1]\n",
    "    # Inside train split, carve a small val set\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=321)\n",
    "    tr_sub, va_sub = next(sss.split(np.zeros_like(labels_aligned.numpy()[train_idx]),\n",
    "                                    labels_aligned.numpy()[train_idx]))\n",
    "    val_idx  = train_idx[va_sub]\n",
    "    train_idx = train_idx[tr_sub]\n",
    "    print(f\"[KFold] Fold0 train={len(train_idx)} val={len(val_idx)} test={len(test_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "786bf00f-cd41-461c-b80c-0c4411ae8d28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:14:20.911287Z",
     "iopub.status.busy": "2025-10-03T16:14:20.910890Z",
     "iopub.status.idle": "2025-10-03T16:14:20.939862Z",
     "shell.execute_reply": "2025-10-03T16:14:20.938864Z",
     "shell.execute_reply.started": "2025-10-03T16:14:20.911249Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protein feat dims: torch.Size([7102, 3])\n",
      "gene feat dims: torch.Size([7093, 4])\n",
      "ppi edges: 191304\n",
      "codes edges: 7093\n",
      "mut edges: 23\n",
      "label: 1\n"
     ]
    }
   ],
   "source": [
    "# ---------- Loaders ----------\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def take_rows(X, idx): \n",
    "    return X[idx]\n",
    "\n",
    "# Split tensors by your precomputed indices\n",
    "X_prot_tr, X_prot_va, X_prot_te = take_rows(X_prot, train_idx), take_rows(X_prot, val_idx), take_rows(X_prot, test_idx)\n",
    "X_phos_tr, X_phos_va, X_phos_te = take_rows(X_phos, train_idx), take_rows(X_phos, val_idx), take_rows(X_phos, test_idx)\n",
    "X_mprt_tr, X_mprt_va, X_mprt_te = take_rows(X_mask_prot, train_idx), take_rows(X_mask_prot, val_idx), take_rows(X_mask_prot, test_idx)\n",
    "\n",
    "X_rna_tr,  X_rna_va,  X_rna_te  = take_rows(X_rna, train_idx),  take_rows(X_rna, val_idx),  take_rows(X_rna, test_idx)\n",
    "X_cnv_tr,  X_cnv_va,  X_cnv_te  = take_rows(X_cnv, train_idx),  take_rows(X_cnv, val_idx),  take_rows(X_cnv, test_idx)\n",
    "X_ravl_tr, X_ravl_va, X_ravl_te = take_rows(X_rna_avl, train_idx), take_rows(X_rna_avl, val_idx), take_rows(X_rna_avl, test_idx)\n",
    "X_cavl_tr, X_cavl_va, X_cavl_te = take_rows(X_cnv_avl, train_idx), take_rows(X_cnv_avl, val_idx), take_rows(X_cnv_avl, test_idx)\n",
    "\n",
    "y_tr, y_va, y_te = labels_aligned[train_idx], labels_aligned[val_idx], labels_aligned[test_idx]\n",
    "pids_tr = [patient_ids[i] for i in train_idx]\n",
    "pids_va = [patient_ids[i] for i in val_idx]\n",
    "pids_te = [patient_ids[i] for i in test_idx]\n",
    "\n",
    "train_ds = MultiOmicsPatientDataset(\n",
    "    pids_tr, y_tr,\n",
    "    X_prot_tr, X_phos_tr, X_mprt_tr,\n",
    "    X_rna_tr,  X_cnv_tr,  X_ravl_tr, X_cavl_tr,\n",
    "    [mut_lists[i] for i in train_idx],\n",
    "    protein_ids, gene_ids,\n",
    "    ppi_edge_index, codes_edge_index, codes_rev_edge_index\n",
    ")\n",
    "val_ds = MultiOmicsPatientDataset(\n",
    "    pids_va, y_va,\n",
    "    X_prot_va, X_phos_va, X_mprt_va,\n",
    "    X_rna_va,  X_cnv_va,  X_ravl_va, X_cavl_va,\n",
    "    [mut_lists[i] for i in val_idx],\n",
    "    protein_ids, gene_ids,\n",
    "    ppi_edge_index, codes_edge_index, codes_rev_edge_index\n",
    ")\n",
    "test_ds = MultiOmicsPatientDataset(\n",
    "    pids_te, y_te,\n",
    "    X_prot_te, X_phos_te, X_mprt_te,\n",
    "    X_rna_te,  X_cnv_te,  X_ravl_te, X_cavl_te,\n",
    "    [mut_lists[i] for i in test_idx],\n",
    "    protein_ids, gene_ids,\n",
    "    ppi_edge_index, codes_edge_index, codes_rev_edge_index\n",
    ")\n",
    "\n",
    "# Smaller batch size is safer; set num_workers=0 in notebooks\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True,  num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "val_loader   = DataLoader(val_ds,   batch_size=8, shuffle=False, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "test_loader  = DataLoader(test_ds,  batch_size=8, shuffle=False, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# Quick sanity\n",
    "g0 = train_ds[0]\n",
    "print(\"protein feat dims:\", g0['protein'].x.shape)\n",
    "print(\"gene feat dims:\",    g0['gene'].x.shape)\n",
    "print(\"ppi edges:\",         g0[('protein','ppi','protein')].edge_index.shape[1])\n",
    "print(\"codes edges:\",       g0[('gene','codes','protein')].edge_index.shape[1])\n",
    "print(\"mut edges:\",         g0[('patient','mutated','protein')].edge_index.shape[1])\n",
    "print(\"label:\",             g0['patient'].y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70ef9cdd-ea1f-4019-981d-2b2bb92150bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:14:20.941462Z",
     "iopub.status.busy": "2025-10-03T16:14:20.940983Z",
     "iopub.status.idle": "2025-10-03T16:14:22.710220Z",
     "shell.execute_reply": "2025-10-03T16:14:22.709278Z",
     "shell.execute_reply.started": "2025-10-03T16:14:20.941424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: (8, 4)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_classes = int(labels_aligned.max().item() + 1)\n",
    "model = MultiOmicsKGNN(in_protein=3, in_gene=4, in_patient=1, hidden=64, n_classes=n_classes).to(device)\n",
    "\n",
    "# Try a forward pass\n",
    "g_test = next(iter(train_loader)).to(device)\n",
    "with torch.no_grad():\n",
    "    out = model(g_test)\n",
    "print(\"Logits shape:\", tuple(out.shape))  # should be (batch_size, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942cea4-455a-4e53-9270-374087ceed13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T16:14:22.712226Z",
     "iopub.status.busy": "2025-10-03T16:14:22.711899Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train 1.377/0.298 | val 1.455/0.133 | macroF1 0.059 balAcc 0.250\n",
      "Epoch 02 | train 1.456/0.246 | val 1.480/0.200 | macroF1 0.083 balAcc 0.250\n"
     ]
    }
   ],
   "source": [
    "# ---------- Training with early-stopping, optional sampler, detailed reports ----------\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_classes = int(labels_aligned.max().item() + 1)\n",
    "\n",
    "model = MultiOmicsKGNN(\n",
    "    in_protein=3,\n",
    "    in_gene=4,\n",
    "    in_patient=1,\n",
    "    hidden=128,\n",
    "    n_classes=n_classes,\n",
    "    feat_drop=0.2,\n",
    "    edge_drop=0.05,\n",
    "    use_bn=True,\n",
    "    residual=True,\n",
    "    mask_gate=True,     # uses protein mask channel to down-weight missing entries\n",
    "    gate_strength=1.0,  # 1.0 -> fully zero out where mask==1\n",
    ").to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def feature_dropout_inplace(g, p_prot=0.10, p_gene=0.10):\n",
    "    \"\"\"\n",
    "    Randomly zero a fraction of *expression* channels to improve robustness.\n",
    "    - protein node: x[:,0]=prot_z, x[:,1]=phos_z, x[:,2]=prot_missing_mask (do NOT drop ch=2)\n",
    "    - gene node:    x[:,0]=rna_z,  x[:,1]=cnv_z,  x[:,2]=rna_avl, x[:,3]=cnv_avl (do NOT drop ch>=2)\n",
    "    \"\"\"\n",
    "    if 'protein' in g.node_types and g['protein'].x.numel() > 0:\n",
    "        for ch in (0, 1):\n",
    "            if ch < g['protein'].x.size(1):\n",
    "                m = (torch.rand_like(g['protein'].x[:, ch]) < p_prot)\n",
    "                g['protein'].x[m, ch] = 0\n",
    "\n",
    "    if 'gene' in g.node_types and g['gene'].x.numel() > 0:\n",
    "        for ch in (0, 1):\n",
    "            if ch < g['gene'].x.size(1):\n",
    "                m = (torch.rand_like(g['gene'].x[:, ch]) < p_gene)\n",
    "                g['gene'].x[m, ch] = 0.2\n",
    "\n",
    "# ----- Optional: Weighted sampler for training -----\n",
    "USE_WEIGHTED_SAMPLER = True\n",
    "if USE_WEIGHTED_SAMPLER:\n",
    "    # class counts on train\n",
    "    y_train_np = labels_aligned[train_idx].numpy()\n",
    "    cnt = Counter(y_train_np.tolist())\n",
    "    cls_weights = {c: 1.0/max(cnt.get(c,1), 1) for c in range(n_classes)}\n",
    "    sample_weights = torch.tensor([cls_weights[int(c)] for c in y_train_np], dtype=torch.float32)\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, sampler=sampler, shuffle=False,\n",
    "                              num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "else:\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True,\n",
    "                              num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "val_loader  = DataLoader(val_ds,  batch_size=8, shuffle=False, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "test_loader = DataLoader(test_ds, batch_size=8, shuffle=False, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# Weighted loss from TRAIN only (matches sampler)\n",
    "cnt = Counter(labels_aligned[train_idx].numpy().tolist())\n",
    "weights = torch.tensor([1.0 / max(cnt.get(i, 1), 1) for i in range(n_classes)],\n",
    "                       dtype=torch.float32, device=device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights, label_smoothing=0.10)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=8e-4, weight_decay=5e-4)\n",
    "\n",
    "# reduce LR on plateau (monitors val macro-F1)\n",
    "sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='max', factor=0.5, patience=5)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    all_y, all_p = [], []\n",
    "    tot_loss, n_batches = 0.0, 0\n",
    "    for g in loader:\n",
    "        g = g.to(device)\n",
    "        logits = model(g)\n",
    "        y = g['patient'].y.view(-1).to(device)\n",
    "        loss = criterion(logits, y)\n",
    "        tot_loss += float(loss.item()); n_batches += 1\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        all_y.extend(y.detach().cpu().numpy().tolist())\n",
    "        all_p.extend(pred.detach().cpu().numpy().tolist())\n",
    "    if n_batches == 0:\n",
    "        return 0.0, 0.0, np.array([]), np.array([])\n",
    "    avg_loss = tot_loss / n_batches\n",
    "    all_y, all_p = np.array(all_y), np.array(all_p)\n",
    "    acc = (all_y == all_p).mean() if all_y.size else 0.0\n",
    "    return avg_loss, acc, all_y, all_p\n",
    "\n",
    "\"\"\"def train_epoch(loader):\n",
    "    model.train()\n",
    "    tot_loss, correct, total, n_batches = 0.0, 0, 0, 0\n",
    "    for g in loader:\n",
    "        g = g.to(device)\n",
    "        feature_dropout_inplace(g, p_prot=0.10, p_gene=0.10)\n",
    "        logits = model(g)\n",
    "        y = g['patient'].y.view(-1).to(device)\n",
    "        loss = criterion(logits, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        opt.step()\n",
    "        tot_loss += float(loss.item()); n_batches += 1\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        correct += int((pred == y).sum().item())\n",
    "        total += y.size(0)\n",
    "    avg_loss = tot_loss / max(n_batches, 1)\n",
    "    acc = correct / max(total, 1)\n",
    "    return avg_loss, acc\"\"\"\n",
    "\n",
    "AUG_FEATURE_DROPOUT = True  # toggle\n",
    "P_PROT = 0.10\n",
    "P_GENE = 0.10\n",
    "\n",
    "def train_epoch(loader):\n",
    "    model.train()\n",
    "    tot_loss, correct, total, n_batches = 0.0, 0, 0, 0\n",
    "    for g in loader:\n",
    "        g = g.to(device)\n",
    "\n",
    "        # --- train-time feature dropout (do NOT do this in evaluate) ---\n",
    "        if AUG_FEATURE_DROPOUT:\n",
    "            feature_dropout_inplace(g, p_prot=P_PROT, p_gene=P_GENE)\n",
    "\n",
    "        logits = model(g)\n",
    "        y = g['patient'].y.view(-1).to(device)\n",
    "        loss = criterion(logits, y)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "        tot_loss += float(loss.item()); n_batches += 1\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        correct += int((pred == y).sum().item())\n",
    "        total += y.size(0)\n",
    "\n",
    "    avg_loss = tot_loss / max(n_batches, 1)\n",
    "    acc = correct / max(total, 1)\n",
    "    return avg_loss, acc\n",
    "\n",
    "best_val_macro_f1, best_state = -1.0, None\n",
    "EPOCHS = 60\n",
    "REPORT_EVERY = 5\n",
    "PATIENCE = 12\n",
    "epochs_since_improve = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_acc = train_epoch(train_loader)\n",
    "    va_loss, va_acc, y_true, y_pred = evaluate(val_loader)\n",
    "\n",
    "    # Robust metrics for imbalanced, small validation splits\n",
    "    if y_true.size > 0:\n",
    "        macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        bal_acc  = balanced_accuracy_score(y_true, y_pred) if len(np.unique(y_true)) > 1 else va_acc\n",
    "    else:\n",
    "        macro_f1, bal_acc = 0.0, 0.0\n",
    "\n",
    "    # LR scheduling & early stopping on macro-F1\n",
    "    sched.step(macro_f1)\n",
    "    improved = macro_f1 > best_val_macro_f1 + 1e-4\n",
    "    if improved:\n",
    "        best_val_macro_f1 = macro_f1\n",
    "        best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "        epochs_since_improve = 0\n",
    "    else:\n",
    "        epochs_since_improve += 1\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"train {tr_loss:.3f}/{tr_acc:.3f} | \"\n",
    "          f\"val {va_loss:.3f}/{va_acc:.3f} | \"\n",
    "          f\"macroF1 {macro_f1:.3f} balAcc {bal_acc:.3f}\")\n",
    "\n",
    "    if (epoch % REPORT_EVERY == 0) and (y_true.size > 0):\n",
    "        print(classification_report(\n",
    "            y_true, y_pred,\n",
    "            labels=list(range(n_classes)),\n",
    "            target_names=[f\"c{i}\" for i in range(n_classes)],\n",
    "            digits=3,\n",
    "            zero_division=0\n",
    "        ))\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=list(range(n_classes)))\n",
    "        print(\"Confusion matrix (val):\\n\", cm)\n",
    "\n",
    "    if epochs_since_improve >= PATIENCE:\n",
    "        print(f\"Early stopping: no improvement in macro-F1 for {PATIENCE} epochs.\")\n",
    "        break\n",
    "\n",
    "# Load best & evaluate on test\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "te_loss, te_acc, y_true_te, y_pred_te = evaluate(test_loader)\n",
    "print(f\"TEST  | loss {te_loss:.3f} acc {te_acc:.3f}\")\n",
    "if y_true_te.size > 0:\n",
    "    print(classification_report(\n",
    "        y_true_te, y_pred_te,\n",
    "        labels=list(range(n_classes)),\n",
    "        target_names=[f\"c{i}\" for i in range(n_classes)],\n",
    "        digits=3,\n",
    "        zero_division=0\n",
    "    ))\n",
    "    cm = confusion_matrix(y_true_te, y_pred_te, labels=list(range(n_classes)))\n",
    "    print(\"Confusion matrix (test):\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd83231-b6dd-452c-9e6b-479f45723480",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "model.eval()\n",
    "for g in test_loader:\n",
    "    g = g.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(g)\n",
    "    y_true.extend(g['patient'].y.view(-1).cpu().numpy().tolist())\n",
    "    y_pred.extend(logits.argmax(dim=-1).cpu().numpy().tolist())\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=3))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_true_te, y_pred_te, labels=list(range(n_classes))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
